CNTK 1.7.2+ (qiwye/asgd-dev 9944ba, Nov  1 2016 16:34:35) on localhost at 2016/11/01 21:13:44

/home/qiwye/git/CNTK-exp-private/build/release/bin/cntk  configfile=03_ResNet-parallel.cntk  parallelTrain=true  deviceId=auto  minibatch=256  epochsize=10
  configName=4gpu-take1  parallelizationMethod=DataParallelASGD  asyncBuffer=false
CNTK 1.7.2+ (qiwye/asgd-dev 9944ba, Nov  1 2016 16:34:35) on localhost at 2016/11/01 21:13:44

/home/qiwye/git/CNTK-exp-private/build/release/bin/cntk  configfile=03_ResNet-parallel.cntk  parallelTrain=true  deviceId=auto  minibatch=256  epochsize=10
  configName=4gpu-take1  parallelizationMethod=DataParallelASGD  asyncBuffer=false
CNTK 1.7.2+ (qiwye/asgd-dev 9944ba, Nov  1 2016 16:34:35) on localhost at 2016/11/01 21:13:44

/home/qiwye/git/CNTK-exp-private/build/release/bin/cntk  configfile=03_ResNet-parallel.cntk  parallelTrain=true  deviceId=auto  minibatch=256  epochsize=10
  configName=4gpu-take1  parallelizationMethod=DataParallelASGD  asyncBuffer=false
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
CNTK 1.7.2+ (qiwye/asgd-dev 9944ba, Nov  1 2016 16:34:35) on localhost at 2016/11/01 21:13:44

/home/qiwye/git/CNTK-exp-private/build/release/bin/cntk  configfile=03_ResNet-parallel.cntk  parallelTrain=true  deviceId=auto  minibatch=256  epochsize=10
  configName=4gpu-take1  parallelizationMethod=DataParallelASGD  asyncBuffer=false
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
-------------------------------------------------------------------
Build info:

                Built time: Nov  1 2016 16:34:35
                Last modified date: Sun Oct  9 17:21:24 2016
                Build type: release
                Build target: GPU
                With 1bit-SGD: yes
                With ASGD: yes
                Math lib: mkl
                CUDA_PATH: /usr/local/cuda-7.5
                CUB_PATH: /usr/local/cub-1.4.1
                CUDNN_PATH: /usr/local/cudnn-5.1
                Build Branch: qiwye/asgd-dev
                Build SHA1: 9944ba4ce647d7435bf27796e45ecd6941351e89 (modified)
                Built by Source/CNTK/buildinfo.h$$0 on msraml-tesla01
                Build Path: /home/qiwye/git/CNTK-exp-private
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info:

                Built time: Nov  1 2016 16:34:35
                Last modified date: Sun Oct  9 17:21:24 2016
                Build type: release
                Build target: GPU
                With 1bit-SGD: yes
                With ASGD: yes
                Math lib: mkl
                CUDA_PATH: /usr/local/cuda-7.5
                CUB_PATH: /usr/local/cub-1.4.1
                CUDNN_PATH: /usr/local/cudnn-5.1
                Build Branch: qiwye/asgd-dev
                Build SHA1: 9944ba4ce647d7435bf27796e45ecd6941351e89 (modified)
                Built by Source/CNTK/buildinfo.h$$0 on msraml-tesla01
                Build Path: /home/qiwye/git/CNTK-exp-private
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info:

                Built time: Nov  1 2016 16:34:35
                Last modified date: Sun Oct  9 17:21:24 2016
                Build type: release
                Build target: GPU
                With 1bit-SGD: yes
                With ASGD: yes
                Math lib: mkl
                CUDA_PATH: /usr/local/cuda-7.5
                CUB_PATH: /usr/local/cub-1.4.1
                CUDNN_PATH: /usr/local/cudnn-5.1
                Build Branch: qiwye/asgd-dev
                Build SHA1: 9944ba4ce647d7435bf27796e45ecd6941351e89 (modified)
                Built by Source/CNTK/buildinfo.h$$0 on msraml-tesla01
                Build Path: /home/qiwye/git/CNTK-exp-private
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info:

                Built time: Nov  1 2016 16:34:35
                Last modified date: Sun Oct  9 17:21:24 2016
                Build type: release
                Build target: GPU
                With 1bit-SGD: yes
                With ASGD: yes
                Math lib: mkl
                CUDA_PATH: /usr/local/cuda-7.5
                CUB_PATH: /usr/local/cub-1.4.1
                CUDNN_PATH: /usr/local/cudnn-5.1
                Build Branch: qiwye/asgd-dev
                Build SHA1: 9944ba4ce647d7435bf27796e45ecd6941351e89 (modified)
                Built by Source/CNTK/buildinfo.h$$0 on msraml-tesla01
                Build Path: /home/qiwye/git/CNTK-exp-private
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

                Device[0]: cores = 2880; computeCapability = 3.5; type = "Tesla K40m"; memory = 11439 MB
                Device[1]: cores = 2880; computeCapability = 3.5; type = "Tesla K40m"; memory = 11439 MB
                Device[2]: cores = 2880; computeCapability = 3.5; type = "Tesla K40m"; memory = 11439 MB
                Device[3]: cores = 2880; computeCapability = 3.5; type = "Tesla K40m"; memory = 11439 MB
                Device[4]: cores = 2880; computeCapability = 3.5; type = "Tesla K40m"; memory = 11439 MB
                Device[5]: cores = 2880; computeCapability = 3.5; type = "Tesla K40m"; memory = 11439 MB
                Device[6]: cores = 2880; computeCapability = 3.5; type = "Tesla K40m"; memory = 11439 MB
                Device[7]: cores = 2880; computeCapability = 3.5; type = "Tesla K40m"; memory = 11439 MB
-------------------------------------------------------------------

MPI Rank 0: Configuration After Processing and Variable Resolution:
MPI Rank 0: 
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:asyncBuffer=false
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:command=Train
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:ConfigDir=.
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:configName=4gpu-take1
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:DataDir=.
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:deviceId=auto
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:epochSize=10
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:imageLayout=cudnn
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:initOnCPUOnly=true
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:makeMode=true
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:minibatch=512
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:minibatchSize=128
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:ModelDir=./Output-4gpu-take1/Models
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:ndlMacros=./Macros.ndl
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:OutputDir=./Output-4gpu-take1
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:parallelizationMethod=DataParallelASGD
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:parallelTrain=true
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:precision=float
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:prefetch=true
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:Proj16to32Filename=./16to32.txt
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:Proj32to64Filename=./32to64.txt
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:RootDir=.
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:stderr=./Output-4gpu-take1/03_ResNet
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:Test=[
MPI Rank 0:     action = "test"
MPI Rank 0:     modelPath = "./Output-4gpu-take1/Models/03_ResNet"
MPI Rank 0:     minibatchSize = 256
MPI Rank 0:     reader = [
MPI Rank 0:         readerType = "ImageReader"
MPI Rank 0:         file = "./cifar-10-batches-py/test_map.txt"
MPI Rank 0:         randomize = "none"
MPI Rank 0:         features = [
MPI Rank 0:             width = 32
MPI Rank 0:             height = 32
MPI Rank 0:             channels = 3
MPI Rank 0:             cropType = "center"
MPI Rank 0:             cropRatio = 1
MPI Rank 0:             jitterType = "uniRatio"
MPI Rank 0:             interpolations = "linear"
MPI Rank 0:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 0:         ]
MPI Rank 0:         labels = [
MPI Rank 0:             labelDim = 10
MPI Rank 0:         ]
MPI Rank 0:     ]    
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:traceLevel=1
MPI Rank 0: configparameters: 03_ResNet-parallel.cntk:Train=[
MPI Rank 0:     action = "train"
MPI Rank 0:     modelPath = "./Output-4gpu-take1/Models/03_ResNet"
MPI Rank 0:      NDLNetworkBuilder = [
MPI Rank 0:         networkDescription = "./03_ResNet.ndl"
MPI Rank 0:     ]
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize = 0
MPI Rank 0:         minibatchSize = 512
MPI Rank 0:         learningRatesPerSample = 0.004*80:0.0004*40:0.00004
MPI Rank 0:         momentumPerMB = 0
MPI Rank 0:         maxEpochs = 10
MPI Rank 0:         L2RegWeight = 0.0001
MPI Rank 0:         dropoutRate = 0
MPI Rank 0:         perfTraceLevel = 0
MPI Rank 0:         firstMBsToShowResult = 1
MPI Rank 0:         numMBsToShowResult = 10
MPI Rank 0:         ParallelTrain = [
MPI Rank 0:             parallelizationMethod = DataParallelASGD
MPI Rank 0:             distributedMBReading = "true"
MPI Rank 0:             parallelizationStartEpoch = 1
MPI Rank 0:             DataParallelSGD = [
MPI Rank 0:                 gradientBits = 32
MPI Rank 0:                 useBufferedAsyncGradientAggregation = false
MPI Rank 0:             ]
MPI Rank 0:             ModelAveragingSGD = [
MPI Rank 0:                 blockSizePerWorker = 128
MPI Rank 0:             ]
MPI Rank 0:             DataParallelASGD = [
MPI Rank 0:                 syncPeriod = 128
MPI Rank 0:                 usePipeline = false
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0:     ]
MPI Rank 0:     reader = [
MPI Rank 0:         readerType = "ImageReader"
MPI Rank 0:         file = "./cifar-10-batches-py/train_map.txt"
MPI Rank 0:         randomize = "auto"
MPI Rank 0:         features = [
MPI Rank 0:             width = 32
MPI Rank 0:             height = 32
MPI Rank 0:             channels = 3
MPI Rank 0:             cropType = "random"
MPI Rank 0:             cropRatio = 0.8
MPI Rank 0:             jitterType = "uniRatio"
MPI Rank 0:             interpolations = "linear"
MPI Rank 0:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 0:         ]
MPI Rank 0:         labels = [
MPI Rank 0:             labelDim = 10
MPI Rank 0:         ]
MPI Rank 0:     ]
MPI Rank 0:     cvReader = [
MPI Rank 0:         readerType = "ImageReader"
MPI Rank 0:         file = "./cifar-10-batches-py/test_map.txt"
MPI Rank 0:         randomize = "none"
MPI Rank 0:         features = [
MPI Rank 0:             width = 32
MPI Rank 0:             height = 32
MPI Rank 0:             channels = 3
MPI Rank 0:             cropType = "center"
MPI Rank 0:             cropRatio = 1
MPI Rank 0:             jitterType = "uniRatio"
MPI Rank 0:             interpolations = "linear"
MPI Rank 0:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 0:         ]
MPI Rank 0:         labels = [
MPI Rank 0:             labelDim = 10
MPI Rank 0:         ]
MPI Rank 0:     ]    
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: Commands: Train
MPI Rank 0: precision = "float"
MPI Rank 0: 
MPI Rank 0: ##############################################################################
MPI Rank 0: #                                                                            #
MPI Rank 0: # Train command (train action)                                               #
MPI Rank 0: #                                                                            #
MPI Rank 0: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Creating virgin network.
MPI Rank 0: NDLBuilder Using GPU 7
MPI Rank 0: SetGaussianRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 0: 
MPI Rank 0: OutputNodes.t Times operation: For legacy compatibility, the sample layout of left input (OutputNodes.W LearnableParameter operation) was patched to [10 x 1 x 1 x 64] (from [10 x 64])
MPI Rank 0: conv1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 16, Kernel: 3 x 3 x 3, Map: 1 x 1 x 16, Stride: 1 x 1 x 3, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn1_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn1_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn1_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn1_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn1_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn1_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn2_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 16 x 16 x 32, Kernel: 3 x 3 x 16, Map: 1 x 1 x 32, Stride: 2 x 2 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn2_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn2_1.c_proj.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 16 x 16 x 32, Kernel: 1 x 1 x 16, Map: 1 x 1 x 32, Stride: 2 x 2 x 16, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn2_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn2_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn2_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn2_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn3_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 8 x 8 x 64, Kernel: 3 x 3 x 32, Map: 1 x 1 x 64, Stride: 2 x 2 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn3_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn3_1.c_proj.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 8 x 8 x 64, Kernel: 1 x 1 x 32, Map: 1 x 1 x 64, Stride: 2 x 2 x 32, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn3_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn3_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn3_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: rn3_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 0: Using CNTK batch normalization engine.
MPI Rank 0: pool: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 1 x 1 x 64, Kernel: 8 x 8 x 1, Map: 1, Stride: 1 x 1 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 0: 
MPI Rank 0: Model has 184 nodes. Using GPU 7.
MPI Rank 0: 
MPI Rank 0: Training criterion:   CE = CrossEntropyWithSoftmax
MPI Rank 0: Evaluation criterion: Err = ClassificationError
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing: Out of 321 matrices, 160 are shared as 62, and 161 are not shared.
MPI Rank 0: 
MPI Rank 0:     { conv1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       conv1.y : [32 x 32 x 16 x *] }
MPI Rank 0:     { conv1.c.W : [16 x 27] (gradient)
MPI Rank 0:       rn1_1.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_1.c1.y : [32 x 32 x 16 x *] }
MPI Rank 0:     { conv1.c.c.b : [16 x 1] (gradient)
MPI Rank 0:       rn1_1.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_1.c1.c.W : [16 x 144] (gradient)
MPI Rank 0:       rn1_1.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_1.p : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_1.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_1.y : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_1.c2.W : [16 x 144] (gradient)
MPI Rank 0:       rn1_2.c1.c.c.c : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_1.c2.c.sc : [16 x 1] (gradient)
MPI Rank 0:       rn1_1.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 0:     { conv1.c.c.sc : [16 x 1] (gradient)
MPI Rank 0:       conv1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_2.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_2.c1.y : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_1.c2.c.b : [16 x 1] (gradient)
MPI Rank 0:       rn1_2.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_2.c1.c.W : [16 x 144] (gradient)
MPI Rank 0:       rn1_2.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_2.p : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_2.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_2.y : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_2.c2.W : [16 x 144] (gradient)
MPI Rank 0:       rn1_3.c1.c.c.c : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_2.c2.c.sc : [16 x 1] (gradient)
MPI Rank 0:       rn1_2.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 0:     { rn1_1.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 0:       rn1_1.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_3.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_3.c1.y : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_2.c2.c.b : [16 x 1] (gradient)
MPI Rank 0:       rn1_3.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_3.c1.c.W : [16 x 144] (gradient)
MPI Rank 0:       rn1_3.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_3.p : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_3.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_3.y : [32 x 32 x 16 x *] }
MPI Rank 0:     { rn1_3.c2.W : [16 x 144] (gradient)
MPI Rank 0:       rn2_1.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn1_3.c2.c.sc : [16 x 1] (gradient)
MPI Rank 0:       rn1_3.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 0:     { rn1_2.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 0:       rn1_2.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_2.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn2_1.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_1.c1.y : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn1_3.c2.c.b : [16 x 1] (gradient)
MPI Rank 0:       rn2_1.c2.c.c : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_1.c2.c.sc : [32 x 1] (gradient)
MPI Rank 0:       rn2_1.c_proj.c : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_1.c1.c.W : [32 x 144] (gradient)
MPI Rank 0:       rn2_1.c2.c.c : [16 x 16 x 32 x *] (gradient) }
MPI Rank 0:     { rn2_1.c2.c.b : [32 x 1] (gradient)
MPI Rank 0:       rn2_1.c_proj.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_1.p : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_1.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_1.y : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_1.c2.W : [32 x 288] (gradient)
MPI Rank 0:       rn2_2.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_1.c_proj.sc : [32 x 1] (gradient)
MPI Rank 0:       rn2_1.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 0:     { rn1_3.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 0:       rn1_3.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 0:       rn1_3.y : [32 x 32 x 16 x *] (gradient) }
MPI Rank 0:     { rn2_1.c_proj.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_2.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_2.c1.y : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_2.c1.c.W : [32 x 288] (gradient)
MPI Rank 0:       rn2_2.c2.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_2.p : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_2.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_2.y : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_2.c2.W : [32 x 288] (gradient)
MPI Rank 0:       rn2_3.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_2.c2.c.sc : [32 x 1] (gradient)
MPI Rank 0:       rn2_2.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 0:     { rn2_1.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 0:       rn2_1.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_3.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_3.c1.y : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_2.c2.c.b : [32 x 1] (gradient)
MPI Rank 0:       rn2_3.c2.c.c : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_3.c1.c.W : [32 x 288] (gradient)
MPI Rank 0:       rn2_3.c2.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_3.p : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_3.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_3.y : [16 x 16 x 32 x *] }
MPI Rank 0:     { rn2_3.c2.W : [32 x 288] (gradient)
MPI Rank 0:       rn3_1.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn2_3.c2.c.sc : [32 x 1] (gradient)
MPI Rank 0:       rn2_3.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 0:     { rn2_2.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 0:       rn2_2.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_2.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn3_1.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_1.c1.y : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn2_3.c2.c.b : [32 x 1] (gradient)
MPI Rank 0:       rn3_1.c2.c.c : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_1.c2.c.sc : [64 x 1] (gradient)
MPI Rank 0:       rn3_1.c_proj.c : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_1.c1.c.W : [64 x 288] (gradient)
MPI Rank 0:       rn3_1.c2.c.c : [8 x 8 x 64 x *] (gradient) }
MPI Rank 0:     { rn3_1.c2.c.b : [64 x 1] (gradient)
MPI Rank 0:       rn3_1.c_proj.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_1.p : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_1.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_1.y : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_1.c2.W : [64 x 576] (gradient)
MPI Rank 0:       rn3_2.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_1.c_proj.sc : [64 x 1] (gradient)
MPI Rank 0:       rn3_1.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 0:     { rn2_3.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 0:       rn2_3.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 0:       rn2_3.y : [16 x 16 x 32 x *] (gradient) }
MPI Rank 0:     { rn3_1.c_proj.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_2.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_2.c1.y : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_2.c1.c.W : [64 x 576] (gradient)
MPI Rank 0:       rn3_2.c2.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_2.p : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_2.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_2.y : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_2.c2.W : [64 x 576] (gradient)
MPI Rank 0:       rn3_3.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_2.c2.c.sc : [64 x 1] (gradient)
MPI Rank 0:       rn3_2.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 0:     { rn3_1.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 0:       rn3_1.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_3.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_3.c1.y : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_2.c2.c.b : [64 x 1] (gradient)
MPI Rank 0:       rn3_3.c2.c.c : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_3.c1.c.W : [64 x 576] (gradient)
MPI Rank 0:       rn3_3.c2.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_3.p : [8 x 8 x 64 x *] }
MPI Rank 0:     { rn3_3.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_3.y : [8 x 8 x 64 x *] }
MPI Rank 0:     { pool : [1 x 1 x 64 x *]
MPI Rank 0:       rn3_3.c2.c.sc : [64 x 1] (gradient)
MPI Rank 0:       rn3_3.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 0:     { OutputNodes.t : [10 x *]
MPI Rank 0:       rn3_3.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 0:       rn3_3.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_3.y : [8 x 8 x 64 x *] (gradient) }
MPI Rank 0:     { OutputNodes.W : [10 x 1 x 1 x 64] (gradient)
MPI Rank 0:       OutputNodes.z : [10 x *] (gradient) }
MPI Rank 0:     { OutputNodes.t : [10 x *] (gradient)
MPI Rank 0:       rn3_2.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 0:       rn3_2.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 0:       rn3_2.y : [8 x 8 x 64 x *] (gradient) }
MPI Rank 0:     { pool : [1 x 1 x 64 x *] (gradient)
MPI Rank 0:       rn3_3.c2.W : [64 x 576] (gradient) }
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Training 269914 parameters in 63 out of 63 parameter tensors and 137 nodes with gradient:
MPI Rank 0: 
MPI Rank 0:     Node 'OutputNodes.W' (LearnableParameter operation) : [10 x 1 x 1 x 64]
MPI Rank 0:     Node 'OutputNodes.b' (LearnableParameter operation) : [10]
MPI Rank 0:     Node 'conv1.c.W' (LearnableParameter operation) : [16 x 27]
MPI Rank 0:     Node 'conv1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'conv1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_1.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 0:     Node 'rn1_1.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_1.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_1.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 0:     Node 'rn1_1.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_1.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_2.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 0:     Node 'rn1_2.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_2.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_2.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 0:     Node 'rn1_2.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_2.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_3.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 0:     Node 'rn1_3.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_3.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_3.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 0:     Node 'rn1_3.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn1_3.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 0:     Node 'rn2_1.c1.c.W' (LearnableParameter operation) : [32 x 144]
MPI Rank 0:     Node 'rn2_1.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_1.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_1.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 0:     Node 'rn2_1.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_1.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_1.c_proj.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_1.c_proj.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_2.c1.c.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 0:     Node 'rn2_2.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_2.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_2.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 0:     Node 'rn2_2.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_2.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_3.c1.c.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 0:     Node 'rn2_3.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_3.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_3.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 0:     Node 'rn2_3.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn2_3.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 0:     Node 'rn3_1.c1.c.W' (LearnableParameter operation) : [64 x 288]
MPI Rank 0:     Node 'rn3_1.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_1.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_1.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 0:     Node 'rn3_1.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_1.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_1.c_proj.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_1.c_proj.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_2.c1.c.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 0:     Node 'rn3_2.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_2.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_2.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 0:     Node 'rn3_2.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_2.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_3.c1.c.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 0:     Node 'rn3_3.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_3.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_3.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 0:     Node 'rn3_3.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 0:     Node 'rn3_3.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 0: 
MPI Rank 0: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 0: 
MPI Rank 0: Starting Epoch 1: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 0: 
MPI Rank 0: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0:  Epoch[ 1 of 10]-Minibatch[   1-   1]: CE = 2.30191588 * 128; Err = 0.86718750 * 128; time = 3.0082s; samplesPerSecond = 42.6
MPI Rank 0:  Epoch[ 1 of 10]-Minibatch[   2-  10]: CE = 2.95862540 * 1152; Err = 0.88888889 * 1152; time = 0.8342s; samplesPerSecond = 1381.0
MPI Rank 0:  Epoch[ 1 of 10]-Minibatch[  11-  20]: CE = 2.21424580 * 1280; Err = 0.84296875 * 1280; time = 0.9275s; samplesPerSecond = 1380.0
MPI Rank 0:  Epoch[ 1 of 10]-Minibatch[  21-  30]: CE = 2.09339790 * 1280; Err = 0.80546875 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 0:  Epoch[ 1 of 10]-Minibatch[  31-  40]: CE = 2.07756271 * 1280; Err = 0.79609375 * 1280; time = 0.9267s; samplesPerSecond = 1381.2
MPI Rank 0:  Epoch[ 1 of 10]-Minibatch[  41-  50]: CE = 1.98548126 * 1280; Err = 0.78125000 * 1280; time = 0.9268s; samplesPerSecond = 1381.0
MPI Rank 0:  Epoch[ 1 of 10]-Minibatch[  51-  60]: CE = 2.01585770 * 1280; Err = 0.78671875 * 1280; time = 0.9263s; samplesPerSecond = 1381.9
MPI Rank 0:  Epoch[ 1 of 10]-Minibatch[  61-  70]: CE = 1.96942749 * 1280; Err = 0.75859375 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 0:  Epoch[ 1 of 10]-Minibatch[  71-  80]: CE = 1.90981750 * 1280; Err = 0.74453125 * 1280; time = 0.9271s; samplesPerSecond = 1380.6
MPI Rank 0:  Epoch[ 1 of 10]-Minibatch[  81-  90]: CE = 1.86671600 * 1280; Err = 0.72890625 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 0: Finished Epoch[ 1 of 10]: [Training] CE = 2.09439328 * 12500; Err = 0.78536000 * 12500; totalSamplesSeen = 12500; learningRatePerSample = 0.0040000002; epochTime=11.9864s
MPI Rank 0: Final Results: Minibatch[1-20]: CE = 2.15033085 * 10000; perplexity = 8.58769914; Err = 0.75900000 * 10000
MPI Rank 0: Finished Epoch[ 1 of 10]: [Validate] CE = 2.15033085 * 10000; Err = 0.75900000 * 10000
MPI Rank 0: SGD: Saving checkpoint model './Output-4gpu-take1/Models/03_ResNet.1'
MPI Rank 0: 
MPI Rank 0: Starting Epoch 2: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 0: 
MPI Rank 0: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0:  Epoch[ 2 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.80672264 * 128; Err = 0.67968750 * 128; time = 0.0960s; samplesPerSecond = 1332.7
MPI Rank 0:  Epoch[ 2 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.87494702 * 1152; Err = 0.71875000 * 1152; time = 0.8327s; samplesPerSecond = 1383.5
MPI Rank 0:  Epoch[ 2 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.86786251 * 1280; Err = 0.72031250 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 0:  Epoch[ 2 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.84436455 * 1280; Err = 0.70468750 * 1280; time = 0.9263s; samplesPerSecond = 1381.8
MPI Rank 0:  Epoch[ 2 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.81489296 * 1280; Err = 0.69765625 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 0:  Epoch[ 2 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.74333801 * 1280; Err = 0.65546875 * 1280; time = 0.9273s; samplesPerSecond = 1380.4
MPI Rank 0:  Epoch[ 2 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.82026062 * 1280; Err = 0.69531250 * 1280; time = 0.9269s; samplesPerSecond = 1381.0
MPI Rank 0:  Epoch[ 2 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.73961182 * 1280; Err = 0.66406250 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 0:  Epoch[ 2 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.79413681 * 1280; Err = 0.68828125 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 0:  Epoch[ 2 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.76722870 * 1280; Err = 0.66484375 * 1280; time = 0.9267s; samplesPerSecond = 1381.2
MPI Rank 0: Finished Epoch[ 2 of 10]: [Training] CE = 1.79953281 * 12500; Err = 0.68760000 * 12500; totalSamplesSeen = 25000; learningRatePerSample = 0.0040000002; epochTime=9.06063s
MPI Rank 0: Final Results: Minibatch[1-20]: CE = 2.07165112 * 10000; perplexity = 7.93791874; Err = 0.73000000 * 10000
MPI Rank 0: Finished Epoch[ 2 of 10]: [Validate] CE = 2.07165112 * 10000; Err = 0.73000000 * 10000
MPI Rank 0: SGD: Saving checkpoint model './Output-4gpu-take1/Models/03_ResNet.2'
MPI Rank 0: 
MPI Rank 0: Starting Epoch 3: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 0: 
MPI Rank 0: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0:  Epoch[ 3 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.66121686 * 128; Err = 0.56250000 * 128; time = 0.0921s; samplesPerSecond = 1390.3
MPI Rank 0:  Epoch[ 3 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.69534067 * 1152; Err = 0.65625000 * 1152; time = 0.8330s; samplesPerSecond = 1382.9
MPI Rank 0:  Epoch[ 3 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.73706722 * 1280; Err = 0.65468750 * 1280; time = 0.9254s; samplesPerSecond = 1383.3
MPI Rank 0:  Epoch[ 3 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.65950851 * 1280; Err = 0.64218750 * 1280; time = 0.9272s; samplesPerSecond = 1380.4
MPI Rank 0:  Epoch[ 3 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.71125565 * 1280; Err = 0.65156250 * 1280; time = 0.9277s; samplesPerSecond = 1379.7
MPI Rank 0:  Epoch[ 3 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.62558823 * 1280; Err = 0.62031250 * 1280; time = 0.9270s; samplesPerSecond = 1380.9
MPI Rank 0:  Epoch[ 3 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.66951523 * 1280; Err = 0.61718750 * 1280; time = 0.9266s; samplesPerSecond = 1381.4
MPI Rank 0:  Epoch[ 3 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.56959457 * 1280; Err = 0.58984375 * 1280; time = 0.9268s; samplesPerSecond = 1381.1
MPI Rank 0:  Epoch[ 3 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.58444366 * 1280; Err = 0.58828125 * 1280; time = 0.9266s; samplesPerSecond = 1381.4
MPI Rank 0:  Epoch[ 3 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.65196075 * 1280; Err = 0.60703125 * 1280; time = 0.9272s; samplesPerSecond = 1380.5
MPI Rank 0: Finished Epoch[ 3 of 10]: [Training] CE = 1.64867359 * 12500; Err = 0.62080000 * 12500; totalSamplesSeen = 37500; learningRatePerSample = 0.0040000002; epochTime=9.06005s
MPI Rank 0: Final Results: Minibatch[1-20]: CE = 1.94108959 * 10000; perplexity = 6.96633727; Err = 0.67580000 * 10000
MPI Rank 0: Finished Epoch[ 3 of 10]: [Validate] CE = 1.94108959 * 10000; Err = 0.67580000 * 10000
MPI Rank 0: SGD: Saving checkpoint model './Output-4gpu-take1/Models/03_ResNet.3'
MPI Rank 0: 
MPI Rank 0: Starting Epoch 4: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 0: 
MPI Rank 0: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0:  Epoch[ 4 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.60635436 * 128; Err = 0.60156250 * 128; time = 0.0922s; samplesPerSecond = 1387.8
MPI Rank 0:  Epoch[ 4 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.63301716 * 1152; Err = 0.59461806 * 1152; time = 0.8333s; samplesPerSecond = 1382.4
MPI Rank 0:  Epoch[ 4 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.55664997 * 1280; Err = 0.57578125 * 1280; time = 0.9265s; samplesPerSecond = 1381.6
MPI Rank 0:  Epoch[ 4 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.54824505 * 1280; Err = 0.58281250 * 1280; time = 0.9274s; samplesPerSecond = 1380.2
MPI Rank 0:  Epoch[ 4 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.53595314 * 1280; Err = 0.58046875 * 1280; time = 0.9274s; samplesPerSecond = 1380.3
MPI Rank 0:  Epoch[ 4 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.53677750 * 1280; Err = 0.57578125 * 1280; time = 0.9263s; samplesPerSecond = 1381.8
MPI Rank 0:  Epoch[ 4 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.48772659 * 1280; Err = 0.58750000 * 1280; time = 0.9268s; samplesPerSecond = 1381.1
MPI Rank 0:  Epoch[ 4 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.50717392 * 1280; Err = 0.56015625 * 1280; time = 0.9263s; samplesPerSecond = 1381.9
MPI Rank 0:  Epoch[ 4 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.47447586 * 1280; Err = 0.54687500 * 1280; time = 0.9269s; samplesPerSecond = 1380.9
MPI Rank 0:  Epoch[ 4 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.44228516 * 1280; Err = 0.52656250 * 1280; time = 0.9273s; samplesPerSecond = 1380.4
MPI Rank 0: Finished Epoch[ 4 of 10]: [Training] CE = 1.52301875 * 12500; Err = 0.56920000 * 12500; totalSamplesSeen = 50000; learningRatePerSample = 0.0040000002; epochTime=9.06078s
MPI Rank 0: Final Results: Minibatch[1-20]: CE = 1.47660469 * 10000; perplexity = 4.37805555; Err = 0.55010000 * 10000
MPI Rank 0: Finished Epoch[ 4 of 10]: [Validate] CE = 1.47660469 * 10000; Err = 0.55010000 * 10000
MPI Rank 0: SGD: Saving checkpoint model './Output-4gpu-take1/Models/03_ResNet.4'
MPI Rank 0: 
MPI Rank 0: Starting Epoch 5: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 0: 
MPI Rank 0: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0:  Epoch[ 5 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.46888924 * 128; Err = 0.55468750 * 128; time = 0.0922s; samplesPerSecond = 1388.5
MPI Rank 0:  Epoch[ 5 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.44236840 * 1152; Err = 0.55729167 * 1152; time = 0.8335s; samplesPerSecond = 1382.2
MPI Rank 0:  Epoch[ 5 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.45803280 * 1280; Err = 0.53906250 * 1280; time = 0.9261s; samplesPerSecond = 1382.1
MPI Rank 0:  Epoch[ 5 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.38523750 * 1280; Err = 0.51015625 * 1280; time = 0.9265s; samplesPerSecond = 1381.5
MPI Rank 0:  Epoch[ 5 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.40540276 * 1280; Err = 0.53515625 * 1280; time = 0.9263s; samplesPerSecond = 1381.8
MPI Rank 0:  Epoch[ 5 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.29166489 * 1280; Err = 0.46484375 * 1280; time = 0.9267s; samplesPerSecond = 1381.2
MPI Rank 0:  Epoch[ 5 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.38113708 * 1280; Err = 0.50937500 * 1280; time = 0.9273s; samplesPerSecond = 1380.4
MPI Rank 0:  Epoch[ 5 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.38459244 * 1280; Err = 0.50000000 * 1280; time = 0.9277s; samplesPerSecond = 1379.8
MPI Rank 0:  Epoch[ 5 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.38938141 * 1280; Err = 0.52734375 * 1280; time = 0.9275s; samplesPerSecond = 1380.0
MPI Rank 0:  Epoch[ 5 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.30604248 * 1280; Err = 0.45703125 * 1280; time = 0.9270s; samplesPerSecond = 1380.7
MPI Rank 0: Finished Epoch[ 5 of 10]: [Training] CE = 1.38046844 * 12500; Err = 0.50944000 * 12500; totalSamplesSeen = 62500; learningRatePerSample = 0.0040000002; epochTime=9.06103s
MPI Rank 0: Final Results: Minibatch[1-20]: CE = 1.43932121 * 10000; perplexity = 4.21783182; Err = 0.52100000 * 10000
MPI Rank 0: Finished Epoch[ 5 of 10]: [Validate] CE = 1.43932121 * 10000; Err = 0.52100000 * 10000
MPI Rank 0: SGD: Saving checkpoint model './Output-4gpu-take1/Models/03_ResNet.5'
MPI Rank 0: 
MPI Rank 0: Starting Epoch 6: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 0: 
MPI Rank 0: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0:  Epoch[ 6 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.18158245 * 128; Err = 0.44531250 * 128; time = 0.0921s; samplesPerSecond = 1389.6
MPI Rank 0:  Epoch[ 6 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.31618871 * 1152; Err = 0.47916667 * 1152; time = 0.8343s; samplesPerSecond = 1380.8
MPI Rank 0:  Epoch[ 6 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.29430084 * 1280; Err = 0.48671875 * 1280; time = 0.9268s; samplesPerSecond = 1381.1
MPI Rank 0:  Epoch[ 6 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.28902531 * 1280; Err = 0.46484375 * 1280; time = 0.9273s; samplesPerSecond = 1380.4
MPI Rank 0:  Epoch[ 6 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.31750870 * 1280; Err = 0.48125000 * 1280; time = 0.9278s; samplesPerSecond = 1379.7
MPI Rank 0:  Epoch[ 6 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.27460480 * 1280; Err = 0.46796875 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 0:  Epoch[ 6 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.35238419 * 1280; Err = 0.49921875 * 1280; time = 0.9279s; samplesPerSecond = 1379.5
MPI Rank 0:  Epoch[ 6 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.32516556 * 1280; Err = 0.47343750 * 1280; time = 0.9273s; samplesPerSecond = 1380.4
MPI Rank 0:  Epoch[ 6 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.20724182 * 1280; Err = 0.42812500 * 1280; time = 0.9284s; samplesPerSecond = 1378.7
MPI Rank 0:  Epoch[ 6 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.26076202 * 1280; Err = 0.46171875 * 1280; time = 0.9281s; samplesPerSecond = 1379.2
MPI Rank 0: Finished Epoch[ 6 of 10]: [Training] CE = 1.28948500 * 12500; Err = 0.47032000 * 12500; totalSamplesSeen = 75000; learningRatePerSample = 0.0040000002; epochTime=9.06843s
MPI Rank 0: Final Results: Minibatch[1-20]: CE = 1.48801055 * 10000; perplexity = 4.42827693; Err = 0.49200000 * 10000
MPI Rank 0: Finished Epoch[ 6 of 10]: [Validate] CE = 1.48801055 * 10000; Err = 0.49200000 * 10000
MPI Rank 0: SGD: Saving checkpoint model './Output-4gpu-take1/Models/03_ResNet.6'
MPI Rank 0: 
MPI Rank 0: Starting Epoch 7: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 0: 
MPI Rank 0: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0:  Epoch[ 7 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.23044312 * 128; Err = 0.42968750 * 128; time = 0.0925s; samplesPerSecond = 1383.3
MPI Rank 0:  Epoch[ 7 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.23200701 * 1152; Err = 0.44965278 * 1152; time = 0.8336s; samplesPerSecond = 1381.9
MPI Rank 0:  Epoch[ 7 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.21315441 * 1280; Err = 0.44375000 * 1280; time = 0.9281s; samplesPerSecond = 1379.2
MPI Rank 0:  Epoch[ 7 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.22635155 * 1280; Err = 0.43359375 * 1280; time = 0.9265s; samplesPerSecond = 1381.6
MPI Rank 0:  Epoch[ 7 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.26652603 * 1280; Err = 0.45156250 * 1280; time = 0.9277s; samplesPerSecond = 1379.7
MPI Rank 0:  Epoch[ 7 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.22471390 * 1280; Err = 0.44609375 * 1280; time = 0.9283s; samplesPerSecond = 1378.9
MPI Rank 0:  Epoch[ 7 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.16208839 * 1280; Err = 0.44531250 * 1280; time = 0.9280s; samplesPerSecond = 1379.3
MPI Rank 0:  Epoch[ 7 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.22602615 * 1280; Err = 0.44765625 * 1280; time = 0.9285s; samplesPerSecond = 1378.6
MPI Rank 0:  Epoch[ 7 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.25589066 * 1280; Err = 0.43906250 * 1280; time = 0.9285s; samplesPerSecond = 1378.5
MPI Rank 0:  Epoch[ 7 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.15966721 * 1280; Err = 0.42109375 * 1280; time = 0.9282s; samplesPerSecond = 1378.9
MPI Rank 0: Finished Epoch[ 7 of 10]: [Training] CE = 1.21213539 * 12500; Err = 0.44040000 * 12500; totalSamplesSeen = 87500; learningRatePerSample = 0.0040000002; epochTime=9.07199s
MPI Rank 0: Final Results: Minibatch[1-20]: CE = 1.52861367 * 10000; perplexity = 4.61177892; Err = 0.53680000 * 10000
MPI Rank 0: Finished Epoch[ 7 of 10]: [Validate] CE = 1.52861367 * 10000; Err = 0.53680000 * 10000
MPI Rank 0: SGD: Saving checkpoint model './Output-4gpu-take1/Models/03_ResNet.7'
MPI Rank 0: 
MPI Rank 0: Starting Epoch 8: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 0: 
MPI Rank 0: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0:  Epoch[ 8 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.06810570 * 128; Err = 0.42187500 * 128; time = 0.0951s; samplesPerSecond = 1346.6
MPI Rank 0:  Epoch[ 8 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.21366225 * 1152; Err = 0.43836806 * 1152; time = 0.8328s; samplesPerSecond = 1383.3
MPI Rank 0:  Epoch[ 8 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.16418133 * 1280; Err = 0.43671875 * 1280; time = 0.9256s; samplesPerSecond = 1382.9
MPI Rank 0:  Epoch[ 8 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.19225483 * 1280; Err = 0.43203125 * 1280; time = 0.9258s; samplesPerSecond = 1382.5
MPI Rank 0:  Epoch[ 8 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.14202347 * 1280; Err = 0.41640625 * 1280; time = 0.9259s; samplesPerSecond = 1382.5
MPI Rank 0:  Epoch[ 8 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.15065842 * 1280; Err = 0.41718750 * 1280; time = 0.9278s; samplesPerSecond = 1379.7
MPI Rank 0:  Epoch[ 8 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.08536606 * 1280; Err = 0.38437500 * 1280; time = 0.9278s; samplesPerSecond = 1379.6
MPI Rank 0:  Epoch[ 8 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.11576691 * 1280; Err = 0.40859375 * 1280; time = 0.9280s; samplesPerSecond = 1379.3
MPI Rank 0:  Epoch[ 8 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.10264664 * 1280; Err = 0.38593750 * 1280; time = 0.9268s; samplesPerSecond = 1381.1
MPI Rank 0:  Epoch[ 8 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.11880798 * 1280; Err = 0.41484375 * 1280; time = 0.9301s; samplesPerSecond = 1376.3
MPI Rank 0: Finished Epoch[ 8 of 10]: [Training] CE = 1.13186266 * 12500; Err = 0.41144000 * 12500; totalSamplesSeen = 100000; learningRatePerSample = 0.0040000002; epochTime=9.06685s
MPI Rank 0: Final Results: Minibatch[1-20]: CE = 1.14729181 * 10000; perplexity = 3.14965148; Err = 0.40700000 * 10000
MPI Rank 0: Finished Epoch[ 8 of 10]: [Validate] CE = 1.14729181 * 10000; Err = 0.40700000 * 10000
MPI Rank 0: SGD: Saving checkpoint model './Output-4gpu-take1/Models/03_ResNet.8'
MPI Rank 0: 
MPI Rank 0: Starting Epoch 9: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 0: 
MPI Rank 0: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0:  Epoch[ 9 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.09980369 * 128; Err = 0.42968750 * 128; time = 0.1055s; samplesPerSecond = 1212.9
MPI Rank 0:  Epoch[ 9 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.04448152 * 1152; Err = 0.36718750 * 1152; time = 0.8348s; samplesPerSecond = 1380.0
MPI Rank 0:  Epoch[ 9 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.10889950 * 1280; Err = 0.40078125 * 1280; time = 0.9301s; samplesPerSecond = 1376.1
MPI Rank 0:  Epoch[ 9 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.15475712 * 1280; Err = 0.43046875 * 1280; time = 0.9294s; samplesPerSecond = 1377.3
MPI Rank 0:  Epoch[ 9 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.11322556 * 1280; Err = 0.40234375 * 1280; time = 0.9311s; samplesPerSecond = 1374.7
MPI Rank 0:  Epoch[ 9 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.06950226 * 1280; Err = 0.37890625 * 1280; time = 0.9319s; samplesPerSecond = 1373.6
MPI Rank 0:  Epoch[ 9 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.05474472 * 1280; Err = 0.39921875 * 1280; time = 0.9310s; samplesPerSecond = 1374.9
MPI Rank 0:  Epoch[ 9 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.06205673 * 1280; Err = 0.38359375 * 1280; time = 0.9316s; samplesPerSecond = 1374.0
MPI Rank 0:  Epoch[ 9 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.08296280 * 1280; Err = 0.37968750 * 1280; time = 0.9308s; samplesPerSecond = 1375.2
MPI Rank 0:  Epoch[ 9 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.03062363 * 1280; Err = 0.38125000 * 1280; time = 0.9310s; samplesPerSecond = 1374.9
MPI Rank 0: Finished Epoch[ 9 of 10]: [Training] CE = 1.07782617 * 12500; Err = 0.39208000 * 12500; totalSamplesSeen = 112500; learningRatePerSample = 0.0040000002; epochTime=9.11043s
MPI Rank 0: Final Results: Minibatch[1-20]: CE = 1.32417279 * 10000; perplexity = 3.75907451; Err = 0.45900000 * 10000
MPI Rank 0: Finished Epoch[ 9 of 10]: [Validate] CE = 1.32417279 * 10000; Err = 0.45900000 * 10000
MPI Rank 0: SGD: Saving checkpoint model './Output-4gpu-take1/Models/03_ResNet.9'
MPI Rank 0: 
MPI Rank 0: Starting Epoch 10: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 0: 
MPI Rank 0: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0:  Epoch[10 of 10]-Minibatch[   1-   1, 4.44%]: CE = 0.97766179 * 128; Err = 0.35156250 * 128; time = 0.1230s; samplesPerSecond = 1040.8
MPI Rank 0:  Epoch[10 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.07474309 * 1152; Err = 0.39322917 * 1152; time = 0.8373s; samplesPerSecond = 1375.9
MPI Rank 0:  Epoch[10 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.03160925 * 1280; Err = 0.36718750 * 1280; time = 0.9318s; samplesPerSecond = 1373.7
MPI Rank 0:  Epoch[10 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.03240013 * 1280; Err = 0.37031250 * 1280; time = 0.9291s; samplesPerSecond = 1377.7
MPI Rank 0:  Epoch[10 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.05029831 * 1280; Err = 0.35703125 * 1280; time = 0.9330s; samplesPerSecond = 1371.9
MPI Rank 0:  Epoch[10 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.01784325 * 1280; Err = 0.36640625 * 1280; time = 0.9332s; samplesPerSecond = 1371.6
MPI Rank 0:  Epoch[10 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.02696228 * 1280; Err = 0.36171875 * 1280; time = 0.9324s; samplesPerSecond = 1372.7
MPI Rank 0:  Epoch[10 of 10]-Minibatch[  61-  70, 311.11%]: CE = 0.96386833 * 1280; Err = 0.33437500 * 1280; time = 0.9295s; samplesPerSecond = 1377.1
MPI Rank 0:  Epoch[10 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.01814880 * 1280; Err = 0.36015625 * 1280; time = 0.9322s; samplesPerSecond = 1373.0
MPI Rank 0:  Epoch[10 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.04713898 * 1280; Err = 0.38750000 * 1280; time = 0.9309s; samplesPerSecond = 1375.0
MPI Rank 0: Finished Epoch[10 of 10]: [Training] CE = 1.02451867 * 12500; Err = 0.36608000 * 12500; totalSamplesSeen = 125000; learningRatePerSample = 0.0040000002; epochTime=9.13779s
MPI Rank 0: Final Results: Minibatch[1-20]: CE = 1.31393505 * 10000; perplexity = 3.72078643; Err = 0.44800000 * 10000
MPI Rank 0: Finished Epoch[10 of 10]: [Validate] CE = 1.31393505 * 10000; Err = 0.44800000 * 10000
MPI Rank 0: SGD: Saving checkpoint model './Output-4gpu-take1/Models/03_ResNet'
MPI Rank 0: 
MPI Rank 0: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: COMPLETED.
MPI Rank 0: ~MPIWrapper
MPI Rank 1: Configuration After Processing and Variable Resolution:
MPI Rank 1: 
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:asyncBuffer=false
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:command=Train
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:ConfigDir=.
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:configName=4gpu-take1
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:DataDir=.
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:deviceId=auto
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:epochSize=10
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:imageLayout=cudnn
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:initOnCPUOnly=true
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:makeMode=true
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:minibatch=512
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:minibatchSize=128
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:ModelDir=./Output-4gpu-take1/Models
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:ndlMacros=./Macros.ndl
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:OutputDir=./Output-4gpu-take1
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:parallelizationMethod=DataParallelASGD
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:parallelTrain=true
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:precision=float
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:prefetch=true
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:Proj16to32Filename=./16to32.txt
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:Proj32to64Filename=./32to64.txt
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:RootDir=.
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:stderr=./Output-4gpu-take1/03_ResNet
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:Test=[
MPI Rank 1:     action = "test"
MPI Rank 1:     modelPath = "./Output-4gpu-take1/Models/03_ResNet"
MPI Rank 1:     minibatchSize = 256
MPI Rank 1:     reader = [
MPI Rank 1:         readerType = "ImageReader"
MPI Rank 1:         file = "./cifar-10-batches-py/test_map.txt"
MPI Rank 1:         randomize = "none"
MPI Rank 1:         features = [
MPI Rank 1:             width = 32
MPI Rank 1:             height = 32
MPI Rank 1:             channels = 3
MPI Rank 1:             cropType = "center"
MPI Rank 1:             cropRatio = 1
MPI Rank 1:             jitterType = "uniRatio"
MPI Rank 1:             interpolations = "linear"
MPI Rank 1:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 1:         ]
MPI Rank 1:         labels = [
MPI Rank 1:             labelDim = 10
MPI Rank 1:         ]
MPI Rank 1:     ]    
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:traceLevel=1
MPI Rank 1: configparameters: 03_ResNet-parallel.cntk:Train=[
MPI Rank 1:     action = "train"
MPI Rank 1:     modelPath = "./Output-4gpu-take1/Models/03_ResNet"
MPI Rank 1:      NDLNetworkBuilder = [
MPI Rank 1:         networkDescription = "./03_ResNet.ndl"
MPI Rank 1:     ]
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize = 0
MPI Rank 1:         minibatchSize = 512
MPI Rank 1:         learningRatesPerSample = 0.004*80:0.0004*40:0.00004
MPI Rank 1:         momentumPerMB = 0
MPI Rank 1:         maxEpochs = 10
MPI Rank 1:         L2RegWeight = 0.0001
MPI Rank 1:         dropoutRate = 0
MPI Rank 1:         perfTraceLevel = 0
MPI Rank 1:         firstMBsToShowResult = 1
MPI Rank 1:         numMBsToShowResult = 10
MPI Rank 1:         ParallelTrain = [
MPI Rank 1:             parallelizationMethod = DataParallelASGD
MPI Rank 1:             distributedMBReading = "true"
MPI Rank 1:             parallelizationStartEpoch = 1
MPI Rank 1:             DataParallelSGD = [
MPI Rank 1:                 gradientBits = 32
MPI Rank 1:                 useBufferedAsyncGradientAggregation = false
MPI Rank 1:             ]
MPI Rank 1:             ModelAveragingSGD = [
MPI Rank 1:                 blockSizePerWorker = 128
MPI Rank 1:             ]
MPI Rank 1:             DataParallelASGD = [
MPI Rank 1:                 syncPeriod = 128
MPI Rank 1:                 usePipeline = false
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1:     ]
MPI Rank 1:     reader = [
MPI Rank 1:         readerType = "ImageReader"
MPI Rank 1:         file = "./cifar-10-batches-py/train_map.txt"
MPI Rank 1:         randomize = "auto"
MPI Rank 1:         features = [
MPI Rank 1:             width = 32
MPI Rank 1:             height = 32
MPI Rank 1:             channels = 3
MPI Rank 1:             cropType = "random"
MPI Rank 1:             cropRatio = 0.8
MPI Rank 1:             jitterType = "uniRatio"
MPI Rank 1:             interpolations = "linear"
MPI Rank 1:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 1:         ]
MPI Rank 1:         labels = [
MPI Rank 1:             labelDim = 10
MPI Rank 1:         ]
MPI Rank 1:     ]
MPI Rank 1:     cvReader = [
MPI Rank 1:         readerType = "ImageReader"
MPI Rank 1:         file = "./cifar-10-batches-py/test_map.txt"
MPI Rank 1:         randomize = "none"
MPI Rank 1:         features = [
MPI Rank 1:             width = 32
MPI Rank 1:             height = 32
MPI Rank 1:             channels = 3
MPI Rank 1:             cropType = "center"
MPI Rank 1:             cropRatio = 1
MPI Rank 1:             jitterType = "uniRatio"
MPI Rank 1:             interpolations = "linear"
MPI Rank 1:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 1:         ]
MPI Rank 1:         labels = [
MPI Rank 1:             labelDim = 10
MPI Rank 1:         ]
MPI Rank 1:     ]    
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: Commands: Train
MPI Rank 1: precision = "float"
MPI Rank 1: 
MPI Rank 1: ##############################################################################
MPI Rank 1: #                                                                            #
MPI Rank 1: # Train command (train action)                                               #
MPI Rank 1: #                                                                            #
MPI Rank 1: ##############################################################################
MPI Rank 1: 
MPI Rank 1: LockDevice: Failed to lock GPU 7 for exclusive use.
MPI Rank 1: 
MPI Rank 1: Creating virgin network.
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: SetGaussianRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 1: 
MPI Rank 1: OutputNodes.t Times operation: For legacy compatibility, the sample layout of left input (OutputNodes.W LearnableParameter operation) was patched to [10 x 1 x 1 x 64] (from [10 x 64])
MPI Rank 1: conv1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 16, Kernel: 3 x 3 x 3, Map: 1 x 1 x 16, Stride: 1 x 1 x 3, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn1_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn1_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn1_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn1_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn1_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn1_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn2_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 16 x 16 x 32, Kernel: 3 x 3 x 16, Map: 1 x 1 x 32, Stride: 2 x 2 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn2_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn2_1.c_proj.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 16 x 16 x 32, Kernel: 1 x 1 x 16, Map: 1 x 1 x 32, Stride: 2 x 2 x 16, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn2_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn2_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn2_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn2_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn3_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 8 x 8 x 64, Kernel: 3 x 3 x 32, Map: 1 x 1 x 64, Stride: 2 x 2 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn3_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn3_1.c_proj.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 8 x 8 x 64, Kernel: 1 x 1 x 32, Map: 1 x 1 x 64, Stride: 2 x 2 x 32, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn3_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn3_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn3_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: rn3_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 1: Using CNTK batch normalization engine.
MPI Rank 1: pool: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 1 x 1 x 64, Kernel: 8 x 8 x 1, Map: 1, Stride: 1 x 1 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 1: 
MPI Rank 1: Model has 184 nodes. Using GPU 0.
MPI Rank 1: 
MPI Rank 1: Training criterion:   CE = CrossEntropyWithSoftmax
MPI Rank 1: Evaluation criterion: Err = ClassificationError
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing: Out of 321 matrices, 160 are shared as 62, and 161 are not shared.
MPI Rank 1: 
MPI Rank 1:     { conv1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       conv1.y : [32 x 32 x 16 x *] }
MPI Rank 1:     { conv1.c.W : [16 x 27] (gradient)
MPI Rank 1:       rn1_1.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_1.c1.y : [32 x 32 x 16 x *] }
MPI Rank 1:     { conv1.c.c.b : [16 x 1] (gradient)
MPI Rank 1:       rn1_1.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_1.c1.c.W : [16 x 144] (gradient)
MPI Rank 1:       rn1_1.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_1.p : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_1.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_1.y : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_1.c2.W : [16 x 144] (gradient)
MPI Rank 1:       rn1_2.c1.c.c.c : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_1.c2.c.sc : [16 x 1] (gradient)
MPI Rank 1:       rn1_1.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 1:     { conv1.c.c.sc : [16 x 1] (gradient)
MPI Rank 1:       conv1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_2.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_2.c1.y : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_1.c2.c.b : [16 x 1] (gradient)
MPI Rank 1:       rn1_2.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_2.c1.c.W : [16 x 144] (gradient)
MPI Rank 1:       rn1_2.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_2.p : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_2.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_2.y : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_2.c2.W : [16 x 144] (gradient)
MPI Rank 1:       rn1_3.c1.c.c.c : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_2.c2.c.sc : [16 x 1] (gradient)
MPI Rank 1:       rn1_2.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 1:     { rn1_1.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 1:       rn1_1.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_3.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_3.c1.y : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_2.c2.c.b : [16 x 1] (gradient)
MPI Rank 1:       rn1_3.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_3.c1.c.W : [16 x 144] (gradient)
MPI Rank 1:       rn1_3.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_3.p : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_3.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_3.y : [32 x 32 x 16 x *] }
MPI Rank 1:     { rn1_3.c2.W : [16 x 144] (gradient)
MPI Rank 1:       rn2_1.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn1_3.c2.c.sc : [16 x 1] (gradient)
MPI Rank 1:       rn1_3.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 1:     { rn1_2.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 1:       rn1_2.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_2.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn2_1.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_1.c1.y : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn1_3.c2.c.b : [16 x 1] (gradient)
MPI Rank 1:       rn2_1.c2.c.c : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_1.c2.c.sc : [32 x 1] (gradient)
MPI Rank 1:       rn2_1.c_proj.c : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_1.c1.c.W : [32 x 144] (gradient)
MPI Rank 1:       rn2_1.c2.c.c : [16 x 16 x 32 x *] (gradient) }
MPI Rank 1:     { rn2_1.c2.c.b : [32 x 1] (gradient)
MPI Rank 1:       rn2_1.c_proj.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_1.p : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_1.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_1.y : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_1.c2.W : [32 x 288] (gradient)
MPI Rank 1:       rn2_2.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_1.c_proj.sc : [32 x 1] (gradient)
MPI Rank 1:       rn2_1.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 1:     { rn1_3.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 1:       rn1_3.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 1:       rn1_3.y : [32 x 32 x 16 x *] (gradient) }
MPI Rank 1:     { rn2_1.c_proj.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_2.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_2.c1.y : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_2.c1.c.W : [32 x 288] (gradient)
MPI Rank 1:       rn2_2.c2.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_2.p : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_2.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_2.y : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_2.c2.W : [32 x 288] (gradient)
MPI Rank 1:       rn2_3.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_2.c2.c.sc : [32 x 1] (gradient)
MPI Rank 1:       rn2_2.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 1:     { rn2_1.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 1:       rn2_1.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_3.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_3.c1.y : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_2.c2.c.b : [32 x 1] (gradient)
MPI Rank 1:       rn2_3.c2.c.c : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_3.c1.c.W : [32 x 288] (gradient)
MPI Rank 1:       rn2_3.c2.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_3.p : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_3.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_3.y : [16 x 16 x 32 x *] }
MPI Rank 1:     { rn2_3.c2.W : [32 x 288] (gradient)
MPI Rank 1:       rn3_1.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn2_3.c2.c.sc : [32 x 1] (gradient)
MPI Rank 1:       rn2_3.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 1:     { rn2_2.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 1:       rn2_2.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_2.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn3_1.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_1.c1.y : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn2_3.c2.c.b : [32 x 1] (gradient)
MPI Rank 1:       rn3_1.c2.c.c : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_1.c2.c.sc : [64 x 1] (gradient)
MPI Rank 1:       rn3_1.c_proj.c : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_1.c1.c.W : [64 x 288] (gradient)
MPI Rank 1:       rn3_1.c2.c.c : [8 x 8 x 64 x *] (gradient) }
MPI Rank 1:     { rn3_1.c2.c.b : [64 x 1] (gradient)
MPI Rank 1:       rn3_1.c_proj.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_1.p : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_1.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_1.y : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_1.c2.W : [64 x 576] (gradient)
MPI Rank 1:       rn3_2.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_1.c_proj.sc : [64 x 1] (gradient)
MPI Rank 1:       rn3_1.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 1:     { rn2_3.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 1:       rn2_3.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 1:       rn2_3.y : [16 x 16 x 32 x *] (gradient) }
MPI Rank 1:     { rn3_1.c_proj.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_2.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_2.c1.y : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_2.c1.c.W : [64 x 576] (gradient)
MPI Rank 1:       rn3_2.c2.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_2.p : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_2.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_2.y : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_2.c2.W : [64 x 576] (gradient)
MPI Rank 1:       rn3_3.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_2.c2.c.sc : [64 x 1] (gradient)
MPI Rank 1:       rn3_2.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 1:     { rn3_1.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 1:       rn3_1.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_3.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_3.c1.y : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_2.c2.c.b : [64 x 1] (gradient)
MPI Rank 1:       rn3_3.c2.c.c : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_3.c1.c.W : [64 x 576] (gradient)
MPI Rank 1:       rn3_3.c2.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_3.p : [8 x 8 x 64 x *] }
MPI Rank 1:     { rn3_3.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_3.y : [8 x 8 x 64 x *] }
MPI Rank 1:     { pool : [1 x 1 x 64 x *]
MPI Rank 1:       rn3_3.c2.c.sc : [64 x 1] (gradient)
MPI Rank 1:       rn3_3.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 1:     { OutputNodes.t : [10 x *]
MPI Rank 1:       rn3_3.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 1:       rn3_3.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_3.y : [8 x 8 x 64 x *] (gradient) }
MPI Rank 1:     { OutputNodes.W : [10 x 1 x 1 x 64] (gradient)
MPI Rank 1:       OutputNodes.z : [10 x *] (gradient) }
MPI Rank 1:     { OutputNodes.t : [10 x *] (gradient)
MPI Rank 1:       rn3_2.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 1:       rn3_2.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 1:       rn3_2.y : [8 x 8 x 64 x *] (gradient) }
MPI Rank 1:     { pool : [1 x 1 x 64 x *] (gradient)
MPI Rank 1:       rn3_3.c2.W : [64 x 576] (gradient) }
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Training 269914 parameters in 63 out of 63 parameter tensors and 137 nodes with gradient:
MPI Rank 1: 
MPI Rank 1:     Node 'OutputNodes.W' (LearnableParameter operation) : [10 x 1 x 1 x 64]
MPI Rank 1:     Node 'OutputNodes.b' (LearnableParameter operation) : [10]
MPI Rank 1:     Node 'conv1.c.W' (LearnableParameter operation) : [16 x 27]
MPI Rank 1:     Node 'conv1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'conv1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_1.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 1:     Node 'rn1_1.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_1.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_1.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 1:     Node 'rn1_1.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_1.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_2.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 1:     Node 'rn1_2.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_2.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_2.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 1:     Node 'rn1_2.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_2.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_3.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 1:     Node 'rn1_3.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_3.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_3.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 1:     Node 'rn1_3.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn1_3.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 1:     Node 'rn2_1.c1.c.W' (LearnableParameter operation) : [32 x 144]
MPI Rank 1:     Node 'rn2_1.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_1.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_1.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 1:     Node 'rn2_1.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_1.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_1.c_proj.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_1.c_proj.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_2.c1.c.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 1:     Node 'rn2_2.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_2.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_2.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 1:     Node 'rn2_2.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_2.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_3.c1.c.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 1:     Node 'rn2_3.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_3.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_3.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 1:     Node 'rn2_3.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn2_3.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 1:     Node 'rn3_1.c1.c.W' (LearnableParameter operation) : [64 x 288]
MPI Rank 1:     Node 'rn3_1.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_1.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_1.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 1:     Node 'rn3_1.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_1.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_1.c_proj.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_1.c_proj.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_2.c1.c.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 1:     Node 'rn3_2.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_2.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_2.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 1:     Node 'rn3_2.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_2.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_3.c1.c.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 1:     Node 'rn3_3.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_3.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_3.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 1:     Node 'rn3_3.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 1:     Node 'rn3_3.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 1: 
MPI Rank 1: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 1: 
MPI Rank 1: Starting Epoch 1: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 1: 
MPI Rank 1: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1:  Epoch[ 1 of 10]-Minibatch[   1-   1]: CE = 2.29755735 * 128; Err = 0.90625000 * 128; time = 2.9711s; samplesPerSecond = 43.1
MPI Rank 1:  Epoch[ 1 of 10]-Minibatch[   2-  10]: CE = 2.79725970 * 1152; Err = 0.86545139 * 1152; time = 0.8354s; samplesPerSecond = 1379.0
MPI Rank 1:  Epoch[ 1 of 10]-Minibatch[  11-  20]: CE = 2.20739231 * 1280; Err = 0.85078125 * 1280; time = 0.9285s; samplesPerSecond = 1378.6
MPI Rank 1:  Epoch[ 1 of 10]-Minibatch[  21-  30]: CE = 2.11593285 * 1280; Err = 0.79609375 * 1280; time = 0.9288s; samplesPerSecond = 1378.1
MPI Rank 1:  Epoch[ 1 of 10]-Minibatch[  31-  40]: CE = 2.02267456 * 1280; Err = 0.76250000 * 1280; time = 0.9291s; samplesPerSecond = 1377.7
MPI Rank 1:  Epoch[ 1 of 10]-Minibatch[  41-  50]: CE = 2.03623428 * 1280; Err = 0.76718750 * 1280; time = 0.9288s; samplesPerSecond = 1378.1
MPI Rank 1:  Epoch[ 1 of 10]-Minibatch[  51-  60]: CE = 1.96046677 * 1280; Err = 0.76171875 * 1280; time = 0.9288s; samplesPerSecond = 1378.2
MPI Rank 1:  Epoch[ 1 of 10]-Minibatch[  61-  70]: CE = 1.93468170 * 1280; Err = 0.75000000 * 1280; time = 0.9284s; samplesPerSecond = 1378.7
MPI Rank 1:  Epoch[ 1 of 10]-Minibatch[  71-  80]: CE = 1.90753784 * 1280; Err = 0.73984375 * 1280; time = 0.9281s; samplesPerSecond = 1379.2
MPI Rank 1:  Epoch[ 1 of 10]-Minibatch[  81-  90]: CE = 1.87238464 * 1280; Err = 0.70000000 * 1280; time = 0.9287s; samplesPerSecond = 1378.2
MPI Rank 1: Finished Epoch[ 1 of 10]: [Training] CE = 2.07295547 * 12500; Err = 0.77264000 * 12500; totalSamplesSeen = 12500; learningRatePerSample = 0.0040000002; epochTime=11.9618s
MPI Rank 1: Final Results: Minibatch[1-20]: CE = 2.20700717 * 10000; perplexity = 9.08847535; Err = 0.75830000 * 10000
MPI Rank 1: Finished Epoch[ 1 of 10]: [Validate] CE = 2.20700717 * 10000; Err = 0.75830000 * 10000
MPI Rank 1: 
MPI Rank 1: Starting Epoch 2: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 1: 
MPI Rank 1: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1:  Epoch[ 2 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.73665619 * 128; Err = 0.66406250 * 128; time = 0.0905s; samplesPerSecond = 1414.4
MPI Rank 1:  Epoch[ 2 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.87648943 * 1152; Err = 0.72743056 * 1152; time = 0.8319s; samplesPerSecond = 1384.8
MPI Rank 1:  Epoch[ 2 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.86255035 * 1280; Err = 0.70937500 * 1280; time = 0.9259s; samplesPerSecond = 1382.5
MPI Rank 1:  Epoch[ 2 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.79900131 * 1280; Err = 0.67500000 * 1280; time = 0.9249s; samplesPerSecond = 1383.9
MPI Rank 1:  Epoch[ 2 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.82515259 * 1280; Err = 0.68671875 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 1:  Epoch[ 2 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.74746017 * 1280; Err = 0.67890625 * 1280; time = 0.9253s; samplesPerSecond = 1383.3
MPI Rank 1:  Epoch[ 2 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.73641434 * 1280; Err = 0.68984375 * 1280; time = 0.9243s; samplesPerSecond = 1384.8
MPI Rank 1:  Epoch[ 2 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.70452728 * 1280; Err = 0.65390625 * 1280; time = 0.9247s; samplesPerSecond = 1384.2
MPI Rank 1:  Epoch[ 2 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.72471848 * 1280; Err = 0.66015625 * 1280; time = 0.9255s; samplesPerSecond = 1383.0
MPI Rank 1:  Epoch[ 2 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.69060364 * 1280; Err = 0.64609375 * 1280; time = 0.9258s; samplesPerSecond = 1382.6
MPI Rank 1: Finished Epoch[ 2 of 10]: [Training] CE = 1.76711594 * 12500; Err = 0.67840000 * 12500; totalSamplesSeen = 25000; learningRatePerSample = 0.0040000002; epochTime=9.04335s
MPI Rank 1: Final Results: Minibatch[1-20]: CE = 1.83607424 * 10000; perplexity = 6.27186800; Err = 0.69310000 * 10000
MPI Rank 1: Finished Epoch[ 2 of 10]: [Validate] CE = 1.83607424 * 10000; Err = 0.69310000 * 10000
MPI Rank 1: 
MPI Rank 1: Starting Epoch 3: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 1: 
MPI Rank 1: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1:  Epoch[ 3 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.69911373 * 128; Err = 0.71875000 * 128; time = 0.0906s; samplesPerSecond = 1413.2
MPI Rank 1:  Epoch[ 3 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.64701527 * 1152; Err = 0.62673611 * 1152; time = 0.8314s; samplesPerSecond = 1385.7
MPI Rank 1:  Epoch[ 3 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.69924164 * 1280; Err = 0.65078125 * 1280; time = 0.9257s; samplesPerSecond = 1382.7
MPI Rank 1:  Epoch[ 3 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.62020454 * 1280; Err = 0.60859375 * 1280; time = 0.9257s; samplesPerSecond = 1382.8
MPI Rank 1:  Epoch[ 3 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.65986328 * 1280; Err = 0.63046875 * 1280; time = 0.9250s; samplesPerSecond = 1383.8
MPI Rank 1:  Epoch[ 3 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.63717880 * 1280; Err = 0.62578125 * 1280; time = 0.9265s; samplesPerSecond = 1381.6
MPI Rank 1:  Epoch[ 3 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.58812866 * 1280; Err = 0.59296875 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 1:  Epoch[ 3 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.58268127 * 1280; Err = 0.60156250 * 1280; time = 0.9265s; samplesPerSecond = 1381.5
MPI Rank 1:  Epoch[ 3 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.58647156 * 1280; Err = 0.59921875 * 1280; time = 0.9251s; samplesPerSecond = 1383.7
MPI Rank 1:  Epoch[ 3 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.62791290 * 1280; Err = 0.60937500 * 1280; time = 0.9263s; samplesPerSecond = 1381.8
MPI Rank 1: Finished Epoch[ 3 of 10]: [Training] CE = 1.62131984 * 12500; Err = 0.61328000 * 12500; totalSamplesSeen = 37500; learningRatePerSample = 0.0040000002; epochTime=9.04895s
MPI Rank 1: Final Results: Minibatch[1-20]: CE = 1.79555583 * 10000; perplexity = 6.02282145; Err = 0.63760000 * 10000
MPI Rank 1: Finished Epoch[ 3 of 10]: [Validate] CE = 1.79555583 * 10000; Err = 0.63760000 * 10000
MPI Rank 1: 
MPI Rank 1: Starting Epoch 4: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 1: 
MPI Rank 1: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1:  Epoch[ 4 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.57874632 * 128; Err = 0.65625000 * 128; time = 0.0907s; samplesPerSecond = 1411.7
MPI Rank 1:  Epoch[ 4 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.50253608 * 1152; Err = 0.55295139 * 1152; time = 0.8328s; samplesPerSecond = 1383.3
MPI Rank 1:  Epoch[ 4 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.60406561 * 1280; Err = 0.59296875 * 1280; time = 0.9259s; samplesPerSecond = 1382.4
MPI Rank 1:  Epoch[ 4 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.52176476 * 1280; Err = 0.58359375 * 1280; time = 0.9259s; samplesPerSecond = 1382.5
MPI Rank 1:  Epoch[ 4 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.52119408 * 1280; Err = 0.58437500 * 1280; time = 0.9260s; samplesPerSecond = 1382.4
MPI Rank 1:  Epoch[ 4 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.49686356 * 1280; Err = 0.55859375 * 1280; time = 0.9258s; samplesPerSecond = 1382.7
MPI Rank 1:  Epoch[ 4 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.47931290 * 1280; Err = 0.54062500 * 1280; time = 0.9258s; samplesPerSecond = 1382.6
MPI Rank 1:  Epoch[ 4 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.48296280 * 1280; Err = 0.56875000 * 1280; time = 0.9257s; samplesPerSecond = 1382.8
MPI Rank 1:  Epoch[ 4 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.47935333 * 1280; Err = 0.55781250 * 1280; time = 0.9266s; samplesPerSecond = 1381.4
MPI Rank 1:  Epoch[ 4 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.46096954 * 1280; Err = 0.54687500 * 1280; time = 0.9258s; samplesPerSecond = 1382.6
MPI Rank 1: Finished Epoch[ 4 of 10]: [Training] CE = 1.50624203 * 12500; Err = 0.56736000 * 12500; totalSamplesSeen = 50000; learningRatePerSample = 0.0040000002; epochTime=9.04961s
MPI Rank 1: Final Results: Minibatch[1-20]: CE = 1.55448412 * 10000; perplexity = 4.73264441; Err = 0.55940000 * 10000
MPI Rank 1: Finished Epoch[ 4 of 10]: [Validate] CE = 1.55448412 * 10000; Err = 0.55940000 * 10000
MPI Rank 1: 
MPI Rank 1: Starting Epoch 5: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 1: 
MPI Rank 1: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1:  Epoch[ 5 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.43231487 * 128; Err = 0.51562500 * 128; time = 0.0906s; samplesPerSecond = 1412.6
MPI Rank 1:  Epoch[ 5 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.47885842 * 1152; Err = 0.54947917 * 1152; time = 0.8315s; samplesPerSecond = 1385.5
MPI Rank 1:  Epoch[ 5 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.45817852 * 1280; Err = 0.54062500 * 1280; time = 0.9253s; samplesPerSecond = 1383.4
MPI Rank 1:  Epoch[ 5 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.44108772 * 1280; Err = 0.52109375 * 1280; time = 0.9251s; samplesPerSecond = 1383.6
MPI Rank 1:  Epoch[ 5 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.38997040 * 1280; Err = 0.51015625 * 1280; time = 0.9247s; samplesPerSecond = 1384.2
MPI Rank 1:  Epoch[ 5 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.42916870 * 1280; Err = 0.54218750 * 1280; time = 0.9254s; samplesPerSecond = 1383.2
MPI Rank 1:  Epoch[ 5 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.39721222 * 1280; Err = 0.52734375 * 1280; time = 0.9256s; samplesPerSecond = 1382.9
MPI Rank 1:  Epoch[ 5 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.36817856 * 1280; Err = 0.50234375 * 1280; time = 0.9245s; samplesPerSecond = 1384.6
MPI Rank 1:  Epoch[ 5 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.41853409 * 1280; Err = 0.51875000 * 1280; time = 0.9253s; samplesPerSecond = 1383.3
MPI Rank 1:  Epoch[ 5 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.36715012 * 1280; Err = 0.49765625 * 1280; time = 0.9255s; samplesPerSecond = 1383.0
MPI Rank 1: Finished Epoch[ 5 of 10]: [Training] CE = 1.40805203 * 12500; Err = 0.51792000 * 12500; totalSamplesSeen = 62500; learningRatePerSample = 0.0040000002; epochTime=9.04203s
MPI Rank 1: Final Results: Minibatch[1-20]: CE = 1.80379416 * 10000; perplexity = 6.07264439; Err = 0.58170000 * 10000
MPI Rank 1: Finished Epoch[ 5 of 10]: [Validate] CE = 1.80379416 * 10000; Err = 0.58170000 * 10000
MPI Rank 1: 
MPI Rank 1: Starting Epoch 6: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 1: 
MPI Rank 1: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1:  Epoch[ 6 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.32290256 * 128; Err = 0.42968750 * 128; time = 0.0895s; samplesPerSecond = 1430.1
MPI Rank 1:  Epoch[ 6 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.33791035 * 1152; Err = 0.49739583 * 1152; time = 0.8319s; samplesPerSecond = 1384.7
MPI Rank 1:  Epoch[ 6 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.38969078 * 1280; Err = 0.50859375 * 1280; time = 0.9261s; samplesPerSecond = 1382.1
MPI Rank 1:  Epoch[ 6 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.36624660 * 1280; Err = 0.50468750 * 1280; time = 0.9255s; samplesPerSecond = 1383.1
MPI Rank 1:  Epoch[ 6 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.32347641 * 1280; Err = 0.49453125 * 1280; time = 0.9255s; samplesPerSecond = 1383.1
MPI Rank 1:  Epoch[ 6 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.33120041 * 1280; Err = 0.47890625 * 1280; time = 0.9241s; samplesPerSecond = 1385.1
MPI Rank 1:  Epoch[ 6 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.32318649 * 1280; Err = 0.49218750 * 1280; time = 0.9249s; samplesPerSecond = 1384.0
MPI Rank 1:  Epoch[ 6 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.25090485 * 1280; Err = 0.45546875 * 1280; time = 0.9248s; samplesPerSecond = 1384.1
MPI Rank 1:  Epoch[ 6 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.24887085 * 1280; Err = 0.45390625 * 1280; time = 0.9257s; samplesPerSecond = 1382.7
MPI Rank 1:  Epoch[ 6 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.27704620 * 1280; Err = 0.45000000 * 1280; time = 0.9256s; samplesPerSecond = 1382.9
MPI Rank 1: Finished Epoch[ 6 of 10]: [Training] CE = 1.31220344 * 12500; Err = 0.47840000 * 12500; totalSamplesSeen = 75000; learningRatePerSample = 0.0040000002; epochTime=9.043s
MPI Rank 1: Final Results: Minibatch[1-20]: CE = 1.46368885 * 10000; perplexity = 4.32187291; Err = 0.51310000 * 10000
MPI Rank 1: Finished Epoch[ 6 of 10]: [Validate] CE = 1.46368885 * 10000; Err = 0.51310000 * 10000
MPI Rank 1: 
MPI Rank 1: Starting Epoch 7: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 1: 
MPI Rank 1: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1:  Epoch[ 7 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.20619452 * 128; Err = 0.43750000 * 128; time = 0.0910s; samplesPerSecond = 1406.2
MPI Rank 1:  Epoch[ 7 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.23470194 * 1152; Err = 0.45486111 * 1152; time = 0.8326s; samplesPerSecond = 1383.6
MPI Rank 1:  Epoch[ 7 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.24454441 * 1280; Err = 0.45781250 * 1280; time = 0.9253s; samplesPerSecond = 1383.3
MPI Rank 1:  Epoch[ 7 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.19735851 * 1280; Err = 0.43125000 * 1280; time = 0.9257s; samplesPerSecond = 1382.7
MPI Rank 1:  Epoch[ 7 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.17976799 * 1280; Err = 0.42187500 * 1280; time = 0.9252s; samplesPerSecond = 1383.5
MPI Rank 1:  Epoch[ 7 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.21874809 * 1280; Err = 0.45390625 * 1280; time = 0.9265s; samplesPerSecond = 1381.6
MPI Rank 1:  Epoch[ 7 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.24309616 * 1280; Err = 0.44140625 * 1280; time = 0.9265s; samplesPerSecond = 1381.5
MPI Rank 1:  Epoch[ 7 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.19931946 * 1280; Err = 0.42890625 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 1:  Epoch[ 7 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.20858383 * 1280; Err = 0.42031250 * 1280; time = 0.9270s; samplesPerSecond = 1380.8
MPI Rank 1:  Epoch[ 7 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.17254562 * 1280; Err = 0.42500000 * 1280; time = 0.9270s; samplesPerSecond = 1380.8
MPI Rank 1: Finished Epoch[ 7 of 10]: [Training] CE = 1.21004055 * 12500; Err = 0.43784000 * 12500; totalSamplesSeen = 87500; learningRatePerSample = 0.0040000002; epochTime=9.05292s
MPI Rank 1: Final Results: Minibatch[1-20]: CE = 1.73675151 * 10000; perplexity = 5.67886567; Err = 0.54910000 * 10000
MPI Rank 1: Finished Epoch[ 7 of 10]: [Validate] CE = 1.73675151 * 10000; Err = 0.54910000 * 10000
MPI Rank 1: 
MPI Rank 1: Starting Epoch 8: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 1: 
MPI Rank 1: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1:  Epoch[ 8 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.20597017 * 128; Err = 0.42968750 * 128; time = 0.0905s; samplesPerSecond = 1414.0
MPI Rank 1:  Epoch[ 8 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.17277661 * 1152; Err = 0.44184028 * 1152; time = 0.8323s; samplesPerSecond = 1384.2
MPI Rank 1:  Epoch[ 8 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.06030464 * 1280; Err = 0.38437500 * 1280; time = 0.9230s; samplesPerSecond = 1386.7
MPI Rank 1:  Epoch[ 8 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.15821381 * 1280; Err = 0.41718750 * 1280; time = 0.9232s; samplesPerSecond = 1386.5
MPI Rank 1:  Epoch[ 8 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.16836205 * 1280; Err = 0.43125000 * 1280; time = 0.9237s; samplesPerSecond = 1385.7
MPI Rank 1:  Epoch[ 8 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.12226524 * 1280; Err = 0.39765625 * 1280; time = 0.9241s; samplesPerSecond = 1385.2
MPI Rank 1:  Epoch[ 8 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.18367767 * 1280; Err = 0.44218750 * 1280; time = 0.9233s; samplesPerSecond = 1386.3
MPI Rank 1:  Epoch[ 8 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.16624985 * 1280; Err = 0.42890625 * 1280; time = 0.9241s; samplesPerSecond = 1385.1
MPI Rank 1:  Epoch[ 8 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.11522598 * 1280; Err = 0.40625000 * 1280; time = 0.9249s; samplesPerSecond = 1384.0
MPI Rank 1:  Epoch[ 8 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.13272400 * 1280; Err = 0.41171875 * 1280; time = 0.9249s; samplesPerSecond = 1384.0
MPI Rank 1: Finished Epoch[ 8 of 10]: [Training] CE = 1.14270406 * 12500; Err = 0.41672000 * 12500; totalSamplesSeen = 100000; learningRatePerSample = 0.0040000002; epochTime=9.03469s
MPI Rank 1: Final Results: Minibatch[1-20]: CE = 1.43002211 * 10000; perplexity = 4.17879157; Err = 0.47190000 * 10000
MPI Rank 1: Finished Epoch[ 8 of 10]: [Validate] CE = 1.43002211 * 10000; Err = 0.47190000 * 10000
MPI Rank 1: 
MPI Rank 1: Starting Epoch 9: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 1: 
MPI Rank 1: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1:  Epoch[ 9 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.01617670 * 128; Err = 0.39843750 * 128; time = 0.0942s; samplesPerSecond = 1358.4
MPI Rank 1:  Epoch[ 9 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.12609010 * 1152; Err = 0.41493056 * 1152; time = 0.8340s; samplesPerSecond = 1381.3
MPI Rank 1:  Epoch[ 9 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.12273750 * 1280; Err = 0.41640625 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 1:  Epoch[ 9 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.09054737 * 1280; Err = 0.39296875 * 1280; time = 0.9272s; samplesPerSecond = 1380.5
MPI Rank 1:  Epoch[ 9 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.13156357 * 1280; Err = 0.41406250 * 1280; time = 0.9267s; samplesPerSecond = 1381.2
MPI Rank 1:  Epoch[ 9 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.02879791 * 1280; Err = 0.37421875 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 1:  Epoch[ 9 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.09306564 * 1280; Err = 0.39140625 * 1280; time = 0.9260s; samplesPerSecond = 1382.4
MPI Rank 1:  Epoch[ 9 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.05069733 * 1280; Err = 0.36171875 * 1280; time = 0.9269s; samplesPerSecond = 1380.9
MPI Rank 1:  Epoch[ 9 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.05866699 * 1280; Err = 0.37968750 * 1280; time = 0.9253s; samplesPerSecond = 1383.3
MPI Rank 1:  Epoch[ 9 of 10]-Minibatch[  81-  90, 400.00%]: CE = 0.99932632 * 1280; Err = 0.35625000 * 1280; time = 0.9319s; samplesPerSecond = 1373.5
MPI Rank 1: Finished Epoch[ 9 of 10]: [Training] CE = 1.07076813 * 12500; Err = 0.38624000 * 12500; totalSamplesSeen = 112500; learningRatePerSample = 0.0040000002; epochTime=9.06575s
MPI Rank 1: Final Results: Minibatch[1-20]: CE = 1.24147657 * 10000; perplexity = 3.46071971; Err = 0.41610000 * 10000
MPI Rank 1: Finished Epoch[ 9 of 10]: [Validate] CE = 1.24147657 * 10000; Err = 0.41610000 * 10000
MPI Rank 1: 
MPI Rank 1: Starting Epoch 10: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 1: 
MPI Rank 1: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1:  Epoch[10 of 10]-Minibatch[   1-   1, 4.44%]: CE = 0.95460719 * 128; Err = 0.32031250 * 128; time = 0.1214s; samplesPerSecond = 1054.4
MPI Rank 1:  Epoch[10 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.05397805 * 1152; Err = 0.37500000 * 1152; time = 0.8315s; samplesPerSecond = 1385.4
MPI Rank 1:  Epoch[10 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.01994104 * 1280; Err = 0.37656250 * 1280; time = 0.9266s; samplesPerSecond = 1381.4
MPI Rank 1:  Epoch[10 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.06483440 * 1280; Err = 0.38437500 * 1280; time = 0.9249s; samplesPerSecond = 1383.9
MPI Rank 1:  Epoch[10 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.09314823 * 1280; Err = 0.39140625 * 1280; time = 0.9260s; samplesPerSecond = 1382.3
MPI Rank 1:  Epoch[10 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.03655357 * 1280; Err = 0.37656250 * 1280; time = 0.9265s; samplesPerSecond = 1381.5
MPI Rank 1:  Epoch[10 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.01607895 * 1280; Err = 0.36718750 * 1280; time = 0.9268s; samplesPerSecond = 1381.0
MPI Rank 1:  Epoch[10 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.05162354 * 1280; Err = 0.39140625 * 1280; time = 0.9259s; samplesPerSecond = 1382.4
MPI Rank 1:  Epoch[10 of 10]-Minibatch[  71-  80, 355.56%]: CE = 0.94877014 * 1280; Err = 0.33515625 * 1280; time = 0.9252s; samplesPerSecond = 1383.4
MPI Rank 1:  Epoch[10 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.03715973 * 1280; Err = 0.37109375 * 1280; time = 0.9259s; samplesPerSecond = 1382.4
MPI Rank 1: Finished Epoch[10 of 10]: [Training] CE = 1.03298641 * 12500; Err = 0.37424000 * 12500; totalSamplesSeen = 125000; learningRatePerSample = 0.0040000002; epochTime=9.08281s
MPI Rank 1: Final Results: Minibatch[1-20]: CE = 0.98307145 * 10000; perplexity = 2.67265257; Err = 0.35760000 * 10000
MPI Rank 1: Finished Epoch[10 of 10]: [Validate] CE = 0.98307145 * 10000; Err = 0.35760000 * 10000
MPI Rank 1: 
MPI Rank 1: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: COMPLETED.
MPI Rank 1: ~MPIWrapper
MPI Rank 2: Configuration After Processing and Variable Resolution:
MPI Rank 2: 
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:asyncBuffer=false
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:command=Train
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:ConfigDir=.
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:configName=4gpu-take1
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:DataDir=.
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:deviceId=auto
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:epochSize=10
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:imageLayout=cudnn
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:initOnCPUOnly=true
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:makeMode=true
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:minibatch=512
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:minibatchSize=128
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:ModelDir=./Output-4gpu-take1/Models
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:ndlMacros=./Macros.ndl
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:OutputDir=./Output-4gpu-take1
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:parallelizationMethod=DataParallelASGD
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:parallelTrain=true
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:precision=float
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:prefetch=true
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:Proj16to32Filename=./16to32.txt
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:Proj32to64Filename=./32to64.txt
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:RootDir=.
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:stderr=./Output-4gpu-take1/03_ResNet
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:Test=[
MPI Rank 2:     action = "test"
MPI Rank 2:     modelPath = "./Output-4gpu-take1/Models/03_ResNet"
MPI Rank 2:     minibatchSize = 256
MPI Rank 2:     reader = [
MPI Rank 2:         readerType = "ImageReader"
MPI Rank 2:         file = "./cifar-10-batches-py/test_map.txt"
MPI Rank 2:         randomize = "none"
MPI Rank 2:         features = [
MPI Rank 2:             width = 32
MPI Rank 2:             height = 32
MPI Rank 2:             channels = 3
MPI Rank 2:             cropType = "center"
MPI Rank 2:             cropRatio = 1
MPI Rank 2:             jitterType = "uniRatio"
MPI Rank 2:             interpolations = "linear"
MPI Rank 2:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 2:         ]
MPI Rank 2:         labels = [
MPI Rank 2:             labelDim = 10
MPI Rank 2:         ]
MPI Rank 2:     ]    
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:traceLevel=1
MPI Rank 2: configparameters: 03_ResNet-parallel.cntk:Train=[
MPI Rank 2:     action = "train"
MPI Rank 2:     modelPath = "./Output-4gpu-take1/Models/03_ResNet"
MPI Rank 2:      NDLNetworkBuilder = [
MPI Rank 2:         networkDescription = "./03_ResNet.ndl"
MPI Rank 2:     ]
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize = 0
MPI Rank 2:         minibatchSize = 512
MPI Rank 2:         learningRatesPerSample = 0.004*80:0.0004*40:0.00004
MPI Rank 2:         momentumPerMB = 0
MPI Rank 2:         maxEpochs = 10
MPI Rank 2:         L2RegWeight = 0.0001
MPI Rank 2:         dropoutRate = 0
MPI Rank 2:         perfTraceLevel = 0
MPI Rank 2:         firstMBsToShowResult = 1
MPI Rank 2:         numMBsToShowResult = 10
MPI Rank 2:         ParallelTrain = [
MPI Rank 2:             parallelizationMethod = DataParallelASGD
MPI Rank 2:             distributedMBReading = "true"
MPI Rank 2:             parallelizationStartEpoch = 1
MPI Rank 2:             DataParallelSGD = [
MPI Rank 2:                 gradientBits = 32
MPI Rank 2:                 useBufferedAsyncGradientAggregation = false
MPI Rank 2:             ]
MPI Rank 2:             ModelAveragingSGD = [
MPI Rank 2:                 blockSizePerWorker = 128
MPI Rank 2:             ]
MPI Rank 2:             DataParallelASGD = [
MPI Rank 2:                 syncPeriod = 128
MPI Rank 2:                 usePipeline = false
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2:     ]
MPI Rank 2:     reader = [
MPI Rank 2:         readerType = "ImageReader"
MPI Rank 2:         file = "./cifar-10-batches-py/train_map.txt"
MPI Rank 2:         randomize = "auto"
MPI Rank 2:         features = [
MPI Rank 2:             width = 32
MPI Rank 2:             height = 32
MPI Rank 2:             channels = 3
MPI Rank 2:             cropType = "random"
MPI Rank 2:             cropRatio = 0.8
MPI Rank 2:             jitterType = "uniRatio"
MPI Rank 2:             interpolations = "linear"
MPI Rank 2:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 2:         ]
MPI Rank 2:         labels = [
MPI Rank 2:             labelDim = 10
MPI Rank 2:         ]
MPI Rank 2:     ]
MPI Rank 2:     cvReader = [
MPI Rank 2:         readerType = "ImageReader"
MPI Rank 2:         file = "./cifar-10-batches-py/test_map.txt"
MPI Rank 2:         randomize = "none"
MPI Rank 2:         features = [
MPI Rank 2:             width = 32
MPI Rank 2:             height = 32
MPI Rank 2:             channels = 3
MPI Rank 2:             cropType = "center"
MPI Rank 2:             cropRatio = 1
MPI Rank 2:             jitterType = "uniRatio"
MPI Rank 2:             interpolations = "linear"
MPI Rank 2:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 2:         ]
MPI Rank 2:         labels = [
MPI Rank 2:             labelDim = 10
MPI Rank 2:         ]
MPI Rank 2:     ]    
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: Commands: Train
MPI Rank 2: precision = "float"
MPI Rank 2: 
MPI Rank 2: ##############################################################################
MPI Rank 2: #                                                                            #
MPI Rank 2: # Train command (train action)                                               #
MPI Rank 2: #                                                                            #
MPI Rank 2: ##############################################################################
MPI Rank 2: 
MPI Rank 2: LockDevice: Failed to lock GPU 0 for exclusive use.
MPI Rank 2: LockDevice: Failed to lock GPU 7 for exclusive use.
MPI Rank 2: 
MPI Rank 2: Creating virgin network.
MPI Rank 2: NDLBuilder Using GPU 1
MPI Rank 2: SetGaussianRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 2: 
MPI Rank 2: OutputNodes.t Times operation: For legacy compatibility, the sample layout of left input (OutputNodes.W LearnableParameter operation) was patched to [10 x 1 x 1 x 64] (from [10 x 64])
MPI Rank 2: conv1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 16, Kernel: 3 x 3 x 3, Map: 1 x 1 x 16, Stride: 1 x 1 x 3, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn1_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn1_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn1_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn1_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn1_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn1_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn2_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 16 x 16 x 32, Kernel: 3 x 3 x 16, Map: 1 x 1 x 32, Stride: 2 x 2 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn2_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn2_1.c_proj.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 16 x 16 x 32, Kernel: 1 x 1 x 16, Map: 1 x 1 x 32, Stride: 2 x 2 x 16, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn2_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn2_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn2_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn2_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn3_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 8 x 8 x 64, Kernel: 3 x 3 x 32, Map: 1 x 1 x 64, Stride: 2 x 2 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn3_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn3_1.c_proj.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 8 x 8 x 64, Kernel: 1 x 1 x 32, Map: 1 x 1 x 64, Stride: 2 x 2 x 32, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn3_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn3_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn3_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: rn3_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 2: Using CNTK batch normalization engine.
MPI Rank 2: pool: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 1 x 1 x 64, Kernel: 8 x 8 x 1, Map: 1, Stride: 1 x 1 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 2: 
MPI Rank 2: Model has 184 nodes. Using GPU 1.
MPI Rank 2: 
MPI Rank 2: Training criterion:   CE = CrossEntropyWithSoftmax
MPI Rank 2: Evaluation criterion: Err = ClassificationError
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing: Out of 321 matrices, 160 are shared as 62, and 161 are not shared.
MPI Rank 2: 
MPI Rank 2:     { conv1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       conv1.y : [32 x 32 x 16 x *] }
MPI Rank 2:     { conv1.c.W : [16 x 27] (gradient)
MPI Rank 2:       rn1_1.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_1.c1.y : [32 x 32 x 16 x *] }
MPI Rank 2:     { conv1.c.c.b : [16 x 1] (gradient)
MPI Rank 2:       rn1_1.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_1.c1.c.W : [16 x 144] (gradient)
MPI Rank 2:       rn1_1.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_1.p : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_1.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_1.y : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_1.c2.W : [16 x 144] (gradient)
MPI Rank 2:       rn1_2.c1.c.c.c : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_1.c2.c.sc : [16 x 1] (gradient)
MPI Rank 2:       rn1_1.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 2:     { conv1.c.c.sc : [16 x 1] (gradient)
MPI Rank 2:       conv1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_2.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_2.c1.y : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_1.c2.c.b : [16 x 1] (gradient)
MPI Rank 2:       rn1_2.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_2.c1.c.W : [16 x 144] (gradient)
MPI Rank 2:       rn1_2.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_2.p : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_2.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_2.y : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_2.c2.W : [16 x 144] (gradient)
MPI Rank 2:       rn1_3.c1.c.c.c : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_2.c2.c.sc : [16 x 1] (gradient)
MPI Rank 2:       rn1_2.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 2:     { rn1_1.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 2:       rn1_1.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_3.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_3.c1.y : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_2.c2.c.b : [16 x 1] (gradient)
MPI Rank 2:       rn1_3.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_3.c1.c.W : [16 x 144] (gradient)
MPI Rank 2:       rn1_3.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_3.p : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_3.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_3.y : [32 x 32 x 16 x *] }
MPI Rank 2:     { rn1_3.c2.W : [16 x 144] (gradient)
MPI Rank 2:       rn2_1.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn1_3.c2.c.sc : [16 x 1] (gradient)
MPI Rank 2:       rn1_3.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 2:     { rn1_2.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 2:       rn1_2.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_2.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn2_1.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_1.c1.y : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn1_3.c2.c.b : [16 x 1] (gradient)
MPI Rank 2:       rn2_1.c2.c.c : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_1.c2.c.sc : [32 x 1] (gradient)
MPI Rank 2:       rn2_1.c_proj.c : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_1.c1.c.W : [32 x 144] (gradient)
MPI Rank 2:       rn2_1.c2.c.c : [16 x 16 x 32 x *] (gradient) }
MPI Rank 2:     { rn2_1.c2.c.b : [32 x 1] (gradient)
MPI Rank 2:       rn2_1.c_proj.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_1.p : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_1.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_1.y : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_1.c2.W : [32 x 288] (gradient)
MPI Rank 2:       rn2_2.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_1.c_proj.sc : [32 x 1] (gradient)
MPI Rank 2:       rn2_1.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 2:     { rn1_3.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 2:       rn1_3.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 2:       rn1_3.y : [32 x 32 x 16 x *] (gradient) }
MPI Rank 2:     { rn2_1.c_proj.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_2.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_2.c1.y : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_2.c1.c.W : [32 x 288] (gradient)
MPI Rank 2:       rn2_2.c2.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_2.p : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_2.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_2.y : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_2.c2.W : [32 x 288] (gradient)
MPI Rank 2:       rn2_3.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_2.c2.c.sc : [32 x 1] (gradient)
MPI Rank 2:       rn2_2.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 2:     { rn2_1.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 2:       rn2_1.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_3.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_3.c1.y : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_2.c2.c.b : [32 x 1] (gradient)
MPI Rank 2:       rn2_3.c2.c.c : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_3.c1.c.W : [32 x 288] (gradient)
MPI Rank 2:       rn2_3.c2.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_3.p : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_3.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_3.y : [16 x 16 x 32 x *] }
MPI Rank 2:     { rn2_3.c2.W : [32 x 288] (gradient)
MPI Rank 2:       rn3_1.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn2_3.c2.c.sc : [32 x 1] (gradient)
MPI Rank 2:       rn2_3.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 2:     { rn2_2.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 2:       rn2_2.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_2.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn3_1.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_1.c1.y : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn2_3.c2.c.b : [32 x 1] (gradient)
MPI Rank 2:       rn3_1.c2.c.c : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_1.c2.c.sc : [64 x 1] (gradient)
MPI Rank 2:       rn3_1.c_proj.c : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_1.c1.c.W : [64 x 288] (gradient)
MPI Rank 2:       rn3_1.c2.c.c : [8 x 8 x 64 x *] (gradient) }
MPI Rank 2:     { rn3_1.c2.c.b : [64 x 1] (gradient)
MPI Rank 2:       rn3_1.c_proj.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_1.p : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_1.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_1.y : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_1.c2.W : [64 x 576] (gradient)
MPI Rank 2:       rn3_2.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_1.c_proj.sc : [64 x 1] (gradient)
MPI Rank 2:       rn3_1.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 2:     { rn2_3.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 2:       rn2_3.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 2:       rn2_3.y : [16 x 16 x 32 x *] (gradient) }
MPI Rank 2:     { rn3_1.c_proj.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_2.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_2.c1.y : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_2.c1.c.W : [64 x 576] (gradient)
MPI Rank 2:       rn3_2.c2.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_2.p : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_2.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_2.y : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_2.c2.W : [64 x 576] (gradient)
MPI Rank 2:       rn3_3.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_2.c2.c.sc : [64 x 1] (gradient)
MPI Rank 2:       rn3_2.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 2:     { rn3_1.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 2:       rn3_1.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_3.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_3.c1.y : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_2.c2.c.b : [64 x 1] (gradient)
MPI Rank 2:       rn3_3.c2.c.c : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_3.c1.c.W : [64 x 576] (gradient)
MPI Rank 2:       rn3_3.c2.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_3.p : [8 x 8 x 64 x *] }
MPI Rank 2:     { rn3_3.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_3.y : [8 x 8 x 64 x *] }
MPI Rank 2:     { pool : [1 x 1 x 64 x *]
MPI Rank 2:       rn3_3.c2.c.sc : [64 x 1] (gradient)
MPI Rank 2:       rn3_3.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 2:     { OutputNodes.t : [10 x *]
MPI Rank 2:       rn3_3.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 2:       rn3_3.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_3.y : [8 x 8 x 64 x *] (gradient) }
MPI Rank 2:     { OutputNodes.W : [10 x 1 x 1 x 64] (gradient)
MPI Rank 2:       OutputNodes.z : [10 x *] (gradient) }
MPI Rank 2:     { OutputNodes.t : [10 x *] (gradient)
MPI Rank 2:       rn3_2.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 2:       rn3_2.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 2:       rn3_2.y : [8 x 8 x 64 x *] (gradient) }
MPI Rank 2:     { pool : [1 x 1 x 64 x *] (gradient)
MPI Rank 2:       rn3_3.c2.W : [64 x 576] (gradient) }
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Training 269914 parameters in 63 out of 63 parameter tensors and 137 nodes with gradient:
MPI Rank 2: 
MPI Rank 2:     Node 'OutputNodes.W' (LearnableParameter operation) : [10 x 1 x 1 x 64]
MPI Rank 2:     Node 'OutputNodes.b' (LearnableParameter operation) : [10]
MPI Rank 2:     Node 'conv1.c.W' (LearnableParameter operation) : [16 x 27]
MPI Rank 2:     Node 'conv1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'conv1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_1.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 2:     Node 'rn1_1.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_1.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_1.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 2:     Node 'rn1_1.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_1.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_2.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 2:     Node 'rn1_2.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_2.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_2.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 2:     Node 'rn1_2.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_2.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_3.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 2:     Node 'rn1_3.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_3.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_3.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 2:     Node 'rn1_3.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn1_3.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 2:     Node 'rn2_1.c1.c.W' (LearnableParameter operation) : [32 x 144]
MPI Rank 2:     Node 'rn2_1.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_1.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_1.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 2:     Node 'rn2_1.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_1.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_1.c_proj.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_1.c_proj.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_2.c1.c.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 2:     Node 'rn2_2.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_2.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_2.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 2:     Node 'rn2_2.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_2.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_3.c1.c.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 2:     Node 'rn2_3.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_3.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_3.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 2:     Node 'rn2_3.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn2_3.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 2:     Node 'rn3_1.c1.c.W' (LearnableParameter operation) : [64 x 288]
MPI Rank 2:     Node 'rn3_1.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_1.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_1.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 2:     Node 'rn3_1.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_1.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_1.c_proj.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_1.c_proj.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_2.c1.c.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 2:     Node 'rn3_2.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_2.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_2.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 2:     Node 'rn3_2.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_2.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_3.c1.c.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 2:     Node 'rn3_3.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_3.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_3.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 2:     Node 'rn3_3.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 2:     Node 'rn3_3.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 2: 
MPI Rank 2: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 2: 
MPI Rank 2: Starting Epoch 1: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 2: 
MPI Rank 2: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2:  Epoch[ 1 of 10]-Minibatch[   1-   1]: CE = 2.30143619 * 128; Err = 0.92187500 * 128; time = 2.8481s; samplesPerSecond = 44.9
MPI Rank 2:  Epoch[ 1 of 10]-Minibatch[   2-  10]: CE = 2.74810931 * 1152; Err = 0.90625000 * 1152; time = 0.8353s; samplesPerSecond = 1379.2
MPI Rank 2:  Epoch[ 1 of 10]-Minibatch[  11-  20]: CE = 2.26273994 * 1280; Err = 0.85234375 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 2:  Epoch[ 1 of 10]-Minibatch[  21-  30]: CE = 2.18159180 * 1280; Err = 0.82734375 * 1280; time = 0.9274s; samplesPerSecond = 1380.2
MPI Rank 2:  Epoch[ 1 of 10]-Minibatch[  31-  40]: CE = 2.07492905 * 1280; Err = 0.77109375 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 2:  Epoch[ 1 of 10]-Minibatch[  41-  50]: CE = 2.03990860 * 1280; Err = 0.78984375 * 1280; time = 0.9269s; samplesPerSecond = 1381.0
MPI Rank 2:  Epoch[ 1 of 10]-Minibatch[  51-  60]: CE = 1.97048492 * 1280; Err = 0.75312500 * 1280; time = 0.9272s; samplesPerSecond = 1380.5
MPI Rank 2:  Epoch[ 1 of 10]-Minibatch[  61-  70]: CE = 1.99899445 * 1280; Err = 0.77031250 * 1280; time = 0.9265s; samplesPerSecond = 1381.6
MPI Rank 2:  Epoch[ 1 of 10]-Minibatch[  71-  80]: CE = 1.95247955 * 1280; Err = 0.77265625 * 1280; time = 0.9275s; samplesPerSecond = 1380.0
MPI Rank 2:  Epoch[ 1 of 10]-Minibatch[  81-  90]: CE = 1.96528168 * 1280; Err = 0.75859375 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 2: Finished Epoch[ 1 of 10]: [Training] CE = 2.10694656 * 12500; Err = 0.79464000 * 12500; totalSamplesSeen = 12500; learningRatePerSample = 0.0040000002; epochTime=11.8231s
MPI Rank 2: Final Results: Minibatch[1-20]: CE = 1.85908548 * 10000; perplexity = 6.41786484; Err = 0.71600000 * 10000
MPI Rank 2: Finished Epoch[ 1 of 10]: [Validate] CE = 1.85908548 * 10000; Err = 0.71600000 * 10000
MPI Rank 2: 
MPI Rank 2: Starting Epoch 2: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 2: 
MPI Rank 2: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2:  Epoch[ 2 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.86717653 * 128; Err = 0.71875000 * 128; time = 0.0930s; samplesPerSecond = 1376.0
MPI Rank 2:  Epoch[ 2 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.86210606 * 1152; Err = 0.71440972 * 1152; time = 0.8343s; samplesPerSecond = 1380.8
MPI Rank 2:  Epoch[ 2 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.80580349 * 1280; Err = 0.70312500 * 1280; time = 0.9279s; samplesPerSecond = 1379.4
MPI Rank 2:  Epoch[ 2 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.79860001 * 1280; Err = 0.68203125 * 1280; time = 0.9279s; samplesPerSecond = 1379.5
MPI Rank 2:  Epoch[ 2 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.81655655 * 1280; Err = 0.68750000 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 2:  Epoch[ 2 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.77120438 * 1280; Err = 0.69687500 * 1280; time = 0.9271s; samplesPerSecond = 1380.6
MPI Rank 2:  Epoch[ 2 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.77330017 * 1280; Err = 0.68671875 * 1280; time = 0.9280s; samplesPerSecond = 1379.3
MPI Rank 2:  Epoch[ 2 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.72636795 * 1280; Err = 0.64921875 * 1280; time = 0.9275s; samplesPerSecond = 1380.1
MPI Rank 2:  Epoch[ 2 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.70862961 * 1280; Err = 0.61953125 * 1280; time = 0.9271s; samplesPerSecond = 1380.6
MPI Rank 2:  Epoch[ 2 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.72210388 * 1280; Err = 0.66328125 * 1280; time = 0.9287s; samplesPerSecond = 1378.3
MPI Rank 2: Finished Epoch[ 2 of 10]: [Training] CE = 1.76838500 * 12500; Err = 0.67504000 * 12500; totalSamplesSeen = 25000; learningRatePerSample = 0.0040000002; epochTime=9.0706s
MPI Rank 2: Final Results: Minibatch[1-20]: CE = 1.77518572 * 10000; perplexity = 5.90137706; Err = 0.66660000 * 10000
MPI Rank 2: Finished Epoch[ 2 of 10]: [Validate] CE = 1.77518572 * 10000; Err = 0.66660000 * 10000
MPI Rank 2: 
MPI Rank 2: Starting Epoch 3: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 2: 
MPI Rank 2: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2:  Epoch[ 3 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.61677980 * 128; Err = 0.56250000 * 128; time = 0.0936s; samplesPerSecond = 1368.0
MPI Rank 2:  Epoch[ 3 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.69759130 * 1152; Err = 0.61024306 * 1152; time = 0.8340s; samplesPerSecond = 1381.3
MPI Rank 2:  Epoch[ 3 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.74267902 * 1280; Err = 0.67031250 * 1280; time = 0.9274s; samplesPerSecond = 1380.1
MPI Rank 2:  Epoch[ 3 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.67744331 * 1280; Err = 0.63984375 * 1280; time = 0.9272s; samplesPerSecond = 1380.6
MPI Rank 2:  Epoch[ 3 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.63101540 * 1280; Err = 0.62578125 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 2:  Epoch[ 3 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.63789139 * 1280; Err = 0.60937500 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 2:  Epoch[ 3 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.62628174 * 1280; Err = 0.61250000 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 2:  Epoch[ 3 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.61424484 * 1280; Err = 0.60390625 * 1280; time = 0.9268s; samplesPerSecond = 1381.1
MPI Rank 2:  Epoch[ 3 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.60680237 * 1280; Err = 0.58593750 * 1280; time = 0.9272s; samplesPerSecond = 1380.6
MPI Rank 2:  Epoch[ 3 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.55009308 * 1280; Err = 0.56875000 * 1280; time = 0.9280s; samplesPerSecond = 1379.3
MPI Rank 2: Finished Epoch[ 3 of 10]: [Training] CE = 1.64256781 * 12500; Err = 0.61400000 * 12500; totalSamplesSeen = 37500; learningRatePerSample = 0.0040000002; epochTime=9.06755s
MPI Rank 2: Final Results: Minibatch[1-20]: CE = 2.41066101 * 10000; perplexity = 11.14132325; Err = 0.72600000 * 10000
MPI Rank 2: Finished Epoch[ 3 of 10]: [Validate] CE = 2.41066101 * 10000; Err = 0.72600000 * 10000
MPI Rank 2: 
MPI Rank 2: Starting Epoch 4: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 2: 
MPI Rank 2: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2:  Epoch[ 4 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.50991583 * 128; Err = 0.59375000 * 128; time = 0.0937s; samplesPerSecond = 1365.8
MPI Rank 2:  Epoch[ 4 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.56187868 * 1152; Err = 0.57031250 * 1152; time = 0.8347s; samplesPerSecond = 1380.1
MPI Rank 2:  Epoch[ 4 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.59060535 * 1280; Err = 0.59062500 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 2:  Epoch[ 4 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.55350494 * 1280; Err = 0.59062500 * 1280; time = 0.9285s; samplesPerSecond = 1378.6
MPI Rank 2:  Epoch[ 4 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.53606949 * 1280; Err = 0.58671875 * 1280; time = 0.9274s; samplesPerSecond = 1380.2
MPI Rank 2:  Epoch[ 4 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.45735626 * 1280; Err = 0.52890625 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 2:  Epoch[ 4 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.52657318 * 1280; Err = 0.56718750 * 1280; time = 0.9265s; samplesPerSecond = 1381.6
MPI Rank 2:  Epoch[ 4 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.48956375 * 1280; Err = 0.53750000 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 2:  Epoch[ 4 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.48909607 * 1280; Err = 0.56171875 * 1280; time = 0.9263s; samplesPerSecond = 1381.8
MPI Rank 2:  Epoch[ 4 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.49958115 * 1280; Err = 0.55000000 * 1280; time = 0.9268s; samplesPerSecond = 1381.1
MPI Rank 2: Finished Epoch[ 4 of 10]: [Training] CE = 1.51817453 * 12500; Err = 0.56200000 * 12500; totalSamplesSeen = 50000; learningRatePerSample = 0.0040000002; epochTime=9.06596s
MPI Rank 2: Final Results: Minibatch[1-20]: CE = 1.50784795 * 10000; perplexity = 4.51699951; Err = 0.55600000 * 10000
MPI Rank 2: Finished Epoch[ 4 of 10]: [Validate] CE = 1.50784795 * 10000; Err = 0.55600000 * 10000
MPI Rank 2: 
MPI Rank 2: Starting Epoch 5: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 2: 
MPI Rank 2: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2:  Epoch[ 5 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.44590676 * 128; Err = 0.46093750 * 128; time = 0.0964s; samplesPerSecond = 1328.3
MPI Rank 2:  Epoch[ 5 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.47172132 * 1152; Err = 0.56770833 * 1152; time = 0.8335s; samplesPerSecond = 1382.1
MPI Rank 2:  Epoch[ 5 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.43673115 * 1280; Err = 0.54140625 * 1280; time = 0.9272s; samplesPerSecond = 1380.5
MPI Rank 2:  Epoch[ 5 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.52224827 * 1280; Err = 0.55781250 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 2:  Epoch[ 5 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.40509644 * 1280; Err = 0.51562500 * 1280; time = 0.9276s; samplesPerSecond = 1379.8
MPI Rank 2:  Epoch[ 5 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.39969673 * 1280; Err = 0.52578125 * 1280; time = 0.9275s; samplesPerSecond = 1380.1
MPI Rank 2:  Epoch[ 5 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.43492737 * 1280; Err = 0.52265625 * 1280; time = 0.9269s; samplesPerSecond = 1380.9
MPI Rank 2:  Epoch[ 5 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.44466553 * 1280; Err = 0.54140625 * 1280; time = 0.9264s; samplesPerSecond = 1381.8
MPI Rank 2:  Epoch[ 5 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.30872421 * 1280; Err = 0.48671875 * 1280; time = 0.9277s; samplesPerSecond = 1379.7
MPI Rank 2:  Epoch[ 5 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.37634888 * 1280; Err = 0.50156250 * 1280; time = 0.9261s; samplesPerSecond = 1382.1
MPI Rank 2: Finished Epoch[ 5 of 10]: [Training] CE = 1.41547656 * 12500; Err = 0.52544000 * 12500; totalSamplesSeen = 62500; learningRatePerSample = 0.0040000002; epochTime=9.0684s
MPI Rank 2: Final Results: Minibatch[1-20]: CE = 1.40771795 * 10000; perplexity = 4.08661889; Err = 0.50820000 * 10000
MPI Rank 2: Finished Epoch[ 5 of 10]: [Validate] CE = 1.40771795 * 10000; Err = 0.50820000 * 10000
MPI Rank 2: 
MPI Rank 2: Starting Epoch 6: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 2: 
MPI Rank 2: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2:  Epoch[ 6 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.30798674 * 128; Err = 0.47656250 * 128; time = 0.0936s; samplesPerSecond = 1367.8
MPI Rank 2:  Epoch[ 6 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.33994850 * 1152; Err = 0.48784722 * 1152; time = 0.8341s; samplesPerSecond = 1381.1
MPI Rank 2:  Epoch[ 6 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.39527416 * 1280; Err = 0.51718750 * 1280; time = 0.9271s; samplesPerSecond = 1380.6
MPI Rank 2:  Epoch[ 6 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.31698780 * 1280; Err = 0.48437500 * 1280; time = 0.9273s; samplesPerSecond = 1380.4
MPI Rank 2:  Epoch[ 6 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.27182999 * 1280; Err = 0.45937500 * 1280; time = 0.9274s; samplesPerSecond = 1380.1
MPI Rank 2:  Epoch[ 6 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.28720703 * 1280; Err = 0.48281250 * 1280; time = 0.9273s; samplesPerSecond = 1380.4
MPI Rank 2:  Epoch[ 6 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.26226578 * 1280; Err = 0.46250000 * 1280; time = 0.9270s; samplesPerSecond = 1380.8
MPI Rank 2:  Epoch[ 6 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.29755859 * 1280; Err = 0.47656250 * 1280; time = 0.9285s; samplesPerSecond = 1378.5
MPI Rank 2:  Epoch[ 6 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.29364166 * 1280; Err = 0.45546875 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 2:  Epoch[ 6 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.23769531 * 1280; Err = 0.45234375 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 2: Finished Epoch[ 6 of 10]: [Training] CE = 1.30156086 * 12500; Err = 0.47672000 * 12500; totalSamplesSeen = 75000; learningRatePerSample = 0.0040000002; epochTime=9.0695s
MPI Rank 2: Final Results: Minibatch[1-20]: CE = 1.53514716 * 10000; perplexity = 4.64200859; Err = 0.55830000 * 10000
MPI Rank 2: Finished Epoch[ 6 of 10]: [Validate] CE = 1.53514716 * 10000; Err = 0.55830000 * 10000
MPI Rank 2: 
MPI Rank 2: Starting Epoch 7: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 2: 
MPI Rank 2: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2:  Epoch[ 7 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.33770728 * 128; Err = 0.44531250 * 128; time = 0.0931s; samplesPerSecond = 1375.0
MPI Rank 2:  Epoch[ 7 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.20926881 * 1152; Err = 0.46006944 * 1152; time = 0.8344s; samplesPerSecond = 1380.6
MPI Rank 2:  Epoch[ 7 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.18445911 * 1280; Err = 0.42734375 * 1280; time = 0.9284s; samplesPerSecond = 1378.7
MPI Rank 2:  Epoch[ 7 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.17488461 * 1280; Err = 0.42265625 * 1280; time = 0.9278s; samplesPerSecond = 1379.6
MPI Rank 2:  Epoch[ 7 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.24591751 * 1280; Err = 0.44921875 * 1280; time = 0.9290s; samplesPerSecond = 1377.9
MPI Rank 2:  Epoch[ 7 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.29504738 * 1280; Err = 0.48515625 * 1280; time = 0.9279s; samplesPerSecond = 1379.5
MPI Rank 2:  Epoch[ 7 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.22345428 * 1280; Err = 0.43515625 * 1280; time = 0.9287s; samplesPerSecond = 1378.2
MPI Rank 2:  Epoch[ 7 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.23423996 * 1280; Err = 0.46015625 * 1280; time = 0.9285s; samplesPerSecond = 1378.6
MPI Rank 2:  Epoch[ 7 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.21561508 * 1280; Err = 0.42812500 * 1280; time = 0.9289s; samplesPerSecond = 1378.0
MPI Rank 2:  Epoch[ 7 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.23531494 * 1280; Err = 0.46328125 * 1280; time = 0.9293s; samplesPerSecond = 1377.3
MPI Rank 2: Finished Epoch[ 7 of 10]: [Training] CE = 1.22077484 * 12500; Err = 0.44584000 * 12500; totalSamplesSeen = 87500; learningRatePerSample = 0.0040000002; epochTime=9.07825s
MPI Rank 2: Final Results: Minibatch[1-20]: CE = 1.33411228 * 10000; perplexity = 3.79662412; Err = 0.46680000 * 10000
MPI Rank 2: Finished Epoch[ 7 of 10]: [Validate] CE = 1.33411228 * 10000; Err = 0.46680000 * 10000
MPI Rank 2: 
MPI Rank 2: Starting Epoch 8: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 2: 
MPI Rank 2: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2:  Epoch[ 8 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.11248040 * 128; Err = 0.42187500 * 128; time = 0.0941s; samplesPerSecond = 1360.2
MPI Rank 2:  Epoch[ 8 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.13768310 * 1152; Err = 0.41753472 * 1152; time = 0.8339s; samplesPerSecond = 1381.5
MPI Rank 2:  Epoch[ 8 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.10279398 * 1280; Err = 0.41406250 * 1280; time = 0.9258s; samplesPerSecond = 1382.6
MPI Rank 2:  Epoch[ 8 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.13244572 * 1280; Err = 0.40937500 * 1280; time = 0.9264s; samplesPerSecond = 1381.7
MPI Rank 2:  Epoch[ 8 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.15526123 * 1280; Err = 0.41484375 * 1280; time = 0.9261s; samplesPerSecond = 1382.1
MPI Rank 2:  Epoch[ 8 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.19716148 * 1280; Err = 0.43281250 * 1280; time = 0.9279s; samplesPerSecond = 1379.4
MPI Rank 2:  Epoch[ 8 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.09121513 * 1280; Err = 0.41015625 * 1280; time = 0.9281s; samplesPerSecond = 1379.2
MPI Rank 2:  Epoch[ 8 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.10397339 * 1280; Err = 0.39218750 * 1280; time = 0.9287s; samplesPerSecond = 1378.3
MPI Rank 2:  Epoch[ 8 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.13713531 * 1280; Err = 0.39843750 * 1280; time = 0.9286s; samplesPerSecond = 1378.4
MPI Rank 2:  Epoch[ 8 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.07400208 * 1280; Err = 0.39453125 * 1280; time = 0.9296s; samplesPerSecond = 1376.9
MPI Rank 2: Finished Epoch[ 8 of 10]: [Training] CE = 1.11895180 * 12500; Err = 0.40768000 * 12500; totalSamplesSeen = 100000; learningRatePerSample = 0.0040000002; epochTime=9.0737s
MPI Rank 2: Final Results: Minibatch[1-20]: CE = 1.52033350 * 10000; perplexity = 4.57375027; Err = 0.48410000 * 10000
MPI Rank 2: Finished Epoch[ 8 of 10]: [Validate] CE = 1.52033350 * 10000; Err = 0.48410000 * 10000
MPI Rank 2: 
MPI Rank 2: Starting Epoch 9: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 2: 
MPI Rank 2: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2:  Epoch[ 9 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.21850038 * 128; Err = 0.42968750 * 128; time = 0.0959s; samplesPerSecond = 1334.7
MPI Rank 2:  Epoch[ 9 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.14268989 * 1152; Err = 0.41840278 * 1152; time = 0.8411s; samplesPerSecond = 1369.6
MPI Rank 2:  Epoch[ 9 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.07050829 * 1280; Err = 0.40390625 * 1280; time = 0.9316s; samplesPerSecond = 1374.0
MPI Rank 2:  Epoch[ 9 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.02944775 * 1280; Err = 0.35859375 * 1280; time = 0.9310s; samplesPerSecond = 1374.8
MPI Rank 2:  Epoch[ 9 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.16665611 * 1280; Err = 0.43359375 * 1280; time = 0.9307s; samplesPerSecond = 1375.4
MPI Rank 2:  Epoch[ 9 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.12256851 * 1280; Err = 0.41718750 * 1280; time = 0.9326s; samplesPerSecond = 1372.5
MPI Rank 2:  Epoch[ 9 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.08656082 * 1280; Err = 0.39765625 * 1280; time = 0.9307s; samplesPerSecond = 1375.3
MPI Rank 2:  Epoch[ 9 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.02180634 * 1280; Err = 0.36015625 * 1280; time = 0.9325s; samplesPerSecond = 1372.7
MPI Rank 2:  Epoch[ 9 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.08331299 * 1280; Err = 0.39062500 * 1280; time = 0.9317s; samplesPerSecond = 1373.9
MPI Rank 2:  Epoch[ 9 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.06549225 * 1280; Err = 0.38906250 * 1280; time = 0.9338s; samplesPerSecond = 1370.7
MPI Rank 2: Finished Epoch[ 9 of 10]: [Training] CE = 1.08531039 * 12500; Err = 0.39544000 * 12500; totalSamplesSeen = 112500; learningRatePerSample = 0.0040000002; epochTime=9.11738s
MPI Rank 2: Final Results: Minibatch[1-20]: CE = 1.39803456 * 10000; perplexity = 4.04723753; Err = 0.46810000 * 10000
MPI Rank 2: Finished Epoch[ 9 of 10]: [Validate] CE = 1.39803456 * 10000; Err = 0.46810000 * 10000
MPI Rank 2: 
MPI Rank 2: Starting Epoch 10: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 2: 
MPI Rank 2: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2:  Epoch[10 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.02113879 * 128; Err = 0.34375000 * 128; time = 0.1078s; samplesPerSecond = 1186.9
MPI Rank 2:  Epoch[10 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.02396513 * 1152; Err = 0.36892361 * 1152; time = 0.8379s; samplesPerSecond = 1374.9
MPI Rank 2:  Epoch[10 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.03566504 * 1280; Err = 0.36171875 * 1280; time = 0.9364s; samplesPerSecond = 1367.0
MPI Rank 2:  Epoch[10 of 10]-Minibatch[  21-  30, 133.33%]: CE = 0.96921520 * 1280; Err = 0.36640625 * 1280; time = 0.9312s; samplesPerSecond = 1374.6
MPI Rank 2:  Epoch[10 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.02436657 * 1280; Err = 0.36875000 * 1280; time = 0.9321s; samplesPerSecond = 1373.2
MPI Rank 2:  Epoch[10 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.02996407 * 1280; Err = 0.35859375 * 1280; time = 0.9310s; samplesPerSecond = 1374.8
MPI Rank 2:  Epoch[10 of 10]-Minibatch[  51-  60, 266.67%]: CE = 0.97493591 * 1280; Err = 0.35390625 * 1280; time = 0.9319s; samplesPerSecond = 1373.6
MPI Rank 2:  Epoch[10 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.01661072 * 1280; Err = 0.35937500 * 1280; time = 0.9311s; samplesPerSecond = 1374.7
MPI Rank 2:  Epoch[10 of 10]-Minibatch[  71-  80, 355.56%]: CE = 0.99883575 * 1280; Err = 0.35859375 * 1280; time = 0.9314s; samplesPerSecond = 1374.3
MPI Rank 2:  Epoch[10 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.07768173 * 1280; Err = 0.38828125 * 1280; time = 0.9321s; samplesPerSecond = 1373.2
MPI Rank 2: Finished Epoch[10 of 10]: [Training] CE = 1.01222555 * 12500; Err = 0.36248000 * 12500; totalSamplesSeen = 125000; learningRatePerSample = 0.0040000002; epochTime=9.13291s
MPI Rank 2: Final Results: Minibatch[1-20]: CE = 0.98009556 * 10000; perplexity = 2.66471086; Err = 0.35040000 * 10000
MPI Rank 2: Finished Epoch[10 of 10]: [Validate] CE = 0.98009556 * 10000; Err = 0.35040000 * 10000
MPI Rank 2: 
MPI Rank 2: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: COMPLETED.
MPI Rank 2: ~MPIWrapper
MPI Rank 3: Configuration After Processing and Variable Resolution:
MPI Rank 3: 
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:asyncBuffer=false
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:command=Train
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:ConfigDir=.
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:configName=4gpu-take1
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:DataDir=.
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:deviceId=auto
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:epochSize=10
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:imageLayout=cudnn
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:initOnCPUOnly=true
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:makeMode=true
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:minibatch=512
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:minibatchSize=128
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:ModelDir=./Output-4gpu-take1/Models
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:ndlMacros=./Macros.ndl
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:OutputDir=./Output-4gpu-take1
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:parallelizationMethod=DataParallelASGD
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:parallelTrain=true
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:precision=float
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:prefetch=true
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:Proj16to32Filename=./16to32.txt
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:Proj32to64Filename=./32to64.txt
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:RootDir=.
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:stderr=./Output-4gpu-take1/03_ResNet
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:Test=[
MPI Rank 3:     action = "test"
MPI Rank 3:     modelPath = "./Output-4gpu-take1/Models/03_ResNet"
MPI Rank 3:     minibatchSize = 256
MPI Rank 3:     reader = [
MPI Rank 3:         readerType = "ImageReader"
MPI Rank 3:         file = "./cifar-10-batches-py/test_map.txt"
MPI Rank 3:         randomize = "none"
MPI Rank 3:         features = [
MPI Rank 3:             width = 32
MPI Rank 3:             height = 32
MPI Rank 3:             channels = 3
MPI Rank 3:             cropType = "center"
MPI Rank 3:             cropRatio = 1
MPI Rank 3:             jitterType = "uniRatio"
MPI Rank 3:             interpolations = "linear"
MPI Rank 3:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 3:         ]
MPI Rank 3:         labels = [
MPI Rank 3:             labelDim = 10
MPI Rank 3:         ]
MPI Rank 3:     ]    
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:traceLevel=1
MPI Rank 3: configparameters: 03_ResNet-parallel.cntk:Train=[
MPI Rank 3:     action = "train"
MPI Rank 3:     modelPath = "./Output-4gpu-take1/Models/03_ResNet"
MPI Rank 3:      NDLNetworkBuilder = [
MPI Rank 3:         networkDescription = "./03_ResNet.ndl"
MPI Rank 3:     ]
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize = 0
MPI Rank 3:         minibatchSize = 512
MPI Rank 3:         learningRatesPerSample = 0.004*80:0.0004*40:0.00004
MPI Rank 3:         momentumPerMB = 0
MPI Rank 3:         maxEpochs = 10
MPI Rank 3:         L2RegWeight = 0.0001
MPI Rank 3:         dropoutRate = 0
MPI Rank 3:         perfTraceLevel = 0
MPI Rank 3:         firstMBsToShowResult = 1
MPI Rank 3:         numMBsToShowResult = 10
MPI Rank 3:         ParallelTrain = [
MPI Rank 3:             parallelizationMethod = DataParallelASGD
MPI Rank 3:             distributedMBReading = "true"
MPI Rank 3:             parallelizationStartEpoch = 1
MPI Rank 3:             DataParallelSGD = [
MPI Rank 3:                 gradientBits = 32
MPI Rank 3:                 useBufferedAsyncGradientAggregation = false
MPI Rank 3:             ]
MPI Rank 3:             ModelAveragingSGD = [
MPI Rank 3:                 blockSizePerWorker = 128
MPI Rank 3:             ]
MPI Rank 3:             DataParallelASGD = [
MPI Rank 3:                 syncPeriod = 128
MPI Rank 3:                 usePipeline = false
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3:     ]
MPI Rank 3:     reader = [
MPI Rank 3:         readerType = "ImageReader"
MPI Rank 3:         file = "./cifar-10-batches-py/train_map.txt"
MPI Rank 3:         randomize = "auto"
MPI Rank 3:         features = [
MPI Rank 3:             width = 32
MPI Rank 3:             height = 32
MPI Rank 3:             channels = 3
MPI Rank 3:             cropType = "random"
MPI Rank 3:             cropRatio = 0.8
MPI Rank 3:             jitterType = "uniRatio"
MPI Rank 3:             interpolations = "linear"
MPI Rank 3:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 3:         ]
MPI Rank 3:         labels = [
MPI Rank 3:             labelDim = 10
MPI Rank 3:         ]
MPI Rank 3:     ]
MPI Rank 3:     cvReader = [
MPI Rank 3:         readerType = "ImageReader"
MPI Rank 3:         file = "./cifar-10-batches-py/test_map.txt"
MPI Rank 3:         randomize = "none"
MPI Rank 3:         features = [
MPI Rank 3:             width = 32
MPI Rank 3:             height = 32
MPI Rank 3:             channels = 3
MPI Rank 3:             cropType = "center"
MPI Rank 3:             cropRatio = 1
MPI Rank 3:             jitterType = "uniRatio"
MPI Rank 3:             interpolations = "linear"
MPI Rank 3:             meanFile = "./cifar-10-batches-py/CIFAR-10_mean.xml"
MPI Rank 3:         ]
MPI Rank 3:         labels = [
MPI Rank 3:             labelDim = 10
MPI Rank 3:         ]
MPI Rank 3:     ]    
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: Commands: Train
MPI Rank 3: precision = "float"
MPI Rank 3: 
MPI Rank 3: ##############################################################################
MPI Rank 3: #                                                                            #
MPI Rank 3: # Train command (train action)                                               #
MPI Rank 3: #                                                                            #
MPI Rank 3: ##############################################################################
MPI Rank 3: 
MPI Rank 3: LockDevice: Failed to lock GPU 1 for exclusive use.
MPI Rank 3: LockDevice: Failed to lock GPU 7 for exclusive use.
MPI Rank 3: LockDevice: Failed to lock GPU 0 for exclusive use.
MPI Rank 3: 
MPI Rank 3: Creating virgin network.
MPI Rank 3: NDLBuilder Using GPU 2
MPI Rank 3: SetGaussianRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 3: 
MPI Rank 3: OutputNodes.t Times operation: For legacy compatibility, the sample layout of left input (OutputNodes.W LearnableParameter operation) was patched to [10 x 1 x 1 x 64] (from [10 x 64])
MPI Rank 3: conv1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 16, Kernel: 3 x 3 x 3, Map: 1 x 1 x 16, Stride: 1 x 1 x 3, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn1_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn1_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn1_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn1_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn1_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn1_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 32 x 32 x 16, Kernel: 3 x 3 x 16, Map: 1 x 1 x 16, Stride: 1 x 1 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn2_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 16 x 16 x 32, Kernel: 3 x 3 x 16, Map: 1 x 1 x 32, Stride: 2 x 2 x 16, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn2_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn2_1.c_proj.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 16, Output: 16 x 16 x 32, Kernel: 1 x 1 x 16, Map: 1 x 1 x 32, Stride: 2 x 2 x 16, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn2_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn2_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn2_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn2_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 16 x 16 x 32, Kernel: 3 x 3 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn3_1.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 8 x 8 x 64, Kernel: 3 x 3 x 32, Map: 1 x 1 x 64, Stride: 2 x 2 x 32, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn3_1.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn3_1.c_proj.c: using cuDNN convolution engine for geometry: Input: 16 x 16 x 32, Output: 8 x 8 x 64, Kernel: 1 x 1 x 32, Map: 1 x 1 x 64, Stride: 2 x 2 x 32, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn3_2.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn3_2.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn3_3.c1.c.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: rn3_3.c2.c.c: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 8 x 8 x 64, Kernel: 3 x 3 x 64, Map: 1 x 1 x 64, Stride: 1 x 1 x 64, Sharing: (1), AutoPad: (1), LowerPad: 0, UpperPad: 0.
MPI Rank 3: Using CNTK batch normalization engine.
MPI Rank 3: pool: using cuDNN convolution engine for geometry: Input: 8 x 8 x 64, Output: 1 x 1 x 64, Kernel: 8 x 8 x 1, Map: 1, Stride: 1 x 1 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
MPI Rank 3: 
MPI Rank 3: Model has 184 nodes. Using GPU 2.
MPI Rank 3: 
MPI Rank 3: Training criterion:   CE = CrossEntropyWithSoftmax
MPI Rank 3: Evaluation criterion: Err = ClassificationError
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing: Out of 321 matrices, 160 are shared as 62, and 161 are not shared.
MPI Rank 3: 
MPI Rank 3:     { pool : [1 x 1 x 64 x *] (gradient)
MPI Rank 3:       rn3_3.c2.W : [64 x 576] (gradient) }
MPI Rank 3:     { conv1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       conv1.y : [32 x 32 x 16 x *] }
MPI Rank 3:     { conv1.c.W : [16 x 27] (gradient)
MPI Rank 3:       rn1_1.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_1.c1.y : [32 x 32 x 16 x *] }
MPI Rank 3:     { conv1.c.c.b : [16 x 1] (gradient)
MPI Rank 3:       rn1_1.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_1.c1.c.W : [16 x 144] (gradient)
MPI Rank 3:       rn1_1.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_1.p : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_1.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_1.y : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_1.c2.W : [16 x 144] (gradient)
MPI Rank 3:       rn1_2.c1.c.c.c : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_1.c2.c.sc : [16 x 1] (gradient)
MPI Rank 3:       rn1_1.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 3:     { conv1.c.c.sc : [16 x 1] (gradient)
MPI Rank 3:       conv1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_2.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_2.c1.y : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_1.c2.c.b : [16 x 1] (gradient)
MPI Rank 3:       rn1_2.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_2.c1.c.W : [16 x 144] (gradient)
MPI Rank 3:       rn1_2.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_2.p : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_2.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_2.y : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_2.c2.W : [16 x 144] (gradient)
MPI Rank 3:       rn1_3.c1.c.c.c : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_2.c2.c.sc : [16 x 1] (gradient)
MPI Rank 3:       rn1_2.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 3:     { rn1_1.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 3:       rn1_1.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_3.c1.c.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_3.c1.y : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_2.c2.c.b : [16 x 1] (gradient)
MPI Rank 3:       rn1_3.c2.c.c : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_3.c1.c.W : [16 x 144] (gradient)
MPI Rank 3:       rn1_3.c2.c.c : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_3.p : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_3.c2.c.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_3.y : [32 x 32 x 16 x *] }
MPI Rank 3:     { rn1_3.c2.W : [16 x 144] (gradient)
MPI Rank 3:       rn2_1.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn1_3.c2.c.sc : [16 x 1] (gradient)
MPI Rank 3:       rn1_3.p : [32 x 32 x 16 x *] (gradient) }
MPI Rank 3:     { rn1_2.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 3:       rn1_2.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_2.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn2_1.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_1.c1.y : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn1_3.c2.c.b : [16 x 1] (gradient)
MPI Rank 3:       rn2_1.c2.c.c : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_1.c2.c.sc : [32 x 1] (gradient)
MPI Rank 3:       rn2_1.c_proj.c : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_1.c1.c.W : [32 x 144] (gradient)
MPI Rank 3:       rn2_1.c2.c.c : [16 x 16 x 32 x *] (gradient) }
MPI Rank 3:     { rn2_1.c2.c.b : [32 x 1] (gradient)
MPI Rank 3:       rn2_1.c_proj.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_1.p : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_1.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_1.y : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_1.c2.W : [32 x 288] (gradient)
MPI Rank 3:       rn2_2.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_1.c_proj.sc : [32 x 1] (gradient)
MPI Rank 3:       rn2_1.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 3:     { rn1_3.c1.c.c.sc : [16 x 1] (gradient)
MPI Rank 3:       rn1_3.c1.y : [32 x 32 x 16 x *] (gradient)
MPI Rank 3:       rn1_3.y : [32 x 32 x 16 x *] (gradient) }
MPI Rank 3:     { rn2_1.c_proj.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_2.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_2.c1.y : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_2.c1.c.W : [32 x 288] (gradient)
MPI Rank 3:       rn2_2.c2.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_2.p : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_2.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_2.y : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_2.c2.W : [32 x 288] (gradient)
MPI Rank 3:       rn2_3.c1.c.c.c : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_2.c2.c.sc : [32 x 1] (gradient)
MPI Rank 3:       rn2_2.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 3:     { rn2_1.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 3:       rn2_1.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_3.c1.c.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_3.c1.y : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_2.c2.c.b : [32 x 1] (gradient)
MPI Rank 3:       rn2_3.c2.c.c : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_3.c1.c.W : [32 x 288] (gradient)
MPI Rank 3:       rn2_3.c2.c.c : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_3.p : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_3.c2.c.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_3.y : [16 x 16 x 32 x *] }
MPI Rank 3:     { rn2_3.c2.W : [32 x 288] (gradient)
MPI Rank 3:       rn3_1.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn2_3.c2.c.sc : [32 x 1] (gradient)
MPI Rank 3:       rn2_3.p : [16 x 16 x 32 x *] (gradient) }
MPI Rank 3:     { rn2_2.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 3:       rn2_2.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_2.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn3_1.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_1.c1.y : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn2_3.c2.c.b : [32 x 1] (gradient)
MPI Rank 3:       rn3_1.c2.c.c : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_1.c2.c.sc : [64 x 1] (gradient)
MPI Rank 3:       rn3_1.c_proj.c : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_1.c1.c.W : [64 x 288] (gradient)
MPI Rank 3:       rn3_1.c2.c.c : [8 x 8 x 64 x *] (gradient) }
MPI Rank 3:     { rn3_1.c2.c.b : [64 x 1] (gradient)
MPI Rank 3:       rn3_1.c_proj.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_1.p : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_1.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_1.y : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_1.c2.W : [64 x 576] (gradient)
MPI Rank 3:       rn3_2.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_1.c_proj.sc : [64 x 1] (gradient)
MPI Rank 3:       rn3_1.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 3:     { rn2_3.c1.c.c.sc : [32 x 1] (gradient)
MPI Rank 3:       rn2_3.c1.y : [16 x 16 x 32 x *] (gradient)
MPI Rank 3:       rn2_3.y : [16 x 16 x 32 x *] (gradient) }
MPI Rank 3:     { rn3_1.c_proj.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_2.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_2.c1.y : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_2.c1.c.W : [64 x 576] (gradient)
MPI Rank 3:       rn3_2.c2.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_2.p : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_2.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_2.y : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_2.c2.W : [64 x 576] (gradient)
MPI Rank 3:       rn3_3.c1.c.c.c : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_2.c2.c.sc : [64 x 1] (gradient)
MPI Rank 3:       rn3_2.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 3:     { rn3_1.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 3:       rn3_1.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_3.c1.c.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_3.c1.y : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_2.c2.c.b : [64 x 1] (gradient)
MPI Rank 3:       rn3_3.c2.c.c : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_3.c1.c.W : [64 x 576] (gradient)
MPI Rank 3:       rn3_3.c2.c.c : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_3.p : [8 x 8 x 64 x *] }
MPI Rank 3:     { rn3_3.c2.c.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_3.y : [8 x 8 x 64 x *] }
MPI Rank 3:     { pool : [1 x 1 x 64 x *]
MPI Rank 3:       rn3_3.c2.c.sc : [64 x 1] (gradient)
MPI Rank 3:       rn3_3.p : [8 x 8 x 64 x *] (gradient) }
MPI Rank 3:     { OutputNodes.t : [10 x *]
MPI Rank 3:       rn3_3.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 3:       rn3_3.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_3.y : [8 x 8 x 64 x *] (gradient) }
MPI Rank 3:     { OutputNodes.W : [10 x 1 x 1 x 64] (gradient)
MPI Rank 3:       OutputNodes.z : [10 x *] (gradient) }
MPI Rank 3:     { OutputNodes.t : [10 x *] (gradient)
MPI Rank 3:       rn3_2.c1.c.c.sc : [64 x 1] (gradient)
MPI Rank 3:       rn3_2.c1.y : [8 x 8 x 64 x *] (gradient)
MPI Rank 3:       rn3_2.y : [8 x 8 x 64 x *] (gradient) }
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Training 269914 parameters in 63 out of 63 parameter tensors and 137 nodes with gradient:
MPI Rank 3: 
MPI Rank 3:     Node 'OutputNodes.W' (LearnableParameter operation) : [10 x 1 x 1 x 64]
MPI Rank 3:     Node 'OutputNodes.b' (LearnableParameter operation) : [10]
MPI Rank 3:     Node 'conv1.c.W' (LearnableParameter operation) : [16 x 27]
MPI Rank 3:     Node 'conv1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'conv1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_1.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 3:     Node 'rn1_1.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_1.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_1.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 3:     Node 'rn1_1.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_1.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_2.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 3:     Node 'rn1_2.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_2.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_2.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 3:     Node 'rn1_2.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_2.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_3.c1.c.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 3:     Node 'rn1_3.c1.c.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_3.c1.c.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_3.c2.W' (LearnableParameter operation) : [16 x 144]
MPI Rank 3:     Node 'rn1_3.c2.c.b' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn1_3.c2.c.sc' (LearnableParameter operation) : [16 x 1]
MPI Rank 3:     Node 'rn2_1.c1.c.W' (LearnableParameter operation) : [32 x 144]
MPI Rank 3:     Node 'rn2_1.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_1.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_1.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 3:     Node 'rn2_1.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_1.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_1.c_proj.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_1.c_proj.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_2.c1.c.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 3:     Node 'rn2_2.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_2.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_2.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 3:     Node 'rn2_2.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_2.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_3.c1.c.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 3:     Node 'rn2_3.c1.c.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_3.c1.c.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_3.c2.W' (LearnableParameter operation) : [32 x 288]
MPI Rank 3:     Node 'rn2_3.c2.c.b' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn2_3.c2.c.sc' (LearnableParameter operation) : [32 x 1]
MPI Rank 3:     Node 'rn3_1.c1.c.W' (LearnableParameter operation) : [64 x 288]
MPI Rank 3:     Node 'rn3_1.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_1.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_1.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 3:     Node 'rn3_1.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_1.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_1.c_proj.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_1.c_proj.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_2.c1.c.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 3:     Node 'rn3_2.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_2.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_2.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 3:     Node 'rn3_2.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_2.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_3.c1.c.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 3:     Node 'rn3_3.c1.c.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_3.c1.c.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_3.c2.W' (LearnableParameter operation) : [64 x 576]
MPI Rank 3:     Node 'rn3_3.c2.c.b' (LearnableParameter operation) : [64 x 1]
MPI Rank 3:     Node 'rn3_3.c2.c.sc' (LearnableParameter operation) : [64 x 1]
MPI Rank 3: 
MPI Rank 3: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 3: 
MPI Rank 3: Starting Epoch 1: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 3: 
MPI Rank 3: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3:  Epoch[ 1 of 10]-Minibatch[   1-   1]: CE = 2.30333328 * 128; Err = 0.90625000 * 128; time = 2.8960s; samplesPerSecond = 44.2
MPI Rank 3:  Epoch[ 1 of 10]-Minibatch[   2-  10]: CE = 2.75547388 * 1152; Err = 0.90017361 * 1152; time = 0.8376s; samplesPerSecond = 1375.4
MPI Rank 3:  Epoch[ 1 of 10]-Minibatch[  11-  20]: CE = 2.27822094 * 1280; Err = 0.86093750 * 1280; time = 0.9296s; samplesPerSecond = 1376.9
MPI Rank 3:  Epoch[ 1 of 10]-Minibatch[  21-  30]: CE = 2.18503761 * 1280; Err = 0.79921875 * 1280; time = 0.9301s; samplesPerSecond = 1376.2
MPI Rank 3:  Epoch[ 1 of 10]-Minibatch[  31-  40]: CE = 2.06597900 * 1280; Err = 0.78125000 * 1280; time = 0.9306s; samplesPerSecond = 1375.5
MPI Rank 3:  Epoch[ 1 of 10]-Minibatch[  41-  50]: CE = 2.01211395 * 1280; Err = 0.77421875 * 1280; time = 0.9304s; samplesPerSecond = 1375.7
MPI Rank 3:  Epoch[ 1 of 10]-Minibatch[  51-  60]: CE = 1.96677399 * 1280; Err = 0.75859375 * 1280; time = 0.9304s; samplesPerSecond = 1375.7
MPI Rank 3:  Epoch[ 1 of 10]-Minibatch[  61-  70]: CE = 2.01400299 * 1280; Err = 0.78046875 * 1280; time = 0.9300s; samplesPerSecond = 1376.4
MPI Rank 3:  Epoch[ 1 of 10]-Minibatch[  71-  80]: CE = 1.83690491 * 1280; Err = 0.72656250 * 1280; time = 0.9310s; samplesPerSecond = 1374.9
MPI Rank 3:  Epoch[ 1 of 10]-Minibatch[  81-  90]: CE = 1.88523102 * 1280; Err = 0.71875000 * 1280; time = 0.9300s; samplesPerSecond = 1376.3
MPI Rank 3: Finished Epoch[ 1 of 10]: [Training] CE = 2.08759438 * 12500; Err = 0.78416000 * 12500; totalSamplesSeen = 12500; learningRatePerSample = 0.0040000002; epochTime=11.9015s
MPI Rank 3: Final Results: Minibatch[1-20]: CE = 2.43377809 * 10000; perplexity = 11.40187818; Err = 0.79740000 * 10000
MPI Rank 3: Finished Epoch[ 1 of 10]: [Validate] CE = 2.43377809 * 10000; Err = 0.79740000 * 10000
MPI Rank 3: 
MPI Rank 3: Starting Epoch 2: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 3: 
MPI Rank 3: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3:  Epoch[ 2 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.89088452 * 128; Err = 0.75000000 * 128; time = 0.0931s; samplesPerSecond = 1374.1
MPI Rank 3:  Epoch[ 2 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.84145990 * 1152; Err = 0.71527778 * 1152; time = 0.8333s; samplesPerSecond = 1382.4
MPI Rank 3:  Epoch[ 2 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.80466576 * 1280; Err = 0.69687500 * 1280; time = 0.9274s; samplesPerSecond = 1380.1
MPI Rank 3:  Epoch[ 2 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.76080399 * 1280; Err = 0.66171875 * 1280; time = 0.9278s; samplesPerSecond = 1379.6
MPI Rank 3:  Epoch[ 2 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.74784012 * 1280; Err = 0.69140625 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 3:  Epoch[ 2 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.69956512 * 1280; Err = 0.64843750 * 1280; time = 0.9275s; samplesPerSecond = 1380.1
MPI Rank 3:  Epoch[ 2 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.73585663 * 1280; Err = 0.67031250 * 1280; time = 0.9278s; samplesPerSecond = 1379.7
MPI Rank 3:  Epoch[ 2 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.72769928 * 1280; Err = 0.65546875 * 1280; time = 0.9275s; samplesPerSecond = 1380.0
MPI Rank 3:  Epoch[ 2 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.69202881 * 1280; Err = 0.64218750 * 1280; time = 0.9278s; samplesPerSecond = 1379.6
MPI Rank 3:  Epoch[ 2 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.67485809 * 1280; Err = 0.62500000 * 1280; time = 0.9279s; samplesPerSecond = 1379.4
MPI Rank 3: Finished Epoch[ 2 of 10]: [Training] CE = 1.73884391 * 12500; Err = 0.66704000 * 12500; totalSamplesSeen = 25000; learningRatePerSample = 0.0040000002; epochTime=9.06876s
MPI Rank 3: Final Results: Minibatch[1-20]: CE = 1.74458469 * 10000; perplexity = 5.72352392; Err = 0.65490000 * 10000
MPI Rank 3: Finished Epoch[ 2 of 10]: [Validate] CE = 1.74458469 * 10000; Err = 0.65490000 * 10000
MPI Rank 3: 
MPI Rank 3: Starting Epoch 3: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 3: 
MPI Rank 3: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3:  Epoch[ 3 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.88070107 * 128; Err = 0.67968750 * 128; time = 0.0900s; samplesPerSecond = 1422.7
MPI Rank 3:  Epoch[ 3 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.61596553 * 1152; Err = 0.61197917 * 1152; time = 0.8327s; samplesPerSecond = 1383.5
MPI Rank 3:  Epoch[ 3 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.62421894 * 1280; Err = 0.61015625 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 3:  Epoch[ 3 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.67533379 * 1280; Err = 0.61562500 * 1280; time = 0.9267s; samplesPerSecond = 1381.2
MPI Rank 3:  Epoch[ 3 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.60124626 * 1280; Err = 0.61328125 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 3:  Epoch[ 3 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.60940399 * 1280; Err = 0.60781250 * 1280; time = 0.9269s; samplesPerSecond = 1380.9
MPI Rank 3:  Epoch[ 3 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.56083832 * 1280; Err = 0.58593750 * 1280; time = 0.9277s; samplesPerSecond = 1379.7
MPI Rank 3:  Epoch[ 3 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.61849136 * 1280; Err = 0.60000000 * 1280; time = 0.9277s; samplesPerSecond = 1379.8
MPI Rank 3:  Epoch[ 3 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.57575073 * 1280; Err = 0.61953125 * 1280; time = 0.9270s; samplesPerSecond = 1380.8
MPI Rank 3:  Epoch[ 3 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.56731415 * 1280; Err = 0.60937500 * 1280; time = 0.9289s; samplesPerSecond = 1378.0
MPI Rank 3: Finished Epoch[ 3 of 10]: [Training] CE = 1.60329891 * 12500; Err = 0.60712000 * 12500; totalSamplesSeen = 37500; learningRatePerSample = 0.0040000002; epochTime=9.06282s
MPI Rank 3: Final Results: Minibatch[1-20]: CE = 1.61874455 * 10000; perplexity = 5.04675038; Err = 0.60020000 * 10000
MPI Rank 3: Finished Epoch[ 3 of 10]: [Validate] CE = 1.61874455 * 10000; Err = 0.60020000 * 10000
MPI Rank 3: 
MPI Rank 3: Starting Epoch 4: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 3: 
MPI Rank 3: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3:  Epoch[ 4 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.65222073 * 128; Err = 0.64062500 * 128; time = 0.0905s; samplesPerSecond = 1414.6
MPI Rank 3:  Epoch[ 4 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.52860737 * 1152; Err = 0.57812500 * 1152; time = 0.8330s; samplesPerSecond = 1383.0
MPI Rank 3:  Epoch[ 4 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.52298717 * 1280; Err = 0.57968750 * 1280; time = 0.9262s; samplesPerSecond = 1382.1
MPI Rank 3:  Epoch[ 4 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.45207043 * 1280; Err = 0.52265625 * 1280; time = 0.9274s; samplesPerSecond = 1380.2
MPI Rank 3:  Epoch[ 4 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.54776726 * 1280; Err = 0.59140625 * 1280; time = 0.9278s; samplesPerSecond = 1379.6
MPI Rank 3:  Epoch[ 4 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.50887260 * 1280; Err = 0.56640625 * 1280; time = 0.9272s; samplesPerSecond = 1380.4
MPI Rank 3:  Epoch[ 4 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.46521072 * 1280; Err = 0.56406250 * 1280; time = 0.9268s; samplesPerSecond = 1381.1
MPI Rank 3:  Epoch[ 4 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.37703629 * 1280; Err = 0.51015625 * 1280; time = 0.9272s; samplesPerSecond = 1380.5
MPI Rank 3:  Epoch[ 4 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.45106812 * 1280; Err = 0.52578125 * 1280; time = 0.9268s; samplesPerSecond = 1381.0
MPI Rank 3:  Epoch[ 4 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.40172043 * 1280; Err = 0.50156250 * 1280; time = 0.9272s; samplesPerSecond = 1380.6
MPI Rank 3: Finished Epoch[ 4 of 10]: [Training] CE = 1.46810266 * 12500; Err = 0.54736000 * 12500; totalSamplesSeen = 50000; learningRatePerSample = 0.0040000002; epochTime=9.06074s
MPI Rank 3: Final Results: Minibatch[1-20]: CE = 1.60729235 * 10000; perplexity = 4.98928367; Err = 0.58020000 * 10000
MPI Rank 3: Finished Epoch[ 4 of 10]: [Validate] CE = 1.60729235 * 10000; Err = 0.58020000 * 10000
MPI Rank 3: 
MPI Rank 3: Starting Epoch 5: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 3: 
MPI Rank 3: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3:  Epoch[ 5 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.43907487 * 128; Err = 0.46875000 * 128; time = 0.0918s; samplesPerSecond = 1394.1
MPI Rank 3:  Epoch[ 5 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.40758701 * 1152; Err = 0.49826389 * 1152; time = 0.8327s; samplesPerSecond = 1383.5
MPI Rank 3:  Epoch[ 5 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.39019270 * 1280; Err = 0.52343750 * 1280; time = 0.9258s; samplesPerSecond = 1382.5
MPI Rank 3:  Epoch[ 5 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.44101410 * 1280; Err = 0.55078125 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 3:  Epoch[ 5 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.33823318 * 1280; Err = 0.49140625 * 1280; time = 0.9268s; samplesPerSecond = 1381.1
MPI Rank 3:  Epoch[ 5 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.32283020 * 1280; Err = 0.47968750 * 1280; time = 0.9268s; samplesPerSecond = 1381.0
MPI Rank 3:  Epoch[ 5 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.36808624 * 1280; Err = 0.50312500 * 1280; time = 0.9264s; samplesPerSecond = 1381.8
MPI Rank 3:  Epoch[ 5 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.28800735 * 1280; Err = 0.47812500 * 1280; time = 0.9273s; samplesPerSecond = 1380.4
MPI Rank 3:  Epoch[ 5 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.31306915 * 1280; Err = 0.48593750 * 1280; time = 0.9273s; samplesPerSecond = 1380.3
MPI Rank 3:  Epoch[ 5 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.26314926 * 1280; Err = 0.45000000 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 3: Finished Epoch[ 5 of 10]: [Training] CE = 1.35411094 * 12500; Err = 0.49824000 * 12500; totalSamplesSeen = 62500; learningRatePerSample = 0.0040000002; epochTime=9.05816s
MPI Rank 3: Final Results: Minibatch[1-20]: CE = 1.44262655 * 10000; perplexity = 4.23179625; Err = 0.51820000 * 10000
MPI Rank 3: Finished Epoch[ 5 of 10]: [Validate] CE = 1.44262655 * 10000; Err = 0.51820000 * 10000
MPI Rank 3: 
MPI Rank 3: Starting Epoch 6: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 3: 
MPI Rank 3: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3:  Epoch[ 6 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.28053093 * 128; Err = 0.45312500 * 128; time = 0.0905s; samplesPerSecond = 1413.8
MPI Rank 3:  Epoch[ 6 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.28805097 * 1152; Err = 0.46006944 * 1152; time = 0.8332s; samplesPerSecond = 1382.7
MPI Rank 3:  Epoch[ 6 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.36275539 * 1280; Err = 0.49921875 * 1280; time = 0.9279s; samplesPerSecond = 1379.4
MPI Rank 3:  Epoch[ 6 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.30200901 * 1280; Err = 0.47812500 * 1280; time = 0.9270s; samplesPerSecond = 1380.8
MPI Rank 3:  Epoch[ 6 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.26499939 * 1280; Err = 0.45781250 * 1280; time = 0.9278s; samplesPerSecond = 1379.7
MPI Rank 3:  Epoch[ 6 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.24043846 * 1280; Err = 0.45078125 * 1280; time = 0.9269s; samplesPerSecond = 1381.0
MPI Rank 3:  Epoch[ 6 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.26955032 * 1280; Err = 0.46484375 * 1280; time = 0.9267s; samplesPerSecond = 1381.2
MPI Rank 3:  Epoch[ 6 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.22381134 * 1280; Err = 0.43984375 * 1280; time = 0.9273s; samplesPerSecond = 1380.4
MPI Rank 3:  Epoch[ 6 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.23573380 * 1280; Err = 0.44531250 * 1280; time = 0.9268s; samplesPerSecond = 1381.1
MPI Rank 3:  Epoch[ 6 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.20773468 * 1280; Err = 0.44453125 * 1280; time = 0.9268s; samplesPerSecond = 1381.0
MPI Rank 3: Finished Epoch[ 6 of 10]: [Training] CE = 1.26467961 * 12500; Err = 0.46008000 * 12500; totalSamplesSeen = 75000; learningRatePerSample = 0.0040000002; epochTime=9.06178s
MPI Rank 3: Final Results: Minibatch[1-20]: CE = 1.25039600 * 10000; perplexity = 3.49172541; Err = 0.45500000 * 10000
MPI Rank 3: Finished Epoch[ 6 of 10]: [Validate] CE = 1.25039600 * 10000; Err = 0.45500000 * 10000
MPI Rank 3: 
MPI Rank 3: Starting Epoch 7: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 3: 
MPI Rank 3: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3:  Epoch[ 7 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.26244330 * 128; Err = 0.48437500 * 128; time = 0.0898s; samplesPerSecond = 1424.6
MPI Rank 3:  Epoch[ 7 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.22284860 * 1152; Err = 0.45920139 * 1152; time = 0.8333s; samplesPerSecond = 1382.5
MPI Rank 3:  Epoch[ 7 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.22770472 * 1280; Err = 0.44765625 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 3:  Epoch[ 7 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.17119064 * 1280; Err = 0.42421875 * 1280; time = 0.9276s; samplesPerSecond = 1379.9
MPI Rank 3:  Epoch[ 7 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.16060944 * 1280; Err = 0.41406250 * 1280; time = 0.9278s; samplesPerSecond = 1379.7
MPI Rank 3:  Epoch[ 7 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.23770790 * 1280; Err = 0.44687500 * 1280; time = 0.9278s; samplesPerSecond = 1379.6
MPI Rank 3:  Epoch[ 7 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.19030647 * 1280; Err = 0.42812500 * 1280; time = 0.9278s; samplesPerSecond = 1379.6
MPI Rank 3:  Epoch[ 7 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.14614487 * 1280; Err = 0.41562500 * 1280; time = 0.9280s; samplesPerSecond = 1379.4
MPI Rank 3:  Epoch[ 7 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.19225769 * 1280; Err = 0.43750000 * 1280; time = 0.9280s; samplesPerSecond = 1379.3
MPI Rank 3:  Epoch[ 7 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.16186066 * 1280; Err = 0.42031250 * 1280; time = 0.9275s; samplesPerSecond = 1380.0
MPI Rank 3: Finished Epoch[ 7 of 10]: [Training] CE = 1.18435633 * 12500; Err = 0.43176000 * 12500; totalSamplesSeen = 87500; learningRatePerSample = 0.0040000002; epochTime=9.06599s
MPI Rank 3: Final Results: Minibatch[1-20]: CE = 1.60055994 * 10000; perplexity = 4.95580662; Err = 0.51040000 * 10000
MPI Rank 3: Finished Epoch[ 7 of 10]: [Validate] CE = 1.60055994 * 10000; Err = 0.51040000 * 10000
MPI Rank 3: 
MPI Rank 3: Starting Epoch 8: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 3: 
MPI Rank 3: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3:  Epoch[ 8 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.20213258 * 128; Err = 0.42187500 * 128; time = 0.0899s; samplesPerSecond = 1424.4
MPI Rank 3:  Epoch[ 8 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.14707773 * 1152; Err = 0.41059028 * 1152; time = 0.8321s; samplesPerSecond = 1384.4
MPI Rank 3:  Epoch[ 8 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.16835451 * 1280; Err = 0.41718750 * 1280; time = 0.9254s; samplesPerSecond = 1383.1
MPI Rank 3:  Epoch[ 8 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.17756500 * 1280; Err = 0.42109375 * 1280; time = 0.9256s; samplesPerSecond = 1382.8
MPI Rank 3:  Epoch[ 8 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.12900200 * 1280; Err = 0.40937500 * 1280; time = 0.9265s; samplesPerSecond = 1381.5
MPI Rank 3:  Epoch[ 8 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.06216316 * 1280; Err = 0.38671875 * 1280; time = 0.9272s; samplesPerSecond = 1380.5
MPI Rank 3:  Epoch[ 8 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.07322044 * 1280; Err = 0.40312500 * 1280; time = 0.9264s; samplesPerSecond = 1381.7
MPI Rank 3:  Epoch[ 8 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.14764709 * 1280; Err = 0.39687500 * 1280; time = 0.9270s; samplesPerSecond = 1380.8
MPI Rank 3:  Epoch[ 8 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.14328232 * 1280; Err = 0.41562500 * 1280; time = 0.9277s; samplesPerSecond = 1379.7
MPI Rank 3:  Epoch[ 8 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.06364441 * 1280; Err = 0.37500000 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 3: Finished Epoch[ 8 of 10]: [Training] CE = 1.11978328 * 12500; Err = 0.40264000 * 12500; totalSamplesSeen = 100000; learningRatePerSample = 0.0040000002; epochTime=9.05753s
MPI Rank 3: Final Results: Minibatch[1-20]: CE = 1.23558414 * 10000; perplexity = 3.44038761; Err = 0.45070000 * 10000
MPI Rank 3: Finished Epoch[ 8 of 10]: [Validate] CE = 1.23558414 * 10000; Err = 0.45070000 * 10000
MPI Rank 3: 
MPI Rank 3: Starting Epoch 9: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 3: 
MPI Rank 3: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3:  Epoch[ 9 of 10]-Minibatch[   1-   1, 4.44%]: CE = 1.06330514 * 128; Err = 0.40625000 * 128; time = 0.0995s; samplesPerSecond = 1285.9
MPI Rank 3:  Epoch[ 9 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.02786379 * 1152; Err = 0.36371528 * 1152; time = 0.8342s; samplesPerSecond = 1380.9
MPI Rank 3:  Epoch[ 9 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.12283535 * 1280; Err = 0.40000000 * 1280; time = 0.9286s; samplesPerSecond = 1378.4
MPI Rank 3:  Epoch[ 9 of 10]-Minibatch[  21-  30, 133.33%]: CE = 1.07076550 * 1280; Err = 0.39375000 * 1280; time = 0.9267s; samplesPerSecond = 1381.3
MPI Rank 3:  Epoch[ 9 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.03006325 * 1280; Err = 0.37812500 * 1280; time = 0.9299s; samplesPerSecond = 1376.4
MPI Rank 3:  Epoch[ 9 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.07991714 * 1280; Err = 0.38515625 * 1280; time = 0.9266s; samplesPerSecond = 1381.4
MPI Rank 3:  Epoch[ 9 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.09335785 * 1280; Err = 0.39375000 * 1280; time = 0.9272s; samplesPerSecond = 1380.4
MPI Rank 3:  Epoch[ 9 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.05563354 * 1280; Err = 0.36718750 * 1280; time = 0.9262s; samplesPerSecond = 1382.0
MPI Rank 3:  Epoch[ 9 of 10]-Minibatch[  71-  80, 355.56%]: CE = 1.03740921 * 1280; Err = 0.39062500 * 1280; time = 0.9272s; samplesPerSecond = 1380.5
MPI Rank 3:  Epoch[ 9 of 10]-Minibatch[  81-  90, 400.00%]: CE = 1.03251343 * 1280; Err = 0.35546875 * 1280; time = 0.9259s; samplesPerSecond = 1382.4
MPI Rank 3: Finished Epoch[ 9 of 10]: [Training] CE = 1.05663891 * 12500; Err = 0.38008000 * 12500; totalSamplesSeen = 112500; learningRatePerSample = 0.0040000002; epochTime=9.07408s
MPI Rank 3: Final Results: Minibatch[1-20]: CE = 1.08704030 * 10000; perplexity = 2.96548411; Err = 0.39480000 * 10000
MPI Rank 3: Finished Epoch[ 9 of 10]: [Validate] CE = 1.08704030 * 10000; Err = 0.39480000 * 10000
MPI Rank 3: 
MPI Rank 3: Starting Epoch 10: learning rate per sample = 0.004000  effective momentum = 0.000000  momentum as time constant = 0.0 samples
MPI Rank 3: 
MPI Rank 3: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3:  Epoch[10 of 10]-Minibatch[   1-   1, 4.44%]: CE = 0.87801558 * 128; Err = 0.31250000 * 128; time = 0.1047s; samplesPerSecond = 1222.3
MPI Rank 3:  Epoch[10 of 10]-Minibatch[   2-  10, 44.44%]: CE = 1.02677472 * 1152; Err = 0.36979167 * 1152; time = 0.8329s; samplesPerSecond = 1383.1
MPI Rank 3:  Epoch[10 of 10]-Minibatch[  11-  20, 88.89%]: CE = 1.01339226 * 1280; Err = 0.36796875 * 1280; time = 0.9271s; samplesPerSecond = 1380.7
MPI Rank 3:  Epoch[10 of 10]-Minibatch[  21-  30, 133.33%]: CE = 0.99107285 * 1280; Err = 0.37578125 * 1280; time = 0.9253s; samplesPerSecond = 1383.3
MPI Rank 3:  Epoch[10 of 10]-Minibatch[  31-  40, 177.78%]: CE = 1.03633919 * 1280; Err = 0.35078125 * 1280; time = 0.9268s; samplesPerSecond = 1381.1
MPI Rank 3:  Epoch[10 of 10]-Minibatch[  41-  50, 222.22%]: CE = 1.00550079 * 1280; Err = 0.37343750 * 1280; time = 0.9263s; samplesPerSecond = 1381.8
MPI Rank 3:  Epoch[10 of 10]-Minibatch[  51-  60, 266.67%]: CE = 1.00788651 * 1280; Err = 0.34765625 * 1280; time = 0.9261s; samplesPerSecond = 1382.2
MPI Rank 3:  Epoch[10 of 10]-Minibatch[  61-  70, 311.11%]: CE = 1.01046448 * 1280; Err = 0.36640625 * 1280; time = 0.9256s; samplesPerSecond = 1382.9
MPI Rank 3:  Epoch[10 of 10]-Minibatch[  71-  80, 355.56%]: CE = 0.96965714 * 1280; Err = 0.35156250 * 1280; time = 0.9281s; samplesPerSecond = 1379.1
MPI Rank 3:  Epoch[10 of 10]-Minibatch[  81-  90, 400.00%]: CE = 0.98041458 * 1280; Err = 0.33750000 * 1280; time = 0.9253s; samplesPerSecond = 1383.4
MPI Rank 3: Finished Epoch[10 of 10]: [Training] CE = 1.00519164 * 12500; Err = 0.35976000 * 12500; totalSamplesSeen = 125000; learningRatePerSample = 0.0040000002; epochTime=9.06915s
MPI Rank 3: Final Results: Minibatch[1-20]: CE = 1.23569298 * 10000; perplexity = 3.44076209; Err = 0.42500000 * 10000
MPI Rank 3: Finished Epoch[10 of 10]: [Validate] CE = 1.23569298 * 10000; Err = 0.42500000 * 10000
MPI Rank 3: 
MPI Rank 3: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: COMPLETED.
MPI Rank 3: ~MPIWrapper
