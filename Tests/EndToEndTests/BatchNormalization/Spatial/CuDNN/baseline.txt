CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz
    Hardware threads: 12
    Total Memory: 33474872 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/repos/CNTK/x64/release/cntk.exe configFile=C:\repos\CNTK\Tests\EndToEndTests\BatchNormalization\Spatial/02_BatchNormConv.cntk currentDirectory=C:\repos\CNTK\Examples\Image\DataSets\CIFAR-10 RunDir=C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu DataDir=C:\repos\CNTK\Examples\Image\DataSets\CIFAR-10 ConfigDir=C:\repos\CNTK\Tests\EndToEndTests\BatchNormalization\Spatial OutputDir=C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu DeviceId=0 timestamping=true batchNormalizationEngine=cudnn
CNTK 2.1+ (t-ivrodr/v2tests_to_boost_squashed 962593, Aug 18 2017 12:41:36) on DESKTOP-9TF8PB8 at 2017/08/22 13:28:13

C:\repos\CNTK\x64\release\cntk.exe  configFile=C:\repos\CNTK\Tests\EndToEndTests\BatchNormalization\Spatial/02_BatchNormConv.cntk  currentDirectory=C:\repos\CNTK\Examples\Image\DataSets\CIFAR-10  RunDir=C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu  DataDir=C:\repos\CNTK\Examples\Image\DataSets\CIFAR-10  ConfigDir=C:\repos\CNTK\Tests\EndToEndTests\BatchNormalization\Spatial  OutputDir=C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu  DeviceId=0  timestamping=true  batchNormalizationEngine=cudnn
Changed current directory to C:\repos\CNTK\Examples\Image\DataSets\CIFAR-10
-------------------------------------------------------------------
Build info: 

		Built time: Aug 18 2017 12:40:12
		Last modified date: Thu Aug  3 17:41:03 2017
		Build type: Release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0
		CUB_PATH: c:\local\cub-1.4.1
		CUDNN_PATH: c:\local\cudnn-8.0-v5.1\cuda
		Build Branch: t-ivrodr/small_fixes_to_training_log
		Build SHA1: 57260963c605c12d3796e37783433518ab8ba039 (modified)
		Built by t-ivrodr on DESKTOP-9TF8PB8
		Build Path: C:\repos\CNTK\Source\CNTKv2LibraryDll\
		MPI distribution: Microsoft MPI
		MPI version: 7.0.12437.6
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; total memory = 8192 MB; free memory = 6842 MB
		Device[1]: cores = 96; computeCapability = 2.1; type = "Quadro 600"; total memory = 1024 MB; free memory = 979 MB
-------------------------------------------------------------------

Configuration After Processing and Variable Resolution:

configparameters: 02_BatchNormConv.cntk:batchNormalizationEngine=cudnn
configparameters: 02_BatchNormConv.cntk:command=Train:Test
configparameters: 02_BatchNormConv.cntk:ConfigDir=C:\repos\CNTK\Tests\EndToEndTests\BatchNormalization\Spatial
configparameters: 02_BatchNormConv.cntk:currentDirectory=C:\repos\CNTK\Examples\Image\DataSets\CIFAR-10
configparameters: 02_BatchNormConv.cntk:DataDir=C:\repos\CNTK\Examples\Image\DataSets\CIFAR-10
configparameters: 02_BatchNormConv.cntk:deviceId=0
configparameters: 02_BatchNormConv.cntk:imageLayout=cudnn
configparameters: 02_BatchNormConv.cntk:initOnCPUOnly=true
configparameters: 02_BatchNormConv.cntk:ModelDir=C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu/Models
configparameters: 02_BatchNormConv.cntk:ndlMacros=C:\repos\CNTK\Tests\EndToEndTests\BatchNormalization\Spatial/Macros.ndl
configparameters: 02_BatchNormConv.cntk:numMBsToShowResult=500
configparameters: 02_BatchNormConv.cntk:OutputDir=C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu
configparameters: 02_BatchNormConv.cntk:precision=float
configparameters: 02_BatchNormConv.cntk:RootDir=.
configparameters: 02_BatchNormConv.cntk:RunDir=C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu
configparameters: 02_BatchNormConv.cntk:Test=[
    action = "test"
    modelPath = "C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu/Models/02_BatchNormConv"
    minibatchSize = 16
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "C:\repos\CNTK\Examples\Image\DataSets\CIFAR-10/Test_cntk_text.txt"
        input = [
            features = [
                dim = 3072
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]    
]

configparameters: 02_BatchNormConv.cntk:timestamping=true
configparameters: 02_BatchNormConv.cntk:traceLevel=1
configparameters: 02_BatchNormConv.cntk:Train=[
    action = "train"
    modelPath = "C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu/Models/02_BatchNormConv"
     NDLNetworkBuilder = [
        networkDescription = "C:\repos\CNTK\Tests\EndToEndTests\BatchNormalization\Spatial/02_BatchNormConv.ndl"
    ]
    SGD = [
        epochSize = 1024
        minibatchSize = 64
        learningRatesPerMB = 0.03*7:0.01
        momentumPerMB = 0
        maxEpochs = 2
        L2RegWeight = 0
        dropoutRate = 0
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "C:\repos\CNTK\Examples\Image\DataSets\CIFAR-10/Train_cntk_text.txt"
        input = [
            features = [
                dim = 3072
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]    
]

08/22/2017 13:28:14: Commands: Train Test
08/22/2017 13:28:14: precision = "float"

08/22/2017 13:28:14: ##############################################################################
08/22/2017 13:28:14: #                                                                            #
08/22/2017 13:28:14: # Train command (train action)                                               #
08/22/2017 13:28:14: #                                                                            #
08/22/2017 13:28:14: ##############################################################################

08/22/2017 13:28:14: 
Creating virgin network.
NDLBuilder Using GPU 0
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetGaussianRandomValue (GPU): creating curand object with seed 2, sizeof(ElemType)==4
c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 3, Kernel: 5 x 5 x 3, Map: 1 x 1 x 3, Stride: 1 x 1 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using cuDNN batch normalization engine.
pool1: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 15 x 15 x 3, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
08/22/2017 13:28:17: 
Model has 25 nodes. Using GPU 0.

08/22/2017 13:28:17: Training criterion:   CE = CrossEntropyWithSoftmax
08/22/2017 13:28:17: Evaluation criterion: Err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 4 are aliased.
	features (gradient) reuses featScaled (gradient)
	OutputNodes.t (gradient) reuses OutputNodes.z (gradient)

Memory Sharing: Out of 42 matrices, 20 are shared as 5, and 22 are not shared.

Here are the ones that share memory:
	{ W : [3 x 75] (gradient)
	  conv1 : [32 x 32 x 3 x *] }
	{ OutputNodes.z : [10 x 1 x *]
	  c : [32 x 32 x 3 x *] (gradient)
	  conv1 : [32 x 32 x 3 x *] (gradient)
	  h1.t : [64 x *] (gradient)
	  h1.y : [64 x 1 x *] (gradient)
	  h1.z : [64 x 1 x *]
	  y : [32 x 32 x 3 x *] }
	{ h1.t : [64 x *]
	  h1.y : [64 x 1 x *]
	  pool1 : [15 x 15 x 3 x *] (gradient)
	  sc : [3 x 1] (gradient) }
	{ OutputNodes.t : [10 x 1 x *]
	  OutputNodes.t : [10 x 1 x *] (gradient)
	  OutputNodes.z : [10 x 1 x *] (gradient)
	  h1.W : [64 x 15 x 15 x 3] (gradient)
	  h1.z : [64 x 1 x *] (gradient) }
	{ pool1 : [15 x 15 x 3 x *]
	  y : [32 x 32 x 3 x *] (gradient) }

Here are the ones that don't share memory:
	{features : [32 x 32 x 3 x *]}
	{b : [3 x 1]}
	{W : [3 x 75]}
	{featOffs : [1 x 1]}
	{labels : [10 x *]}
	{sc : [3 x 1]}
	{y.run_sample_count : [1]}
	{h1.W : [64 x 15 x 15 x 3]}
	{m : [3 x 1]}
	{v : [3 x 1]}
	{h1.b : [64 x 1]}
	{OutputNodes.b : [10]}
	{OutputNodes.W : [10 x 64]}
	{CE : [1]}
	{c : [32 x 32 x 3 x *]}
	{Err : [1]}
	{featScaled : [32 x 32 x 3 x *]}
	{OutputNodes.W : [10 x 64] (gradient)}
	{OutputNodes.b : [10] (gradient)}
	{h1.b : [64 x 1] (gradient)}
	{CE : [1] (gradient)}
	{b : [3 x 1] (gradient)}


08/22/2017 13:28:17: Training 44145 parameters in 7 out of 7 parameter tensors and 17 nodes with gradient:

08/22/2017 13:28:17: 	Node 'OutputNodes.W' (LearnableParameter operation) : [10 x 64]
08/22/2017 13:28:17: 	Node 'OutputNodes.b' (LearnableParameter operation) : [10]
08/22/2017 13:28:17: 	Node 'W' (LearnableParameter operation) : [3 x 75]
08/22/2017 13:28:17: 	Node 'b' (LearnableParameter operation) : [3 x 1]
08/22/2017 13:28:17: 	Node 'h1.W' (LearnableParameter operation) : [64 x 15 x 15 x 3]
08/22/2017 13:28:17: 	Node 'h1.b' (LearnableParameter operation) : [64 x 1]
08/22/2017 13:28:17: 	Node 'sc' (LearnableParameter operation) : [3 x 1]

08/22/2017 13:28:17: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/22/2017 13:28:17: Starting Epoch 1: learning rate per sample = 0.00046875  effective momentum = 0.000000  momentum as time constant = 0.0 samples

08/22/2017 13:28:17: Starting minibatch loop.
08/22/2017 13:28:25: Finished Epoch[ 1 of 2]: [Training] CE = 2.30601311 * 1024; Err = 0.86425781 * 1024; totalSamplesSeen = 1024; learningRatePerSample = 0.00046874999; epochTime=7.12911s
08/22/2017 13:28:25: SGD: Saving checkpoint model 'C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu/Models/02_BatchNormConv.1'

08/22/2017 13:28:25: Starting Epoch 2: learning rate per sample = 0.00046875  effective momentum = 0.000000  momentum as time constant = 0.0 samples

08/22/2017 13:28:25: Starting minibatch loop.
08/22/2017 13:28:25: Finished Epoch[ 2 of 2]: [Training] CE = 2.17934513 * 1024; Err = 0.78027344 * 1024; totalSamplesSeen = 2048; learningRatePerSample = 0.00046874999; epochTime=0.07023s
08/22/2017 13:28:25: SGD: Saving checkpoint model 'C:\cygwin64\tmp\cntk-test-20170822142811.317847\BatchNormalization\Spatial_CuDNN@release_gpu/Models/02_BatchNormConv'

08/22/2017 13:28:25: Action "train" complete.


08/22/2017 13:28:25: ##############################################################################
08/22/2017 13:28:25: #                                                                            #
08/22/2017 13:28:25: # Test command (test action)                                                 #
08/22/2017 13:28:25: #                                                                            #
08/22/2017 13:28:25: ##############################################################################


Post-processing network...

3 roots:
	CE = CrossEntropyWithSoftmax()
	Err = ClassificationError()
	OutputNodes.z = Plus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [10 x *1]
Validating --> OutputNodes.W = LearnableParameter() :  -> [10 x 64]
Validating --> h1.W = LearnableParameter() :  -> [64 x 15 x 15 x 3]
Validating --> W = LearnableParameter() :  -> [3 x 75]
Validating --> features = InputValue() :  -> [32 x 32 x 3 x *1]
Validating --> featOffs = LearnableParameter() :  -> [1 x 1]
Validating --> featScaled = Minus (features, featOffs) : [32 x 32 x 3 x *1], [1 x 1] -> [32 x 32 x 3 x *1]
Validating --> c = Convolution (W, featScaled) : [3 x 75], [32 x 32 x 3 x *1] -> [32 x 32 x 3 x *1]
Validating --> sc = LearnableParameter() :  -> [3 x 1]
Validating --> b = LearnableParameter() :  -> [3 x 1]
Validating --> m = LearnableParameter() :  -> [3 x 1]
Validating --> v = LearnableParameter() :  -> [3 x 1]
Validating --> y.run_sample_count = LearnableParameter() :  -> [1]
Validating --> y = BatchNormalization (c, sc, b, m, v, y.run_sample_count) : [32 x 32 x 3 x *1], [3 x 1], [3 x 1], [3 x 1], [3 x 1], [1] -> [32 x 32 x 3 x *1]
Validating --> conv1 = RectifiedLinear (y) : [32 x 32 x 3 x *1] -> [32 x 32 x 3 x *1]
Validating --> pool1 = MaxPooling (conv1) : [32 x 32 x 3 x *1] -> [15 x 15 x 3 x *1]
Validating --> h1.t = Times (h1.W, pool1) : [64 x 15 x 15 x 3], [15 x 15 x 3 x *1] -> [64 x *1]
Validating --> h1.b = LearnableParameter() :  -> [64 x 1]
Validating --> h1.z = Plus (h1.t, h1.b) : [64 x *1], [64 x 1] -> [64 x 1 x *1]
Validating --> h1.y = RectifiedLinear (h1.z) : [64 x 1 x *1] -> [64 x 1 x *1]
Validating --> OutputNodes.t = Times (OutputNodes.W, h1.y) : [10 x 64], [64 x 1 x *1] -> [10 x 1 x *1]
Validating --> OutputNodes.b = LearnableParameter() :  -> [10]
Validating --> OutputNodes.z = Plus (OutputNodes.t, OutputNodes.b) : [10 x 1 x *1], [10] -> [10 x 1 x *1]
Validating --> CE = CrossEntropyWithSoftmax (labels, OutputNodes.z) : [10 x *1], [10 x 1 x *1] -> [1]
Validating --> Err = ClassificationError (labels, OutputNodes.z) : [10 x *1], [10 x 1 x *1] -> [1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.

c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 3, Kernel: 5 x 5 x 3, Map: 1 x 1 x 3, Stride: 1 x 1 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using cuDNN batch normalization engine.
pool1: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 15 x 15 x 3, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.



Post-processing network complete.

evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 25 matrices, 10 are shared as 2, and 15 are not shared.

Here are the ones that share memory:
	{ OutputNodes.t : [10 x 1 x *1]
	  featScaled : [32 x 32 x 3 x *1]
	  h1.z : [64 x 1 x *1]
	  pool1 : [15 x 15 x 3 x *1]
	  y : [32 x 32 x 3 x *1] }
	{ OutputNodes.z : [10 x 1 x *1]
	  c : [32 x 32 x 3 x *1]
	  conv1 : [32 x 32 x 3 x *1]
	  h1.t : [64 x *1]
	  h1.y : [64 x 1 x *1] }

Here are the ones that don't share memory:
	{h1.b : [64 x 1]}
	{h1.W : [64 x 15 x 15 x 3]}
	{labels : [10 x *1]}
	{m : [3 x 1]}
	{OutputNodes.b : [10]}
	{OutputNodes.W : [10 x 64]}
	{features : [32 x 32 x 3 x *1]}
	{featOffs : [1 x 1]}
	{b : [3 x 1]}
	{v : [3 x 1]}
	{sc : [3 x 1]}
	{y.run_sample_count : [1]}
	{Err : [1]}
	{W : [3 x 75]}
	{CE : [1]}

08/22/2017 13:28:27: Minibatch[1-500]: Err = 0.77200000 * 8000; CE = 2.14807269 * 8000
08/22/2017 13:28:28: Minibatch[501-625]: Err = 0.78000000 * 2000; CE = 2.15983341 * 2000
08/22/2017 13:28:28: Final Results: Minibatch[1-625]: Err = 0.77360000 * 10000; CE = 2.15042483 * 10000; perplexity = 8.58850628

08/22/2017 13:28:28: Action "test" complete.

08/22/2017 13:28:28: __COMPLETED__