CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz
    Hardware threads: 6
    Total Memory: 58719796 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/release/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\BatchNormalization\Spatial/02_BatchNormConv.cntk currentDirectory=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu\TestData RunDir=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu DataDir=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\BatchNormalization\Spatial OutputDir=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu DeviceId=0 timestamping=true batchNormalizationEngine=cntk
CNTK 2.2+ (HEAD c097a3, Nov  2 2017 08:29:14) at 2017/11/02 11:15:57

C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\BatchNormalization\Spatial/02_BatchNormConv.cntk  currentDirectory=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu\TestData  RunDir=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu  DataDir=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\BatchNormalization\Spatial  OutputDir=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu  DeviceId=0  timestamping=true  batchNormalizationEngine=cntk
Changed current directory to D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu\TestData
-------------------------------------------------------------------
Build info: 

		Built time: Nov  2 2017 08:18:07
		Last modified date: Mon Oct 16 19:27:07 2017
		Build type: Release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA version: 9.0.0
		CUDNN version: 6.0.21
		Build Branch: HEAD
		Build SHA1: c097a3e4f61165870365768f5c1e2a35ff119087
		MPI distribution: Microsoft MPI
		MPI version: 7.0.12437.6
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8124 MB; free memory = 8001 MB
-------------------------------------------------------------------

Configuration After Processing and Variable Resolution:

configparameters: 02_BatchNormConv.cntk:batchNormalizationEngine=cntk
configparameters: 02_BatchNormConv.cntk:command=Train:Test
configparameters: 02_BatchNormConv.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\BatchNormalization\Spatial
configparameters: 02_BatchNormConv.cntk:currentDirectory=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu\TestData
configparameters: 02_BatchNormConv.cntk:DataDir=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu\TestData
configparameters: 02_BatchNormConv.cntk:deviceId=0
configparameters: 02_BatchNormConv.cntk:imageLayout=cudnn
configparameters: 02_BatchNormConv.cntk:initOnCPUOnly=true
configparameters: 02_BatchNormConv.cntk:ModelDir=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu/Models
configparameters: 02_BatchNormConv.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\BatchNormalization\Spatial/Macros.ndl
configparameters: 02_BatchNormConv.cntk:numMBsToShowResult=500
configparameters: 02_BatchNormConv.cntk:OutputDir=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu
configparameters: 02_BatchNormConv.cntk:precision=float
configparameters: 02_BatchNormConv.cntk:RootDir=.
configparameters: 02_BatchNormConv.cntk:RunDir=D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu
configparameters: 02_BatchNormConv.cntk:Test=[
    action = "test"
    modelPath = "D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu/Models/02_BatchNormConv"
    minibatchSize = 16
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu\TestData/Test_cntk_text.txt"
        input = [
            features = [
                dim = 3072
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]    
]

configparameters: 02_BatchNormConv.cntk:timestamping=true
configparameters: 02_BatchNormConv.cntk:traceLevel=1
configparameters: 02_BatchNormConv.cntk:Train=[
    action = "train"
    modelPath = "D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu/Models/02_BatchNormConv"
     NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\BatchNormalization\Spatial/02_BatchNormConv.ndl"
    ]
    SGD = [
        epochSize = 1024
        minibatchSize = 64
        learningRatesPerMB = 0.03*7:0.01
        momentumPerMB = 0
        maxEpochs = 2
        L2RegWeight = 0
        dropoutRate = 0
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu\TestData/Train_cntk_text.txt"
        input = [
            features = [
                dim = 3072
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]    
]

11/02/2017 11:15:57: Commands: Train Test
11/02/2017 11:15:57: precision = "float"

11/02/2017 11:15:57: ##############################################################################
11/02/2017 11:15:57: #                                                                            #
11/02/2017 11:15:57: # Train command (train action)                                               #
11/02/2017 11:15:57: #                                                                            #
11/02/2017 11:15:57: ##############################################################################

11/02/2017 11:15:57: 
Creating virgin network.
NDLBuilder Using GPU 0
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetGaussianRandomValue (GPU): creating curand object with seed 2, sizeof(ElemType)==4
c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 3, Kernel: 5 x 5 x 3, Map: 1 x 1 x 3, Stride: 1 x 1 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool1: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 15 x 15 x 3, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
11/02/2017 11:15:59: 
Model has 25 nodes. Using GPU 0.

11/02/2017 11:15:59: Training criterion:   CE = CrossEntropyWithSoftmax
11/02/2017 11:15:59: Evaluation criterion: Err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 4 are aliased.
	features (gradient) reuses featScaled (gradient)
	OutputNodes.t (gradient) reuses OutputNodes.z (gradient)

Memory Sharing: Out of 42 matrices, 20 are shared as 5, and 22 are not shared.

Here are the ones that share memory:
	{ h1.t : [64 x *]
	  h1.y : [64 x 1 x *]
	  pool1 : [15 x 15 x 3 x *] (gradient)
	  sc : [3 x 1] (gradient) }
	{ W : [3 x 75] (gradient)
	  conv1 : [32 x 32 x 3 x *] }
	{ OutputNodes.z : [10 x 1 x *]
	  c : [32 x 32 x 3 x *] (gradient)
	  conv1 : [32 x 32 x 3 x *] (gradient)
	  h1.t : [64 x *] (gradient)
	  h1.y : [64 x 1 x *] (gradient)
	  h1.z : [64 x 1 x *]
	  y : [32 x 32 x 3 x *] }
	{ pool1 : [15 x 15 x 3 x *]
	  y : [32 x 32 x 3 x *] (gradient) }
	{ OutputNodes.t : [10 x 1 x *]
	  OutputNodes.t : [10 x 1 x *] (gradient)
	  OutputNodes.z : [10 x 1 x *] (gradient)
	  h1.W : [64 x 15 x 15 x 3] (gradient)
	  h1.z : [64 x 1 x *] (gradient) }

Here are the ones that don't share memory:
	{features : [32 x 32 x 3 x *]}
	{b : [3 x 1]}
	{sc : [3 x 1]}
	{labels : [10 x *]}
	{W : [3 x 75]}
	{m : [3 x 1]}
	{featOffs : [1 x 1]}
	{OutputNodes.b : [10]}
	{h1.b : [64 x 1]}
	{OutputNodes.W : [10 x 64]}
	{h1.W : [64 x 15 x 15 x 3]}
	{y.run_sample_count : [1]}
	{v : [3 x 1]}
	{CE : [1]}
	{Err : [1]}
	{c : [32 x 32 x 3 x *]}
	{b : [3 x 1] (gradient)}
	{OutputNodes.b : [10] (gradient)}
	{OutputNodes.W : [10 x 64] (gradient)}
	{h1.b : [64 x 1] (gradient)}
	{CE : [1] (gradient)}
	{featScaled : [32 x 32 x 3 x *]}


11/02/2017 11:15:59: Training 44145 parameters in 7 out of 7 parameter tensors and 17 nodes with gradient:

11/02/2017 11:15:59: 	Node 'OutputNodes.W' (LearnableParameter operation) : [10 x 64]
11/02/2017 11:15:59: 	Node 'OutputNodes.b' (LearnableParameter operation) : [10]
11/02/2017 11:15:59: 	Node 'W' (LearnableParameter operation) : [3 x 75]
11/02/2017 11:15:59: 	Node 'b' (LearnableParameter operation) : [3 x 1]
11/02/2017 11:15:59: 	Node 'h1.W' (LearnableParameter operation) : [64 x 15 x 15 x 3]
11/02/2017 11:15:59: 	Node 'h1.b' (LearnableParameter operation) : [64 x 1]
11/02/2017 11:15:59: 	Node 'sc' (LearnableParameter operation) : [3 x 1]

11/02/2017 11:15:59: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

11/02/2017 11:15:59: Starting Epoch 1: learning rate per sample = 0.000469  effective momentum = 0.000000  momentum as time constant = 0.0 samples

11/02/2017 11:15:59: Starting minibatch loop.
11/02/2017 11:16:03: Finished Epoch[ 1 of 2]: [Training] CE = 2.27208686 * 1024; Err = 0.85546875 * 1024; totalSamplesSeen = 1024; learningRatePerSample = 0.00046874999; epochTime=4.61596s
11/02/2017 11:16:03: SGD: Saving checkpoint model 'D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu/Models/02_BatchNormConv.1'

11/02/2017 11:16:03: Starting Epoch 2: learning rate per sample = 0.000469  effective momentum = 0.000000  momentum as time constant = 0.0 samples

11/02/2017 11:16:03: Starting minibatch loop.
11/02/2017 11:16:03: Finished Epoch[ 2 of 2]: [Training] CE = 2.21359491 * 1024; Err = 0.80761719 * 1024; totalSamplesSeen = 2048; learningRatePerSample = 0.00046874999; epochTime=0.0264472s
11/02/2017 11:16:03: SGD: Saving checkpoint model 'D:\cntk-test-20171102111535.446130\BatchNormalization\Spatial_CNTK@release_gpu/Models/02_BatchNormConv'

11/02/2017 11:16:03: Action "train" complete.


11/02/2017 11:16:03: ##############################################################################
11/02/2017 11:16:03: #                                                                            #
11/02/2017 11:16:03: # Test command (test action)                                                 #
11/02/2017 11:16:03: #                                                                            #
11/02/2017 11:16:03: ##############################################################################


Post-processing network...

3 roots:
	CE = CrossEntropyWithSoftmax()
	Err = ClassificationError()
	OutputNodes.z = Plus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [10 x *1]
Validating --> OutputNodes.W = LearnableParameter() :  -> [10 x 64]
Validating --> h1.W = LearnableParameter() :  -> [64 x 15 x 15 x 3]
Validating --> W = LearnableParameter() :  -> [3 x 75]
Validating --> features = InputValue() :  -> [32 x 32 x 3 x *1]
Validating --> featOffs = LearnableParameter() :  -> [1 x 1]
Validating --> featScaled = Minus (features, featOffs) : [32 x 32 x 3 x *1], [1 x 1] -> [32 x 32 x 3 x *1]
Validating --> c = Convolution (W, featScaled) : [3 x 75], [32 x 32 x 3 x *1] -> [32 x 32 x 3 x *1]
Validating --> sc = LearnableParameter() :  -> [3 x 1]
Validating --> b = LearnableParameter() :  -> [3 x 1]
Validating --> m = LearnableParameter() :  -> [3 x 1]
Validating --> v = LearnableParameter() :  -> [3 x 1]
Validating --> y.run_sample_count = LearnableParameter() :  -> [1]
Validating --> y = BatchNormalization (c, sc, b, m, v, y.run_sample_count) : [32 x 32 x 3 x *1], [3 x 1], [3 x 1], [3 x 1], [3 x 1], [1] -> [32 x 32 x 3 x *1]
Validating --> conv1 = RectifiedLinear (y) : [32 x 32 x 3 x *1] -> [32 x 32 x 3 x *1]
Validating --> pool1 = MaxPooling (conv1) : [32 x 32 x 3 x *1] -> [15 x 15 x 3 x *1]
Validating --> h1.t = Times (h1.W, pool1) : [64 x 15 x 15 x 3], [15 x 15 x 3 x *1] -> [64 x *1]
Validating --> h1.b = LearnableParameter() :  -> [64 x 1]
Validating --> h1.z = Plus (h1.t, h1.b) : [64 x *1], [64 x 1] -> [64 x 1 x *1]
Validating --> h1.y = RectifiedLinear (h1.z) : [64 x 1 x *1] -> [64 x 1 x *1]
Validating --> OutputNodes.t = Times (OutputNodes.W, h1.y) : [10 x 64], [64 x 1 x *1] -> [10 x 1 x *1]
Validating --> OutputNodes.b = LearnableParameter() :  -> [10]
Validating --> OutputNodes.z = Plus (OutputNodes.t, OutputNodes.b) : [10 x 1 x *1], [10] -> [10 x 1 x *1]
Validating --> CE = CrossEntropyWithSoftmax (labels, OutputNodes.z) : [10 x *1], [10 x 1 x *1] -> [1]
Validating --> Err = ClassificationError (labels, OutputNodes.z) : [10 x *1], [10 x 1 x *1] -> [1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.

c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 3, Kernel: 5 x 5 x 3, Map: 1 x 1 x 3, Stride: 1 x 1 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool1: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 15 x 15 x 3, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.



Post-processing network complete.

evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 25 matrices, 10 are shared as 2, and 15 are not shared.

Here are the ones that share memory:
	{ OutputNodes.t : [10 x 1 x *1]
	  featScaled : [32 x 32 x 3 x *1]
	  h1.z : [64 x 1 x *1]
	  pool1 : [15 x 15 x 3 x *1]
	  y : [32 x 32 x 3 x *1] }
	{ OutputNodes.z : [10 x 1 x *1]
	  c : [32 x 32 x 3 x *1]
	  conv1 : [32 x 32 x 3 x *1]
	  h1.t : [64 x *1]
	  h1.y : [64 x 1 x *1] }

Here are the ones that don't share memory:
	{b : [3 x 1]}
	{h1.b : [64 x 1]}
	{featOffs : [1 x 1]}
	{features : [32 x 32 x 3 x *1]}
	{W : [3 x 75]}
	{h1.W : [64 x 15 x 15 x 3]}
	{labels : [10 x *1]}
	{m : [3 x 1]}
	{OutputNodes.b : [10]}
	{v : [3 x 1]}
	{OutputNodes.W : [10 x 64]}
	{sc : [3 x 1]}
	{y.run_sample_count : [1]}
	{Err : [1]}
	{CE : [1]}

11/02/2017 11:16:04: Minibatch[1-500]: Err = 0.81237500 * 8000; CE = 2.18291836 * 8000
11/02/2017 11:16:05: Minibatch[501-625]: Err = 0.81300000 * 2000; CE = 2.18584150 * 2000
11/02/2017 11:16:05: Final Results: Minibatch[1-625]: Err = 0.81250000 * 10000; CE = 2.18350299 * 10000; perplexity = 8.87734910

11/02/2017 11:16:05: Action "test" complete.

11/02/2017 11:16:05: __COMPLETED__
