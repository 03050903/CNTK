{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual: How to set up learning hyper-parameters\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This manual explains how to set learning rate, momentum and other hyper-parameters for the learners (opitimizers) supported in CNTK:\n",
    "\n",
    "* [AdaDelta](https://cntk.ai/pythondocs/cntk.learners.html#cntk.learners.adadelta)\n",
    "* [AdaGrad](https://cntk.ai/pythondocs/cntk.learners.html#cntk.learners.adagrad)\n",
    "* [FSAdaGrad](https://cntk.ai/pythondocs/cntk.learners.html#cntk.learners.fsadagrad)\n",
    "* [Adam](https://cntk.ai/pythondocs/cntk.learners.html#cntk.learners.adam)\n",
    "* [MomentumSGD](https://cntk.ai/pythondocs/cntk.learners.html#cntk.learners.momentum_sgd)\n",
    "* [Nesterov](https://cntk.ai/pythondocs/cntk.learners.html#cntk.learners.nesterov)\n",
    "* [RMSProp](https://cntk.ai/pythondocs/cntk.learners.html#cntk.learners.rmsprop)\n",
    "* [SGD](https://cntk.ai/pythondocs/cntk.learners.html#cntk.learners.sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional details regarding the learners and how to use them in training can found at:\n",
    "\n",
    "* For the details of learning rate schedule, please see [learning_rate_schedule](https://cntk.ai/pythondocs/cntk.learners.html?highlight=learning_rate_schedule#cntk.learners.learning_rate_schedule); for the details of momentum schedule, please see [momentum_schedule](https://cntk.ai/pythondocs/cntk.learners.html?highlight=learning_rate_schedule#cntk.learners.momentum_schedule);\n",
    "\n",
    "* For how to make user the of learners to train model, please refer to [Manual on how to train using declarative and imperative API](https://github.com/Microsoft/CNTK/blob/master/Manual/Manual_How_to_train_using_declarative_and_imperative_API.ipynb).\n",
    "\n",
    "Prepare a simple 1-layer neural network classifiation model for demonstration purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "x = C.input_variable(shape=(4,4))\n",
    "label = C.input_variable(2)\n",
    "model = C.layers.Dense(2, activation=C.sigmoid)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple hyper-parameter set-up\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing learning rates, momentm and other hyper-parameters directly as arguments to the learners\n",
    "\n",
    "* The simplest way to set up a learner with specified learning rate is as follows: \n",
    "\n",
    "```python\n",
    "    C.cntk_learner(parameters=model.parametes, lr=<int or float>, minibatch_size=<None or int>, ...other parameters)\n",
    "```\n",
    "\n",
    "* The simplest way to set up a learner with specified learning rate and momentum is as follows: \n",
    "\n",
    "```python\n",
    "    C.cntk_learner(parameters=model.parametes, lr=<int or float>, momentum=<int or float>, minibatch_size=<None or int>, ...other parameters)\n",
    "```\n",
    "\n",
    "* The simplest way to set up a learner with specified learning rate, momentum and var_momentum (i.e. squared gradient momentum) in Adam and CNTK specific FSAdaGrad is as follows: \n",
    "\n",
    "```python\n",
    "    C.cntk_learner(parameters=model.parametes, lr=<int or float>, momentum=<int or float>, variance_momentum=<int or float>, minibatch_size=<None or int>, ...other parameters)\n",
    "```\n",
    "\n",
    "In the above, CNTK learner requires a *minibatch_size* parameter. This is the case due to CNTK's capability of automatically  adapting a learning hyper-parameter tuned for a specific minibatch size to variable minibatch sizes. To deal with variable length sequences in texts and other other application, CNTK allows variable minibatche sizes. Variable minibatch size also simplifies the design and implementation for efficient distributed training and data randomization. However, it is well-known that for different minibatch sizes, the learning rates and other hyper parameters need to be adjusted accordingly. This is why CNTK learners need such a design-time minibatch size to understand how to scale the hyper-parameters automatically.\n",
    "\n",
    "Please see below a list of concrete examples of all supported learners using this simple set-up approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mysgd = C.sgd(parameters=model.parameters, lr=0.4, minibatch_size=32)\n",
    "\n",
    "mymomentum = C.momentum_sgd(parameters=model.parameters, lr=0.4, momentum=0.9, minibatch_size=32)\n",
    "\n",
    "myadadelta = C.adadelta(parameters=model.parameters, lr=0.4, minibatch_size=32)\n",
    "\n",
    "myadam = C.adam(parameters=model.parameters, lr=0.4, momentum=0.9, variance_momentum=0.9, minibatch_size=32)\n",
    "\n",
    "myadagrad = C.adagrad(parameters=model.parameters, lr=0.4, minibatch_size=32)\n",
    "\n",
    "myfsadagrad = C.fsadagrad(parameters=model.parameters, lr=0.4, momentum=0.9, variance_momentum=0.9, minibatch_size=32)\n",
    "\n",
    "mynesterov = C.nesterov(parameters=model.parameters, lr=0.4, momentum=0.9, minibatch_size=32)\n",
    "\n",
    "myrmsrop = C.rmsprop(parameters=model.parameters, lr=0.4, gamma=0.5, inc=1.2, dec=0.7, max=10, min=1e-8, minibatch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing a schedule of learning rates, momentm and other hyper-parameters directly as arguments to the learners\n",
    "\n",
    "* The simplest way to set up a learner with specified learning rate is as follows: \n",
    "\n",
    "```python\n",
    "    C.cntk_learner(parameters=model.parametes, lr=<list of int or float>, minibatch_size=<None or int>, epoch_size=<None or int>, ...other parameters)\n",
    "```\n",
    "\n",
    "* The simplest way to set up a learner with specified learning rate and momentum is as follows: \n",
    "\n",
    "```python\n",
    "    C.cntk_learner(parameters=model.parametes, lr=<list of int or float>, momentum=<int or float>, minibatch_size=<None or int>, epoch_size=<None or int>,  ...other parameters)\n",
    "```\n",
    "\n",
    "* The simplest way to set up a learner with specified learning rate, momentum and var_momentum (i.e. squared gradient momentum) in Adam and CNTK specific FSAdaGrad is as follows: \n",
    "\n",
    "```python\n",
    "    C.cntk_learner(parameters=model.parametes, lr=<list int or float>, momentum=<int or float>, variance_momentum=<int or float>, minibatch_size=<None or int>, epoch_size=<None or int>, ...other parameters)\n",
    "```\n",
    "\n",
    "For how a list of hyper-parameter (e.g. learning rates) and the epoch size spepcification create a hyper-parameter schedule, please see [learning_rate_schedule](https://cntk.ai/pythondocs/cntk.learners.html?highlight=learning_rate_schedule#cntk.learners.learning_rate_schedule) for details. \n",
    "\n",
    "A list of concrete examples of all supported learners using this simple set-up approach are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysgd = C.sgd(parameters=model.parameters, lr=[0.4, 0.1, 0.001], minibatch_size=32, epoch_size=512)\n",
    "\n",
    "mymomentum = C.momentum_sgd(parameters=model.parameters, lr=[0.4, 0.1, 0.001], momentum=[0.9], \n",
    "                            minibatch_size=32, epoch_size=512)\n",
    "\n",
    "myadadelta = C.adadelta(parameters=model.parameters, lr=[0.4, 0.1, 0.001], \n",
    "                        minibatch_size=32, epoch_size=512)\n",
    "\n",
    "myadam = C.adam(parameters=model.parameters, lr=[0.4], momentum=[0.9, 0.1, 0.001], variance_momentum=[0.9], \n",
    "                minibatch_size=32, epoch_size=512)\n",
    "\n",
    "myadagrad = C.adagrad(parameters=model.parameters, lr=[0.4, 0.1, 0.001], minibatch_size=32, epoch_size=512)\n",
    "\n",
    "myfsadagrad = C.fsadagrad(parameters=model.parameters, lr=[0.4, 0.1, 0.001], momentum=[0.9], variance_momentum=[0.9], \n",
    "                          minibatch_size=32, epoch_size=512)\n",
    "\n",
    "mynesterov = C.nesterov(parameters=model.parameters, lr=[0.4, 0.1, 0.001], momentum=[0.9], \n",
    "                        minibatch_size=32, epoch_size=512)\n",
    "\n",
    "myrmsrop = C.rmsprop(parameters=model.parameters, lr=[0.4, 0.1, 0.001], gamma=0.5, inc=1.2, dec=0.7, max=10, min=1e-8,\n",
    "                     minibatch_size=32, epoch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the complicated scenario combinations --- that different hyper-parameters are designed for different minibatch sizes and having different schedules based on different epoch sizes\n",
    "\n",
    "Use C.adam as an example, we can set up the learner with hyper-parameters each of which are designed for difference minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = C.learning_parameter_schedule([0.4, 0.1, 0.001], minibatch_size = 8, epoch_size = 128)\n",
    "momentum = C.learning_parameter_schedule([0.92, 0.91, 0.9], minibatch_size = 32, epoch_size = 64)\n",
    "var_momentum = C.learning_parameter_schedule([0.99, 0.96, 9.96], minibatch_size = 64, epoch_size = 256)\n",
    "myadam = C.adam(parameters=model.parameters, lr=lr, momentum=momentum, variance_momentum=var_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying hyper-parametes as they are over all minibatches (possibly with different minibatch sizes)\n",
    "\n",
    "We can set up the hyper-parameters to be applied to any minibatch as they are without any scaling. Please note that this is assuming that all minibatch sizes are roughly of the same size. This can be done by simply set minibatch_size to be 0 as follows taking Adam as an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myadam = C.adam(parameters=model.parameters, lr=[0.4], momentum=[0.9, 0.1, 0.001], variance_momentum=[0.9], \n",
    "                minibatch_size=0, epoch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete story\n",
    "\n",
    "Finally let us wrap up this manual with a very common use case which sets the same minibatch size for both minibatch source and the learner: \n",
    "```python\n",
    "import cntk as C\n",
    "x = C.input_variable(shape=(4,4))\n",
    "label = C.input_variable(2)\n",
    "model = C.layers.Dense(2, activation=C.sigmoid)(x)\n",
    "loss = C.squared_error(model, label)\n",
    "err = C.classification_error(model, label)\n",
    "\n",
    "minibatch_size = 32\n",
    "mb_source = C.MinibatchSource(C.CTFDeserializer(input_file, streams))\n",
    "\n",
    "myadam = C.adam(parameters=model.parameters, lr=[0.4], momentum=[0.9, 0.1, 0.001], variance_momentum=[0.9], \n",
    "                minibatch_size=minibatch_size, epoch_size=512)\n",
    "trainer = C.train.Trainer(model, (loss, err), learner, ProgressPrinter(freq=10))\n",
    "                \n",
    "while train and trainer.total_number_of_samples_seen < max_samples:\n",
    "    data = mb_source.next_minibatch(minibatch_size, input_map)\n",
    "    train = trainer.train_minibatch(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "cntkdev-py35",
   "language": "python",
   "name": "cntkdev-py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
