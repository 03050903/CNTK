{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled Softmax\n",
    "\n",
    "For classification and prediction problems a typical criterion function is cross-entropy with softmax. If the number of output classes is high the computation of this criterion and the corresponding gradients could be quite costly. Sampled Softmax is a heuristic to speed up training in these cases.\n",
    "\n",
    "## Basics\n",
    "\n",
    "The softmax function is used in neural networks if we want to interpret the network output as a probability distribution over a set of classes $C$ with $|C|=N_C$.\n",
    "\n",
    "Softmax maps an $N_C$-dimensional vector $z$, which has unrestricted values, to an $N_C$ dimensional vector $p$ whith non-negative values that sum up to 1 so that they can be interpreted as probabilities. More precisely:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_i &= softmax(z, i)\\\\\n",
    "    &= \\frac{exp(z_i)}{\\sum_{k\\in C} exp(z_k)}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In what follows we assume that the input $z$ to the softmax is computed from some hidden vector $h$ of dimension $N_h$  in a specific way, namely:\n",
    "\n",
    "$$ z = W h + b $$\n",
    "\n",
    "where $W$ is a learnable weight matrix of dimension $(N_c, N_h)$ and is $b$ a learnable bias vector.\n",
    "We restrict ourself to this specific choice of $z$ because it helps in implementing an efficient sampled softmax.\n",
    "\n",
    "In a typical use-case like for example a recurrent language model, the hidden vector $h$ would be the output of the recurrent layers and $C$ would be the set of words to predict.   \n",
    "\n",
    "As a training criterion, we use cross-entropy which is a function of the expected (true) class $t\\in C$ and the probability predicted for it:\n",
    "\n",
    "$$cross\\_entropy := -log(p_t)$$\n",
    "\n",
    "## Sampled Softmax from the outside\n",
    "\n",
    "For the normal softmax the CNTK Python-api provides the function [cross_entropy_with_softmax](https://cntk.ai/pythondocs/cntk.ops.html?highlight=softmax#cntk.ops.cross_entropy_with_softmax). This takes as input the vector $N_C$-dimensional vector $z$. As mentioned for our sampled softmax implementation we assume that this z is computed by $ z = W h + b $. In sampled softmax this has to be part of the whole implementation of the criterion.\n",
    "\n",
    "Below we show the code for `cross_entropy_with_sampled_softmax_and_embedding`. Letâ€™s look at signature first.\n",
    "\n",
    "One fundamental difference to the corresponding function in the Python-api (`cross_entropy_with_softmax`) is that in the Python api function the input corresponds to $z$ and must have the same dimension as the target vector, while in cross_entropy_with_full_softmax the input corresponds to our hidden vector $h$ can have any dimension (hidden_dim).\n",
    "Actually, hidden_dim will be typically much lower than the dimension of the target vector.\n",
    "\n",
    "We also have some additional parameter `num_samples, sampling_weights, allow_duplicates` that control the random sampling. \n",
    "Another difference to the api function is that we return a tripple (z, cross_entropy_on_samples, error_on_samples).\n",
    "\n",
    "We will come back to the details of the implementation below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cntk as C\n",
    "\n",
    "# Creates a subgraph computing cross-entropy with sampled softmax.\n",
    "def cross_entropy_with_sampled_softmax_and_embedding(\n",
    "    hidden_vector,            # Node providing hidden input\n",
    "    target_vector,            # Node providing the expected labels (as sparse vectors)\n",
    "    num_classes,              # Number of classes\n",
    "    hidden_dim,               # Dimension of the hidden vector\n",
    "    num_samples,              # Number of samples to use for sampled softmax\n",
    "    sampling_weights,         # Node providing weights to be used for the weighted sampling\n",
    "    allow_duplicates = False, # Boolean flag to control whether to use sampling with replacemement \n",
    "                              # (allow_duplicates == True) or without replacement.\n",
    "    ):\n",
    "    # define the parameters leanabe parameters\n",
    "    b = C.Parameter(shape = (num_classes, 1), init = C.init_bias_default_or_0)\n",
    "    W = C.Parameter(shape = (num_classes, hidden_dim), init = C.init_default_or_glorot_uniform)\n",
    "\n",
    "    # Define the node that generates a set of random samples per minibatch\n",
    "    # Sparse matrix (num_samples * vocab_dim)\n",
    "    sample_selector = C.random_sample(sampling_weights, num_samples, allow_duplicates)\n",
    "\n",
    "    # For each of the samples we also need the probablity that it in the sampled set.\n",
    "    inclusion_probs = C.random_sample_inclusion_frequency(sampling_weights, num_samples, allow_duplicates) # dense row [1 * vocab_size]\n",
    "    log_prior = C.log(inclusion_probs) # dense row [1 * vocab_dim]\n",
    "\n",
    "    # Create a submatrix wS of 'weights\n",
    "    W_sampled = C.times(sample_selector, W) # [num_samples * hidden_dim]\n",
    "    z_sampled = C.times_transpose(W_sampled, hidden_vector) + C.times(sample_selector, b) - C.times_transpose (sample_selector, log_prior)# [num_samples]\n",
    "\n",
    "    # Getting the weight vector for the true label. Dimension hidden_dim\n",
    "    W_target = C.times(target_vector, W) # [1 * hidden_dim]\n",
    "    z_target = C.times_transpose(W_target, hidden_vector) + C.times(target_vector, b) - C.times_transpose(target_vector, log_prior) # [1]\n",
    "\n",
    "\n",
    "    z_reduced = C.reduce_log_sum(z_sampled)\n",
    "    \n",
    "    # Compute the cross entropy that is used for training.\n",
    "    # We don't check whether any of the classes in the random samples conincides with the true label, so it might\n",
    "    # happen that the true class is counted\n",
    "    # twice in the normalising demnominator of sampled softmax.\n",
    "    cross_entropy_on_samples = C.log_add_exp(z_target, z_reduced) - z_target\n",
    "\n",
    "    # For applying the model we also output a node providing the input for the full softmax\n",
    "    z = C.times_transpose(W, hidden_vector) + b\n",
    "    z = C.reshape(z, shape = (num_classes))\n",
    "\n",
    "    zSMax = C.reduce_max(z_sampled)\n",
    "    error_on_samples = C.less(z_target, zSMax)\n",
    "    return (z, cross_entropy_on_samples, error_on_samples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a better idea of what the inputs and outputs are and how this all differs from the normal softmax we give below a corresponding function using normal softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates subgraph computing cross-entropy with (full) softmax.\n",
    "def cross_entropy_with_softmax_and_embedding(\n",
    "    hidden_vector,  # Node providing hidden input\n",
    "    target_vector,  # Node providing the expected labels (as sparse vectors)\n",
    "    num_classes,    # Number of classes\n",
    "    hidden_dim      # Dimension of the hidden vector\n",
    "    ):\n",
    "    # Setup bias and weights\n",
    "    b = C.Parameter(shape = (num_classes, 1), init = C.init_bias_default_or_0)\n",
    "    W = C.Parameter(shape = (num_classes, hidden_dim), init = C.init_default_or_glorot_uniform)\n",
    "\n",
    "    \n",
    "    z = C.reshape( C.times_transpose(W, hidden_vector) + b, (1,vocab_dim))\n",
    "    \n",
    "    # Use cross_entropy_with_softmax\n",
    "    cross_entropy = C.cross_entropy_with_softmax(z, target_vector)\n",
    "\n",
    "    zMax = C.reduce_max(z)\n",
    "    zT = C.times_transpose(z, target_vector)\n",
    "    error_on_samples = C.less(zT, zMax)\n",
    "\n",
    "    return (z, cross_entropy, error_on_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the main differences to the api function `cross_entropy_with_softmax` are:\n",
    "* We include an embedding.\n",
    "* We return a tripple (z, cross_entropy, error_on_samples) instead of just returnting the cross entropy.\n",
    "\n",
    "\n",
    "## A toy example\n",
    "\n",
    "To explain how to integrate sampled softmax let us look at a toy example. In this toy example we first transform one-hot input vectors via some random projection into a lower dimensional vector $h$. The modeling task is to reverse this mapping using (sampled) softmax. Well, as already said this is a toy example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "\n",
      "Minbatch=5 Cross-entropy from full softmax = 3.272 perplexity = 26.370 samples/s = 4069.7\n",
      " Minibatch[   1-  10]: loss = 3.052402 * 1000, metric = 81.0% * 1000\n",
      "\n",
      "Minbatch=15 Cross-entropy from full softmax = 1.731 perplexity = 5.649 samples/s = 4641.2\n",
      " Minibatch[  11-  20]: loss = 1.892709 * 1000, metric = 47.4% * 1000\n",
      "\n",
      "Minbatch=25 Cross-entropy from full softmax = 1.302 perplexity = 3.677 samples/s = 3576.3\n",
      " Minibatch[  21-  30]: loss = 1.414214 * 1000, metric = 28.6% * 1000\n",
      "\n",
      "Minbatch=35 Cross-entropy from full softmax = 1.010 perplexity = 2.746 samples/s = 4051.3\n",
      " Minibatch[  31-  40]: loss = 1.070170 * 1000, metric = 17.0% * 1000\n",
      "\n",
      "Minbatch=45 Cross-entropy from full softmax = 0.843 perplexity = 2.322 samples/s = 4630.1\n",
      " Minibatch[  41-  50]: loss = 0.907601 * 1000, metric = 14.8% * 1000\n",
      "\n",
      "Minbatch=55 Cross-entropy from full softmax = 0.582 perplexity = 1.790 samples/s = 4642.3\n",
      " Minibatch[  51-  60]: loss = 0.764744 * 1000, metric = 13.3% * 1000\n",
      "\n",
      "Minbatch=65 Cross-entropy from full softmax = 0.515 perplexity = 1.673 samples/s = 4183.5\n",
      " Minibatch[  61-  70]: loss = 0.705341 * 1000, metric = 12.7% * 1000\n",
      "\n",
      "Minbatch=75 Cross-entropy from full softmax = 0.390 perplexity = 1.478 samples/s = 4638.4\n",
      " Minibatch[  71-  80]: loss = 0.715320 * 1000, metric = 11.0% * 1000\n",
      "\n",
      "Minbatch=85 Cross-entropy from full softmax = 0.494 perplexity = 1.639 samples/s = 4659.7\n",
      " Minibatch[  81-  90]: loss = 0.655911 * 1000, metric = 13.0% * 1000\n",
      "\n",
      "Minbatch=95 Cross-entropy from full softmax = 0.284 perplexity = 1.328 samples/s = 4612.0\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "from math import log, exp, sqrt\n",
    "from cntk.utils import ProgressPrinter\n",
    "import timeit\n",
    "\n",
    "def zipf(index):\n",
    "    return 1.0 / (index + 5)\n",
    "\n",
    "\n",
    "# A class with all the parameters with use\n",
    "class Param:\n",
    "    # learning parameters\n",
    "    learning_rate = 0.1\n",
    "    minibatch_size = 100\n",
    "    num_minbatches = 100\n",
    "    test_set_size = 100\n",
    "    momentum_time_constant = 5 * minibatch_size\n",
    "    reporting_interval = 10\n",
    "    \n",
    "    # Parameters for sampled softmax\n",
    "    use_sampled_softmax = True\n",
    "    use_sparse = use_sampled_softmax\n",
    "    softmax_sample_size = 30\n",
    "\n",
    "    # Details of data and model\n",
    "    num_classes = 50\n",
    "    hidden_dim = 10\n",
    "\n",
    "    zipf_sampling_weights = np.asarray([ zipf(i) for i in range(num_classes)], dtype=np.float32)\n",
    "    data_sampling_distribution = zipf_sampling_weights/np.sum(zipf_sampling_weights)\n",
    "    softmax_sampling_weights =  np.power(data_sampling_distribution, 0.5)\n",
    "\n",
    "# Creates random one-hot vectors of dimension 'num_classes'.\n",
    "# Returns a tuple with a list of one-hot vectors, and list with the indices they encode.\n",
    "def get_random_one_hot_data(num_vectors):\n",
    "    indices = np.random.choice(\n",
    "        range(Param.num_classes),\n",
    "        size=num_vectors, \n",
    "        p=Param.data_sampling_distribution).reshape((1, num_vectors))\n",
    "    list_of_vectors = C.one_hot(indices, Param.num_classes)\n",
    "    return (list_of_vectors, indices.flatten())\n",
    "\n",
    "# Create a network that:\n",
    "# * Transforms the input one hot-vectors with a constant random embedding\n",
    "# * Applies a linear decoding with parameters we want to learn\n",
    "def create_model(labels):\n",
    "    # random projection matrix\n",
    "    random_data = np.random.normal(scale = sqrt(1.0/Param.hidden_dim), size=(Param.num_classes, Param.hidden_dim)).astype(np.float32)\n",
    "    random_matrix = C.constant(shape = (Param.num_classes, Param.hidden_dim), value = random_data)\n",
    "    \n",
    "    h = C.times(labels, random_matrix)\n",
    "    \n",
    "    # Connect the latent output to (sampled/full) softmax.\n",
    "    if Param.use_sampled_softmax:\n",
    "        sampling_weights = np.asarray(Param.softmax_sampling_weights, dtype=np.float32)\n",
    "        softmax_input, ce, errs = cross_entropy_with_sampled_softmax_and_embedding(\n",
    "            h, \n",
    "            labels,\n",
    "            Param.num_classes, \n",
    "            Param.hidden_dim, \n",
    "            Param.softmax_sample_size, \n",
    "            Param.softmax_sampling_weights)\n",
    "    else:\n",
    "        softmax_input, ce, errs = cross_entropy_with_softmax_and_embedding(\n",
    "            h, \n",
    "            labels, \n",
    "            Param.num_classes, \n",
    "            Param.hidden_dim)\n",
    "\n",
    "    return softmax_input, ce, errs\n",
    "\n",
    "def train(do_print_progress):\n",
    "    labels = C.input_variable(shape = Param.num_classes, is_sparse = Param.use_sparse)\n",
    "    z, cross_entropy, errs = create_model(labels)\n",
    "\n",
    "    # Setup the trainer\n",
    "    learning_rate_schedule = C.learning_rate_schedule(Param.learning_rate, C.UnitType.sample)\n",
    "    momentum_schedule = C.momentum_as_time_constant_schedule(Param.momentum_time_constant)\n",
    "    learner = C.momentum_sgd(z.parameters, learning_rate_schedule, momentum_schedule, True)\n",
    "    trainer = C.Trainer(z, cross_entropy, errs, learner)\n",
    "\n",
    "    minbatch = 0\n",
    "    average_cross_entropy = compute_average_cross_entropy(z)\n",
    "    minbatch_data = [0] # store minibatch values\n",
    "    cross_entropy_data = [average_cross_entropy] # store cross_entropy values\n",
    "\n",
    "    if do_print_progress:\n",
    "        progress_printer = ProgressPrinter(freq=Param.reporting_interval, tag='Training') \n",
    "\n",
    "    # Run training\n",
    "    for minbatch in range(1,Param.num_minbatches):\n",
    "        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "        label_data, indices = get_random_one_hot_data(Param.minibatch_size)\n",
    "        arguments = ({labels : label_data})\n",
    "\n",
    "        t_start = timeit.default_timer()\n",
    "        trainer.train_minibatch(arguments)\n",
    "        t_end = timeit.default_timer()\n",
    "        samples_per_second = Param.minibatch_size / (t_end - t_start)\n",
    "        # Print the progress using progress printer\n",
    "        # The prints numbers computed the the sampled softmax criterion\n",
    "        if do_print_progress:\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "        \n",
    "        # For comparison also print result using the full criterion\n",
    "        if minbatch % Param.reporting_interval == int(Param.reporting_interval/2):\n",
    "            # memorize the progress data for plotting\n",
    "            average_cross_entropy = compute_average_cross_entropy(z)\n",
    "            minbatch_data.append(minbatch)\n",
    "            cross_entropy_data.append(average_cross_entropy)\n",
    "            \n",
    "            if do_print_progress:\n",
    "                print(\"\\nMinbatch=%d Cross-entropy from full softmax = %.3f perplexity = %.3f samples/s = %.1f\"\n",
    "                    % (minbatch, average_cross_entropy, exp(average_cross_entropy), samples_per_second))\n",
    "                \n",
    "    return (minbatch_data, cross_entropy_data) \n",
    "\n",
    "def compute_average_cross_entropy(softmax_input):\n",
    "    vectors, indices = get_random_one_hot_data(Param.test_set_size)\n",
    "    total_cross_entropy = 0.0\n",
    "    arguments = (vectors)\n",
    "    z = softmax_input.eval(arguments).reshape(Param.test_set_size, Param.num_classes)\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        log_p = log_softmax(z[i], indices[i])\n",
    "        total_cross_entropy -= log_p\n",
    "\n",
    "    return total_cross_entropy / len(indices)\n",
    "\n",
    "# Computes log(softmax(z,index)) for a one-dimensional numpy array z in an numerically stable way.\n",
    "def log_softmax(z,    # numpy array\n",
    "                index # index into the array\n",
    "            ):\n",
    "    max_z = np.max(z)\n",
    "    return z[index] - max_z - log(np.sum(np.exp(z - max_z)))\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "print(\"start...\")\n",
    "train(do_print_progress = True)\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we use two different methods to report training progress:\n",
    "1. Using a function that computes the average cross entropy on full softmax.\n",
    "2. Using the build in ProgressPrinter\n",
    "\n",
    "ProgressPrinter reports how the value of the training criterion changes over time.\n",
    "In our case the training criterion is cross-entropy from **sampled** softmax.\n",
    "The same is true for the error rate computed by progress printer, this is computed only for true-class vs sampled-classes an will therefore underestimate the true error rate.\n",
    "\n",
    "Therefore while ProgressPrinter already gives us some idea how training goes on, if we want to compare the behavior for different sampling strategies (sample size, sampling weights, ...) you should should use full softmax.\n",
    "\n",
    "\n",
    "##Importance sampling\n",
    "\n",
    "Often the we don't have uniform distribution for the classes on the output side. The typical example is when we have words as output classes. A typical example are words where e.g. 'the' will be much more frequent than most others.\n",
    "\n",
    "In such cases one often uses a non uniform distribution for drawing the samples in sampled softmax but instead increases the sampling weight for the frequent classes. This is also called importane sampling.\n",
    "In our example the sampling distribution is controlled by the weight array `Param.softmax_sampling_weights`.\n",
    "\n",
    "As an example let's look at the case where the classes are distrubted according to zipf-distrubtion like:\n",
    "$$\n",
    "p[i] \\propto \\frac{1}{i+5},\n",
    "$$\n",
    "actually we use this distribution already in our example.\n",
    "\n",
    "How does training behavior change if we switch uniform sampling to sampling with the zipfian distribution in sampled softmax?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "[0, 5, 15, 25, 35, 45, 55, 65, 75, 85, 95]\n",
      "[3.9139626577051576, 3.4218997319756825, 2.188238443203629, 1.456344715189859, 1.0562772114585197, 0.67000616739098273, 0.51942568101385955, 0.58187898932217197, 0.41551955554750519, 0.33307973692846654, 0.28384260568774783]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1249de48>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFkCAYAAACjCwibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XecXVW99/HPL4WEloCUhBKlSAlFcIYqEEDavcClicIo\nV6QKQYSIREqw0UuoUgQUEHGugBcpj9INPLQHmaFJr4IEQpNEAqEk6/ljzTiTYSaZM+Xsc+Z83q/X\nfpGzz9pn/2ZPyHxn7bXXipQSkiRJ8zOo6AIkSVJ1MDRIkqRuMTRIkqRuMTRIkqRuMTRIkqRuMTRI\nkqRuMTRIkqRuMTRIkqRuMTRIkqRuMTRIkqRu6VVoiIijImJORJw5n3ZbRERTRMyKiGcjYu/enFeS\nJJVfj0NDRKwPHAg8Op92KwA3AXcA6wDnAJdGxDY9PbckSSq/HoWGiFgE+C2wP/DefJofDLyYUpqY\nUnompXQ+cC0woSfnliRJxehpT8P5wI0ppTu70XYj4PYO+24BNu7huSVJUgGGlHpAROwJrAus181D\nRgPTOuybBoyIiGEppY86OccSwHbAy8CsUmuUJKmGDQdWAG5JKb3Tlx9cUmiIiOWBs4GtU0qf9GUh\nHWwHXNWPny9J0kD3LeB3ffmBpfY01ANLAc0RES37BgPjIuJ7wLCUUupwzBvAqA77RgEzOutlaPEy\nwG9/+1vGjh1bYonqqQkTJnDWWWcVXUZN8ZqXn9e8/Lzm5fXUU0+x1157QcvP0r5Uami4HVi7w77L\ngaeAUzoJDAD3A//ZYd+2Lfu7Mgtg7Nix1NXVlViiemrkyJFe7zLzmpef17z8vOaF6fPb+yWFhpTS\nTODJ9vsiYibwTkrpqZbXJwHLpZRa52K4CDgkIk4Ffg1sBewObN/L2iVJUhn1xYyQHXsXlgHG/PvN\nlF4GdgC2Bh4hP2q5X0qp4xMVkiSpgpX89ERHKaWvdni9Tydt7iaPh5AkSVXKtSf0bw0NDUWXUHO8\n5uXnNS8/r/nAEZ2PXSxWRNQBTQ891ER9vYNnJEnqrubmZurr6wHqU0rNffnZFd3TcNttRVcgSZJa\nVXRomDwZZswougpJkgQVHhrefx9+8pOiq5AkSVDhoeHAA+Hcc+GRR4quRJIkVXRo+Na3YPXVYfx4\nmDOn6GokSaptFR0ahgyBCy+E+++Hyy4ruhpJkmpbRYcGgHHj4NvfhokT4e23i65GkqTaVfGhAeC0\n0/LtiaOPLroSSZJqV1WEhlGj4KST4NJL860KSZJUflURGiA/SbHeenDwwfDpp0VXI0lS7ama0DB4\ncB4U+dhjcP75RVcjSVLtqZrQAG09DccdB1OnFl2NJEm1papCA8AJJ8CCC8IPflB0JZIk1ZaqCw2L\nLw5nnAG//70LWkmSVE5VFxoA9torz99wyCHw0UdFVyNJUm2oytAQARdcAC+9BKefXnQ1kiTVhqoM\nDQBrrpnHNZx4Irz4YtHVSJI08FVtaID8FMVSS8Ghh0JKRVcjSdLAVtWhYZFF4Jxz4E9/guuvL7oa\nSZIGtqoODQC77AI77ADf/z7MnFl0NZIkDVxVHxoi4Nxz4a234Pjji65GkqSBq+pDA8BKK8Gxx8Lk\nyfDkk0VXI0nSwDQgQgPAkUfCiivC+PEOipQkqT8MmNAwbFheyOquu+Cqq4quRpKkgWfAhAaAbbaB\nPfaAI46A994ruhpJkgaWARUaAM48Ez78ECZNKroSSZIGlpJCQ0QcFBGPRsT0lu2+iPiPebTfPCLm\ndNhmR8TSvS+9c8suCz//eZ5m+qGH+usskiTVnlJ7Gl4FfgTUAfXAncD1ETF2HsckYBVgdMu2TErp\nzR7U2m3f+x6svTYcfDDMnt2fZ5IkqXaUFBpSSv8npXRzSumFlNLzKaVJwPvARvM59K2U0putW4+r\n7aYhQ+DCC3NPw8UX9/fZJEmqDT0e0xARgyJiT2Ah4P55NQUeiYipEXFrRHylp+csxVe+AvvtB0cf\nDdOmleOMkiQNbCWHhohYKyL+BXwEXADsmlJ6uovmrwPfBb4G7Ea+vTElItbtYb0lOeUUGDwYJk4s\nx9kkSRrYIpU4E1JEDAE+D4wEdgcOAMbNIzh0PH4K8PeU0t7zaFMHNI0bN46RI0fO9V5DQwMNDQ3d\nrvfSS+GAA2DKFNh8824fJklSxWtsbKSxsXGufdOnT+fuu+8GqE8pNffl+UoODZ/5gIjbgOdTSgd3\ns/1pwCYppU3m0aYOaGpqaqKurq5X9c2ZA5tsAjNmwCOPwNChvfo4SZIqWnNzM/X19dAPoaEv5mkY\nBAwrof265NsWZTFoUB4U+fTTcPbZ5TqrJEkDz5BSGkfEScCfgVeARYFvAZsD27a8fzKwbOuth4g4\nDHgJeAIYTr6VsSWwTR/V3y3rrguHHgo//SnsuSeMGVPOs0uSNDCU2tOwNHAF8DRwO3muhm1TSne2\nvD8aaP8jeQFgMvAYMAVYG9gqpTSl5yX3zM9/DiNHwuGHl/vMkiQNDCX1NKSU9p/P+/t0eH06cHoP\n6upzI0bAWWflnoY//Qm2377oiiRJqi4Dbu2JefnGN2DLLeGoo/IASUmS1H01FRoi8m2Kxx+HG28s\nuhpJkqpLTYUGgE03zfM1HH889PJpU0mSakrNhQaA446Dpia45ZaiK5EkqXrUZGj46ldho43sbZAk\nqRQ1GRoicm/Dfffl6aUlSdL81WRoAPjP/4S6utzbIEmS5q9mQ0METJoEf/kL3Htv0dVIklT5ajY0\nAOy8M6y1FpxwQtGVSJJU+Wo6NAwaBMceCzffDA89VHQ1kiRVtpoODQBf/zqsuqq9DZIkzU/Nh4bB\ng+GYY+D66+Gxx4quRpKkylXzoQHgm9+EFVaAE08suhJJkiqXoQEYOhSOPhquuQaefrroaiRJqkyG\nhhZ77w3LLQcnnVR0JZIkVSZDQ4thw2DiRPjd7+CFF4quRpKkymNoaGf//WHJJeGUU4quRJKkymNo\naGfBBeGHP4QrroBXXim6GkmSKouhoYODDoIRI+C004quRJKkymJo6GCRRWDCBLj0Unj99aKrkSSp\nchgaOvG978Hw4XDGGUVXIklS5TA0dGLkSPj+9+Gii+Ctt4quRpKkymBo6MJhh+UFrc46q+hKJEmq\nDIaGLiyxBIwfD7/4Bbz7btHVSJJUPEPDPPzgB/Dpp3DeeUVXIklS8QwN8zBqFBx4IJx9NsyYUXQ1\nkiQVy9AwH0ceCR98ABdcUHQlkiQVq6TQEBEHRcSjETG9ZbsvIv5jPsdsERFNETErIp6NiL17V3J5\nLbcc7LsvTJ4MM2cWXY0kScUptafhVeBHQB1QD9wJXB8RYztrHBErADcBdwDrAOcAl0bENj2stxA/\n+hG89x5cfHHRlUiSVJySQkNK6f+klG5OKb2QUno+pTQJeB/YqItDDgZeTClNTCk9k1I6H7gWmNC7\nsstrhRXgv/8bTj8dZs0quhpJkorR4zENETEoIvYEFgLu76LZRsDtHfbdAmzc0/MW5eijYdo0+PWv\ni65EkqRilBwaImKtiPgX8BFwAbBrSunpLpqPBqZ12DcNGBERw0o9d5FWWQX23DMvm/3xx0VXI0lS\n+fWkp+Fp8viEDYALgd9ExOp9WlWFOuYYePVVuPLKoiuRJKn8hpR6QErpU+DFlpcPR8QGwGHk8Qsd\nvQGM6rBvFDAjpfTR/M41YcIERo4cOde+hoYGGhoaSi27T6y5Jnzta3DSSbD33jCk5KsnSVLfaWxs\npLGxca5906dP77fzRUqpdx8QcQfw95TSvp28dwrwnymlddrt+x2wWEpp+3l8Zh3Q1NTURF1dXa/q\n62sPPwx1dbm3Ya+9iq5GkqS5NTc3U19fD1CfUmruy88udZ6GkyJis4j4QsvYhpOBzYHftrx/ckRc\n0e6Qi4CVIuLUiFgtIsYDuwNn9tUXUG5f/jLsuCOceCLMnl10NZIklU+pYxqWBq4gj2u4nTxXw7Yp\npTtb3h8NjGltnFJ6GdgB2Bp4hPyo5X4ppY5PVFSVSZPg6afhf/+36EokSSqfku7Kp5T2n8/7+3Sy\n725yuBgwNtwQttkGTjghj3EY5GTckqQa4I+7Hpo0CR57DG66qehKJEkqD0NDD40bl7fjj4dejiWV\nJKkqGBp6YdIkeOghuPXWoiuRJKn/GRp6Yeut8/gGexskSbXA0NALEbm34d574a67iq5GkqT+ZWjo\npR12gHXXzb0NkiQNZIaGXmrtbbjzTrjvvqKrkSSp/xga+sCuu8Iaa+R5GyRJGqgMDX1g0CA49lj4\n85+hqanoaiRJ6h+Ghj6yxx6wyir2NkiSBi5DQx8ZPBiOPhr++Ed4/PGiq5Ekqe8ZGvrQXnvBF76Q\nV8CUJGmgMTT0oaFD4aij4Oqr4Zlniq5GkqS+ZWjoY/vsA8ssAyedVHQlkiT1LUNDHxs2DCZOhKuu\nghdfLLoaSZL6jqGhHxxwACyxBJxyStGVSJLUdwwN/WChheCII+Dyy+HVV4uuRpKkvmFo6CcHHwyL\nLgqnnVZ0JZIk9Q1DQz9ZdFE4/HC45BJ4442iq5EkqfcMDf3o0EPzwMgzzii6EkmSes/Q0I8WWywH\nhwsvhLffLroaSZJ6x9DQzw4/PC+ffdZZRVciSVLvGBr62ZJL5kGR550H//xn0dVIktRzhoYyOOII\n+OSTHBwkSapWhoYyGD06T/h09tnwr38VXY0kST1jaCiTiRPh/ffhgguKrkSSpJ4xNJTJ8svnxawm\nT4YPPii6GkmSSmdoKKOjjoJ334WLLy66EkmSSldSaIiIoyPiwYiYERHTIuK6iFh1PsdsHhFzOmyz\nI2Lp3pVefVZcEfbaC04/HWbNKroaSZJKU2pPw2bAecCGwNbAUODWiFhwPsclYBVgdMu2TErpzRLP\nPSAccwy8/jpcdlnRlUiSVJqSQkNKafuU0pUppadSSo8D3wE+D9R34/C3Ukpvtm49qHVAWHVV2GOP\nvGz2J58UXY0kSd3X2zENi5F7Ed6dT7sAHomIqRFxa0R8pZfnrWrHHguvvAJXXll0JZIkdV+PQ0NE\nBHA2cE9K6cl5NH0d+C7wNWA34FVgSkSs29NzV7u11oJdd4WTT4ZPPy26GkmSuqc3PQ0XAGsAe86r\nUUrp2ZTSJSmlh1NKD6SU9gPuAyb04txVb9IkeP55+P3vi65EkqTuGdKTgyLiF8D2wGYppdd78BEP\nApvMr9GECRMYOXLkXPsaGhpoaGjowSkrS10dbL89nHgiNDTAIB9+lSSVqLGxkcbGxrn2TZ8+vd/O\nFyml0g7IgWFnYPOU0os9OmnErcCMlNLuXbxfBzQ1NTVRV1fXk1NUhfvvh698Ba65Bnbv9EpIklSa\n5uZm6uvrAepTSs19+dmlztNwAfAt4JvAzIgY1bINb9fmpIi4ot3rwyJip4hYOSLWjIizgS2BX/TR\n11C1Nt4YttoKTjgBSsxukiSVXamd4gcBI4ApwNR22zfatVkGGNPu9QLAZOCxluPWBrZKKU3pScED\nzXHHwaOPwk03FV2JJEnzVtKYhpTSfENGSmmfDq9PB04vsa6aMW4cbLopHH887LgjRBRdkSRJnXP4\nXcEicm/DX/8Kt91WdDWSJHXN0FABttkG1l8/9zY4tkGSVKkMDRWgtbfhnnvg7ruLrkaSpM4ZGirE\njjvCOuvk3gZJkiqRoaFCRORZIu+4I8/fIElSpTE0VJDddoOxY/O8DZIkVRpDQwUZNCivgPmnP0Fz\nn87hJUlS7xkaKswee8DKK9vbIEmqPIaGCjNkCBxzDFx3Hfztb0VXI0lSG0NDBfrv/4bPfz6vgClJ\nUqUwNFSgoUPhqKPg97+HZ54puhpJkjJDQ4XaZx9YZhk4+eSiK5EkKTM0VKjhw+HII+G3v4WXXiq6\nGkmSDA0V7cAD4XOfg1NOKboSSZIMDRVtoYXgiCPgssvgH/8ouhpJUq0zNFS48eNhkUXgtNOKrkSS\nVOsMDRVu0UXh8MPhkkvgjTeKrkaSVMsMDVXg+9+HBRaAyZOLrkSSVMsMDVVgscXge9+DCy+El18u\nuhpJUq0yNFSJH/4Qll4adt4ZZs4suhpJUi0yNFSJxReHG26AF1+EvfeGOXOKrkiSVGsMDVVkrbXy\nZE9/+IOrYEqSys/QUGV23jkHhp/8BP73f4uuRpJUS4YUXYBKd8wx8PjjeTXMlVeGddYpuiJJUi2w\np6EKRcCvfw2rrZZ7Ht56q+iKJEm1wNBQpRZaCP74R/jwQ9h9d/j446IrkiQNdIaGKvb5z+dxDfff\nnyeAkiSpPxkaqtwmm8BFF8Evf5knf5Ikqb+UFBoi4uiIeDAiZkTEtIi4LiJW7cZxW0REU0TMiohn\nI2LvnpesjvbdFw47LPc2/OUvRVcjSRqoSu1p2Aw4D9gQ2BoYCtwaEQt2dUBErADcBNwBrAOcA1wa\nEdv0oF514YwzYIst4OtfzxNASZLU10p65DKltH371xHxHeBNoB64p4vDDgZeTClNbHn9TERsCkwA\nbiupWnVpyBD4/e9hww3zExX33ZdXyJQkqa/0dkzDYkAC3p1Hm42A2zvsuwXYuJfnVgef+1yeavrv\nf89zODjVtCSpL/U4NEREAGcD96SUnpxH09HAtA77pgEjImJYT8+vzo0dC7/7XQ4PP/lJ0dVIkgaS\n3swIeQGwBrBJH9XyGRMmTGDkyJFz7WtoaKChoaG/Tjkg7LgjnHwyHHVUXq9ijz2KrkiS1B8aGxtp\nbGyca9/06dP77XyRUir9oIhfAP8FbJZSemU+be8CmlJKP2i37zvAWSmlxbs4pg5oampqoq6uruT6\nBCnBXnvBddfBPfeAl1GSakNzczP19fUA9Sml5r787JJvT7QEhp2BLecXGFrcD2zVYd+2LfvVTyLg\n0kthzTVhl11gWscbRJIklajUeRouAL4FfBOYGRGjWrbh7dqcFBFXtDvsImCliDg1IlaLiPHA7sCZ\nfVC/5mHBBfNU0598ArvtBh99VHRFkqRqVmpPw0HACGAKMLXd9o12bZYBxrS+SCm9DOxAntfhEfKj\nlvullDo+UaF+sNxyOTg0NcH48fm2hSRJPVHqPA3zDRkppX062Xc3eS4HFWDDDeHii2HvvfMy2q5T\nIUnqid48PaEq8u1vw2OPwYQJ+bHMbZyPU5JUIhesqiGnngrbbpsfwXzuuaKrkSRVG0NDDRk8GBob\nYaml8lTT/fgoryRpADI01JjFFsuzRU6dCt/6FsyeXXRFkqRqYWioQautlhe3+vOfYdKkoquRJFUL\nQ0ON2m47OP10OOWUvFaFJEnz49MTNWzChPxExX77wSqrwPrrF12RJKmS2dNQwyLgoovy3A277AKv\nv150RZKkSmZoqHHDh+dFrQB23RVmzSq2HklS5TI0iGWWyVNNP/ooHHigU01LkjpnaBCQxzP86ldw\n5ZVwpkuJSZI64UBI/ds3vwmPPw4TJ+Yltf/jP4quSJJUSexp0FxOOAG23x723BOeeaboaiRJlcTQ\noLkMHgxXXQXLLgs77QTvvVd0RZKkSmFo0GeMGJGnmn7rrdzj4FTTkiQwNKgLX/wiXH013H47/OhH\nRVcjSaoEhgZ1aeut85MUkyfDFVcUXY0kqWg+PaF5OvTQPNX0gQfmha422qjoiiRJRbGnQfMUAeef\nn+dx2HVXeO21oiuSJBXF0KD5GjYM/vAHGDo0r1Hx4YdFVyRJKoKhQd0yahRcfz088QTsv79TTUtS\nLTI0qNu+/GW4/HL43e/gtNOKrkaSVG4OhFRJvvGNPNX00UfnqaZ33LHoiiRJ5WJPg0r2s5/l2SK/\n+U148smiq5EklYuhQSUbNCivhvmFL+Tw8O67RVckSSoHQ4N6ZNFF81TT772Xb1l8+mnRFUmS+puh\nQT224opw7bVw111wxBFFVyNJ6m+GBvXKFlvAuefm7Ve/KroaSVJ/Kjk0RMRmEXFDRLwWEXMiYqf5\ntN+8pV37bXZELN3zslVJDj4Yvvvd/N977y26GklSf+lJT8PCwCPAeKC7U/wkYBVgdMu2TErpzR6c\nWxXq3HNh441ht93glVeKrkaS1B9KDg0ppZtTSj9OKV0PRAmHvpVSerN1K/W8qmwLLJDHNyy4IOy8\nM8ycWXRFkqS+Vq4xDQE8EhFTI+LWiPhKmc6rMlpqqfxExXPPwT77ONW0JA005QgNrwPfBb4G7Aa8\nCkyJiHXLcG6V2Ze+BL/5DVxzDZx4YtHVSJL6Ur9PI51SehZ4tt2uByJiZWACsPe8jp0wYQIjR46c\na19DQwMNDQ19Xqf6zm675VkjjzsO1lorr4wpSep7jY2NNDY2zrVv+vTp/Xa+SL3oQ46IOcAuKaUb\nSjzuNGCTlNImXbxfBzQ1NTVRV1fX4/pUnDlzYI894Oab4b77YO21i65IkmpDc3Mz9fX1APUppea+\n/Oyi5mlYl3zbQgPUoEF5RcyVV84DI99+u+iKJEm91ZN5GhaOiHXajUlYqeX1mJb3T46IK9q1Pywi\ndoqIlSNizYg4G9gS+EWffAWqWAsvDNdfD++/D5tsAvffX3RFkqTe6ElPw3rAw0ATef6FyUAz8LOW\n90cDY9q1X6ClzWPAFGBtYKuU0pQeVayq8oUvwN13w+KLw6abwpFHwocfFl2VJKknSh4ImVK6i3mE\njZTSPh1enw6cXnppGihWXx3uuQfOPBN+/GO48Ua47LI8GZQkqXq49oTKYsgQmDgRHn4YRo6010GS\nqpGhQWU1dmxen+Lkk+G88+DLX3asgyRVC0ODys5eB0mqToYGFcZeB0mqLoYGFcpeB0mqHoYGVYTO\neh0eeKDoqiRJ7RkaVDE69jpssom9DpJUSQwNqjj2OkhSZTI0qCLZ6yBJlcfQoIpmr4MkVQ5Dgyqe\nvQ6SVBkMDaoa9jpIUrEMDaoq9jpIUnEMDapK9jpIUvkZGlS1WnsdmpvtdZCkcjA0qOqtsUbudTjp\nJDj3XHsdJKm/GBo0IAwZAj/6kWMdJKk/GRo0oNjrIEn9x9CgAcdeB0nqH4YGDVj2OkhS3zI0aECz\n10GS+o6hQTWhY69DXZ29DpJUKkODakb7XocRI3Kvw8SJMGtW0ZVJUnUwNKjmtO91OOccxzpIUncZ\nGlST7HWQpNIZGlTT7HWQpO4zNKjm2esgSd1jaJBa2OsgSfNWcmiIiM0i4oaIeC0i5kTETt04ZouI\naIqIWRHxbETs3bNypf7VWa/DhAkwc2bRlUlS8XrS07Aw8AgwHkjzaxwRKwA3AXcA6wDnAJdGxDY9\nOLdUFq29DqeeCr/8Jay1Ftx2W9FVSVKxSg4NKaWbU0o/TildD0Q3DjkYeDGlNDGl9ExK6XzgWmBC\nqeeWymnIEPjhD+Hxx2GllWDbbeE734F33y26MkkqRjnGNGwE3N5h3y3AxmU4t9RrK68Mt98Ol14K\nf/wjjB0LV18Nab79bJI0sJQjNIwGpnXYNw0YERHDynB+qdciYL/94KmnYLPNYI89YOed4R//KLoy\nSSqfIUUXMC8TJkxg5MiRc+1raGigoaGhoIpU65ZZBq69Fq67Dg45BNZcM497OPBAGOSzSJLKrLGx\nkcbGxrn2TZ8+vd/OF6kXfawRMQfYJaV0wzza3AU0pZR+0G7fd4CzUkqLd3FMHdDU1NREXV1dj+uT\n+tN77+UVMy+9FMaNg0sugVVXLboqSbWuubmZ+vp6gPqUUnNffnY5fje6H9iqw75tW/ZLVWuxxXJQ\nuPNOeO01+NKX4OST4ZNPiq5MkvpHT+ZpWDgi1omIdVt2rdTyekzL+ydHxBXtDrmopc2pEbFaRIwH\ndgfO7HX1UgXYcsv8hMVhh8Fxx8H668NDDxVdlST1vZ70NKwHPAw0kedpmAw0Az9reX80MKa1cUrp\nZWAHYGvy/A4TgP1SSh2fqJCq1oIL5rENDz6YB01uuGG+dfHBB0VXJkl9p+SBkCmlu5hH2Egp7dPJ\nvruB+lLPJVWburocHM48E3760zxg8uKL4atfLboySeo9x3tLfWzo0DwV9WOPwfLLw1Zbwf77wz//\nWXRlktQ7hgapn6yySh4k+ctfwjXX5Kmp//CHoquSpJ4zNEj9aNCgPIfDk0/mcQ677w677QZTpxZd\nmSSVztAglcFyy+XxDddcA/fdl3sdLrnEqaglVRdDg1QmEbmn4cknc2/DgQfmAZLPP190ZZLUPYYG\nqcw+9zn49a/zUtt//zusvTacdhp8+mnRlUnSvBkapIJsvXWeFOqQQ+Doo2GDDeDhh4uuSpK6ZmiQ\nCrTwwnDGGfDAAzB7dp5N8qij4MMPi65Mkj7L0CBVgNapp3/2MzjrLFhnHbjrrqKrkqS5GRqkCjF0\nKBx7LDz6KIwaBVtsAd/9LvTjKreSVBJDg1RhVl899zJccAE0NubHM6+/vuiqJMnQIFWkQYPg4IPh\niSfyeha77AJf/zq88UbRlUmqZYYGqYKNGQM33JB7HO66K/c6XHaZk0JJKoahQapwEbDnnvDUU/Bf\n/wX77gvbbgsvvlh0ZZJqjaFBqhJLLAFXXAE33wzPPQdrrQWTJzsplKTyMTRIVWa77eBvf8vTUB95\nJGy8cV6GW5L6m6FBqkKLLAJnn50Xv/rwQ6ivh0mTYNasoiuTNJAZGqQqttFG0NwMxx2X169Yd134\nv/+36KokDVSGBqnKLbAA/PjH8MgjeTGsceNg/Hh4552iK5M00BgapAFijTVyL8O558JvfgNLLZXn\neDjiCLjxRnjvvaIrlFTtDA3SADJ4MBx6KDz/fF5++0tfgmuvhZ12yr0Q9fU5RNx0k9NTSyrdkKIL\nkNT3Ro+G73wnbynByy/DlCnwl7/A1VfDmWfmWSe//OW8xsWWW8Kmm8LIkYWWLanCGRqkAS4CVlwx\nb/vsk0PESy+1hYj/+Z8838OgQfl2RvsQMWJE0dVLqiSGBqnGRMBKK+Vt331ziHjxxbYQ0dgIZ5yR\nQ0R9fQ4RW2xhiJBkaJBqXgSsvHLe9tsvh4gXXmgLEVddBaefnsdLdAwRiy5acPGSysrQIGkuEfDF\nL+Zt//1ziHj++bYQceWVeU6IwYNhvfXmDhGLLFJw8WU2ezZMmwavvQZTp+axJOutl6+NNBAZGiTN\nUwSsskoBDPeKAAANoElEQVTeDjggh4jnnmsLEVdcAaeemn9Qrr9+W4jYZJPqDREp5adLpk5tCwSv\nvfbZP7/xBsyZM/exiy8OW2+dp/vebjtYfvlivgapP0TqwRq7EXEI8ENgNPAocGhK6a9dtN0c+EuH\n3QlYJqX0ZhfH1AFNTU1N1NXVlVyfpPJJCZ59ti1ETJmSf/seMuSzIWLhhYutFeDjj/MP/q6CQOuf\nP/hg7uM+9zlYbjlYdtn8345/XmaZPDbkllvy9te/5muzxhptAWLcOFhwwWK+btWO5uZm6uvrAepT\nSs19+dklh4aI2AO4AjgQeBCYAHwdWDWl9HYn7TcH7gRWBf7Vur+rwNByjKFBqlIpwTPPtAWIKVPg\nzTdziNhgg7YQ8ZWv9G2ISAnefnvePQNTp8Jbb8193PDhcweAzkLBssvmdqV45x24/fa2EDF1av6M\ncePaQsQaa+SeHKkvVVpoeAD4fymlw1peB/AqcG5K6bRO2reGhsVTSjO6eQ5DgzRApARPPz13iHjr\nLRg69LMhYqGFOv+MmTPn3zPw+uu5F6FVBIwaNe/egWWXzbcT+vsHd0rwxBNtAeLuu+Gjj/Kti223\nzQFi661zb4bUWxUTGiJiKPAB8LWU0g3t9l8OjEwp7drJMa23J14GhgN/A36aUrpvHucxNEgDVErw\n1FNzh4i3384hYsMN8xMa7703dyDoOHvliBHzDgLLLZcHJQ6p0FFbH3yQp/xuDRFPPpkfcV1//bZe\niA02qNz6Vdn6MzSU+ldySWAwMK3D/mnAal0c8zrwXeAhYBhwADAlIjZIKT1S4vklVbmI3C2/xhpw\nyCE5RDz5ZNuYiD/9CZZcMv/wX2ONzkNBtQ6wbLXQQm3hAODVV+HWW3OAOO88+PnPYbHFYKut2tp9\n/vPF1ixBGZ6eSCk9CzzbbtcDEbEyeSzE3v19fkmVLQLWXDNvhxxSdDXFGDMmz5Gx3375Mc6//rWt\nF+Kgg/ITGquv3hYgNt+861s5Un/q99sTXXzOacAmKaVNuni/DmgaN24cIztMht/Q0EBDQ0O3a5ak\navbPf8Idd7SFiFdfhWHDYLPN2kLEWms5oLJWNTY20tjYONe+6dOnc/fdd0PRYxqgy4GQr5AHQp7e\nzc+4FZiRUtq9i/cd0yBJHbQOKm0NEHfdBR9+mG/ZbLtt3rbZJt/eUe2qpDENAGcCl0dEE22PXC4E\nXA4QEScDy6aU9m55fRjwEvAEeSDkAcCWwDa9LV6SakkEjB2bt8MPh1mz5h5QefnluU19fVsvxEYb\n5UGmUl8YVOoBKaWryRM7/Rx4GPgSsF1KqfXp59HAmHaHLABMBh4DpgBrA1ullKb0uGpJEsOH556F\nM86Axx+Hf/wDfvWrvI7IhRfmOSGWXBJ23RUuuiivbir1Ro9mhOxv3p6QpN6ZPRuamtp6IR54IO9b\nZZW2Xogttqj+J1H0WZV2e0KSVOEGD85zPWywARx3XJ7r4s47c4C46Sb4xS/ybYtNN80TS40d27ba\naSVM963KZGiQpBowcmS+TbHrrm2LjrX2QpxyCvzrX21tR43Kq5y2hojW7YtfhCWW8EmNWmZokKQa\nEwGrrpq3Qw/NIeLNN+GFF9q255/PC5H9+c9zr9cxYkTnYWLllfO02INKHimnamJokKQa17pOx6hR\neQ2QjmbMyCt4toaJ1mDx17/meSNalwdfYAFYccW5g0TrtuKKeX4JVTdDgyRpnkaMgHXXzVtHH38M\nL788d5h44QW47Tb45S/zwlyQg8mYMZ/tpWjdOszjpwplaJAk9dgCC7Td6uhozpy84Fj7MPHCC/Dw\nw3DNNXMvRLbkkp3f8lh55dwD4jiKymBokCT1i0GDcu/CmDH58c72UoJ33/1soHj++bxw2euvt7Vd\neGFYaaXOB2cuv3wOLioPQ4Mkqewi8pMYSyyRHwvt6IMP2sZRtB9Lcd11+XbI7NltbUeMgKWW6v7m\nYl89Z2iQJFWchRbKC3GttdZn3/v0U3jllRwiXnstP93Rfvvb39r+/P77nX/2UkvB0kt3L2Qssoi3\nR1oZGiRJVWXIkHy7YqWV5t921qzPhoqO23PPwX335T+/995nP2PYsNJ6MhZbbOCGDEODJGnAGj68\nbVxFd3zyCbz99rxDxquvQnNz/vM77+TxGe0NGZIHds4vXGywASy4YN9/zf3J0CBJUouhQ2GZZfLW\nHbNn5wGd8+vNePLJ/N+33863VyAvILbCCv32pfQLQ4MkST00eHBbz0F3pJRvgbz1Vn7yo9oYGiRJ\nKpMIWHzxvFUjZwmXJEndYmiQJEndYmiQJEndYmiQJEndYmiQJEndYmiQJEndYmiQJEndYmiQJEnd\nYmiQJEndYmiQJEndYmiQJEndYmiQJEndYmiQJEndYmjQvzU2NhZdQs3xmpef17z8vOYDR49CQ0Qc\nEhEvRcSHEfFARKw/n/ZbRERTRMyKiGcjYu+elav+5P/Y5ec1Lz+vefl5zQeOkkNDROwBTAZ+AnwZ\neBS4JSKW7KL9CsBNwB3AOsA5wKURsU3PSpYkSUXoSU/DBOCXKaXfpJSeBg4CPgD27aL9wcCLKaWJ\nKaVnUkrnA9e2fI4kSaoSJYWGiBgK1JN7DQBIKSXgdmDjLg7bqOX99m6ZR3tJklSBhpTYfklgMDCt\nw/5pwGpdHDO6i/YjImJYSumjTo4ZDvDUU0+VWJ56Y/r06TQ3NxddRk3xmpef17z8vObl1e5n5/C+\n/uxSQ0O5rACw1157FVxG7amvry+6hJrjNS8/r3n5ec0LsQJwX19+YKmh4W1gNjCqw/5RwBtdHPNG\nF+1ndNHLAPn2xbeAl4FZJdYoSVItG04ODLf09QeXFBpSSp9ERBOwFXADQEREy+tzuzjsfuA/O+zb\ntmV/V+d5B/hdKbVJkqR/69MehlY9eXriTOCAiPh2RKwOXAQsBFwOEBEnR8QV7dpfBKwUEadGxGoR\nMR7YveVzJElSlSh5TENK6eqWORl+Tr7N8AiwXUrprZYmo4Ex7dq/HBE7AGcB3wf+AeyXUur4RIUk\nSapgkZ+YlCRJmjfXnpAkSd1iaJAkSd1ScaGh1MWw1H0RcXREPBgRMyJiWkRcFxGrdtLu5xExNSI+\niIjbIuKLRdQ70ETEURExJyLO7LDf693HImLZiLgyIt5uua6PRkRdhzZe9z4SEYMi4viIeLHlej4f\nEZM6aec176GI2CwiboiI11r+HdmpkzbzvL4RMSwizm/5/+JfEXFtRCxdSh0VFRpKXQxLJdsMOA/Y\nENgaGArcGhELtjaIiB8B3wMOBDYAZpK/BwuUv9yBoyX8Hkj+O91+v9e7j0XEYsC9wEfAdsBY4Ajg\nn+3aeN371lHAd4HxwOrARGBiRHyvtYHXvNcWJj94MB74zGDEbl7fs4EdgK8B44BlgT+UVEVKqWI2\n4AHgnHavg/y0xcSiaxuIG3la8DnApu32TQUmtHs9AvgQ+EbR9VbrBiwCPAN8FfgLcKbXu1+v9ynA\nXfNp43Xv22t+I3BJh33XAr/xmvfL9Z4D7NRh3zyvb8vrj4Bd27VZreWzNujuuSump6GHi2GpdxYj\nJ9Z3ASJiRfIjs+2/BzOA/4ffg944H7gxpXRn+51e737zX8BDEXF1y2245ojYv/VNr3u/uA/YKiJW\nAYiIdYBNgD+1vPaa96NuXt/1yNMstG/zDPAKJXwPKmntiZ4shqUeapnJ82zgnpTSky27R5NDRGff\ng9FlLG/AiIg9gXXJ/8N25PXuHysBB5NvdZ5I7qo9NyI+Sildide9P5xC/k326YiYTb71fWxK6X9a\n3vea96/uXN9RwMctYaKrNvNVSaFB5XUBsAb5twH1g4hYnhzMtk4pfVJ0PTVkEPBgSum4ltePRsRa\nwEHAlcWVNaDtAXwT2BN4khyUz4mIqS1BTQNExdyeoGeLYakHIuIXwPbAFiml19u99QZ5HInfg75R\nDywFNEfEJxHxCbA5cFhEfExO+F7vvvc68FSHfU8Bn2/5s3/P+95pwCkppWtSSk+klK4izwJ8dMv7\nXvP+1Z3r+wawQESMmEeb+aqY0NDym1jrYljAXIth9cvCG7WoJTDsDGyZUnql/XsppZfIf3nafw9G\nkJ+28HtQutuBtcm/da3Tsj0E/BZYJ6X0Il7v/nAvn72luRrwd/DveT9ZiPxLX3tzaPkZ4zXvX928\nvk3Apx3arEYO010uINlRpd2eOBO4vGUlzQeBCbRbDEu9ExEXAA3ATsDMiGhNpdNTSq1LkJ8NTIqI\n58lLkx9PfoLl+jKXW/VSSjPJXbX/FhEzgXdSSq2/CXu9+95ZwL0RcTRwNfkfzv2BA9q18br3rRvJ\n1/MfwBNAHfnf70vbtfGa90JELAx8kdyjAHkhyHWAd1NKrzKf65tSmhERvwLOjIh/Av8ir059b0rp\nwW4XUvSjI508SjK+5Qv+kJx+1iu6poGykZP/7E62b3do91Py4zsfkNdj/2LRtQ+UDbiTdo9cer37\n7TpvDzzWck2fAPbtpI3Xve+u98LkX/peIs8P8BzwM2CI17zPrvHmXfwb/uvuXl9gGHmunrdbQsM1\nwNKl1OGCVZIkqVsqZkyDJEmqbIYGSZLULYYGSZLULYYGSZLULYYGSZLULYYGSZLULYYGSZLULYYG\nSZLULYYGSZLULYYGSZLULYYGSZLULf8f8MaLS8pGWfIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124c77b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"start...\")\n",
    "minibatch_data, cross_entropy_data = train(do_print_progress = False)\n",
    "plt.plot(minibatch_data, cross_entropy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
