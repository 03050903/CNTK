{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled Softmax\n",
    "\n",
    "For classification and prediction problems a typical criterion function is cross-entropy with softmax. If the number of output classes is high the computation of this criterion and the corresponding gradients could be quite costly. Sampled Softmax is heuristic to speed up training in these cases.\n",
    "\n",
    "## Basics\n",
    "\n",
    "The softmax function is used in neural networks if we want to interpret the network output as a probability distribution over a set of classes $C$ with $|C|=N_C$.\n",
    "\n",
    "Softmax maps an $N_C$-dimensional vector $z$, which has unrestricted values, to an $N_C$ dimensional vector $p$ whith non-negative values that sum up to 1 so that they can be interpreted as probabilities. More precisely:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_i &= softmax(z, i)\\\\\n",
    "    &= \\frac{exp(z_i)}{\\sum_{k\\in C} exp(z_k)}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In what follows we assume that the input $z$ to the softmax is computed from some hidden vector $h$ of dimension $N_h$  in a specific way, namely:\n",
    "$$ z = W h + b $$\n",
    "where $W$ is a learnable weight matrix of dimension $(N_c, N_h)$ and is $b$ a learnable bias vector.\n",
    "We restrict our self to this specific choice of $z$ because it helps in implementing an efficient sampled softmax.\n",
    "\n",
    "In a typical use-case like for example a recurrent language model, the hidden vector $h$ would be the output of the recurrent layers of dimension $N_h\" and $C$ would be the set of words to predict.   \n",
    "\n",
    "As a training criterion, we use cross-entropy which is a function of the expected (true) class $t\\in C$ and the probability predicted for it:\n",
    "$$cross\\_entropy := -log(p_t)$$\n",
    "\n",
    "## Sampled Softmax from the outside\n",
    "\n",
    "For the normal softmax the python api provides the function [cross_entropy_with_softmax](https://cntk.ai/pythondocs/cntk.ops.html?highlight=softmax#cntk.ops.cross_entropy_with_softmax). This takes as input the vector $N_C$-dimensional vector $z$. As mentioned for sampled softmax we assume that this z is computed by $ z = W h + b $. In sampled softmax this has to be part of the whole implementation of the criterion.\n",
    "\n",
    "Below we show the code for `cross_entropy_with_sampled_softmax_and_embedding`. Letâ€™s look at signature first.\n",
    "\n",
    "One fundamental difference to the corresponding function in the Python api (`cross_entropy_with_softmax`) is that in the Python api function the input corresponds to $z$ and must have the same dimension as the target vector, while in cross_entropy_with_full_softmax the input corresponds to our hidden vector $h$ can have any dimension (hidden_dim).\n",
    "Actually, hidden_dim will be typically much lower than the dimension of the target vector.\n",
    "\n",
    "We also have some additional parameter `num_samples, sampling_weights, allow_duplicates` that control the random sampling. \n",
    "Another difference to the api function is that we return a tripple (z, cross_entropy_on_samples, error_on_samples).\n",
    "\n",
    "We will come boack to the details of the implementation below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cntk as C\n",
    "\n",
    "# Creates a model subgraph computing cross-entropy with sampled softmax.\n",
    "def cross_entropy_with_sampled_softmax_and_embedding(\n",
    "    hidden_vector,            # Node providing hidden input\n",
    "    target_vector,            # Node providing the expected labels (as sparse vectors)\n",
    "    vocab_dim,                # Vocabulary size\n",
    "    hidden_dim,               # Dimension of the hidden vector\n",
    "    num_samples,              # Number of samples to use for sampled softmax\n",
    "    sampling_weights,         # Node providing weights to be used for the weighted sampling\n",
    "    allow_duplicates = False, # Boolean flag to control whether to use sampling with replacemement (allow_duplicates == True) or without replacement.\n",
    "    ):\n",
    "    bias = C.Parameter(shape = (vocab_dim, 1), init = C.init_bias_default_or_0)\n",
    "    weights = C.Parameter(shape = (vocab_dim, hidden_dim), init = C.init_default_or_glorot_uniform)\n",
    "\n",
    "    sample_selector_sparse = C.random_sample(sampling_weights, num_samples, allow_duplicates) # sparse matrix [num_samples * vocab_size]\n",
    "    sample_selector = sample_selector_sparse\n",
    "\n",
    "    inclusion_probs = C.random_sample_inclusion_frequency(sampling_weights, num_samples, allow_duplicates) # dense row [1 * vocab_size]\n",
    "    log_prior = C.log(inclusion_probs) # dense row [1 * vocab_dim]\n",
    "\n",
    "    wS = C.times(sample_selector, weights) # [num_samples * hidden_dim]\n",
    "    zS = C.times_transpose(wS, hidden_vector) + C.times(sample_selector, bias) - C.times_transpose (sample_selector, log_prior)# [num_samples]\n",
    "\n",
    "    # Getting the weight vector for the true label. Dimension hidden_dim\n",
    "    wT = C.times(target_vector, weights) # [1 * hidden_dim]\n",
    "    zT = C.times_transpose(wT, hidden_vector) + C.times(target_vector, bias) - C.times_transpose(target_vector, log_prior) # [1]\n",
    "\n",
    "\n",
    "    zSReduced = C.reduce_log_sum(zS)\n",
    "    \n",
    "    # Compute the cross entropy that is used for training.\n",
    "    # We don't check whether any of the classes in the random samples conincides with the true label, so it might\n",
    "    # happen that the true class is counted\n",
    "    # twice in the normalising demnominator of sampled softmax.\n",
    "    cross_entropy_on_samples = C.log_add_exp(zT, zSReduced) - zT\n",
    "\n",
    "    # For applying the model we also output a node providing the input for the full softmax\n",
    "    z = C.times_transpose(weights, hidden_vector) + bias\n",
    "    z = C.reshape(z, shape = (vocab_dim))\n",
    "\n",
    "    zSMax = C.reduce_max(zS)\n",
    "    error_on_samples = C.less(zT, zSMax)\n",
    "    return (z, cross_entropy_on_samples, error_on_samples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a better idea of what the inputs and outputs are and how this all differs from the normal softmax we give below a corresponding function using normal softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates model subgraph computing cross-entropy with (full) softmax.\n",
    "def cross_entropy_with_softmax_and_embedding(\n",
    "    hidden_vector,  # Node providing hidden input\n",
    "    target_vector,  # Node providing the expected labels (as sparse vectors)\n",
    "    vocab_dim,      # Vocabulary size\n",
    "    hidden_dim      # Dimension of the hidden vector\n",
    "    ):\n",
    "    # Setup bias and weights\n",
    "    bias = C.Parameter(shape = (vocab_dim, 1), init = C.init_bias_default_or_0)\n",
    "    weights = C.Parameter(shape = (vocab_dim, hidden_dim), init = C.init_default_or_glorot_uniform)\n",
    "\n",
    "    \n",
    "    z = C.reshape( C.times_transpose(weights, hidden_vector) + bias, (1,vocab_dim))\n",
    "    \n",
    "    # Use cross_entropy_with_softmax\n",
    "    cross_entropy = C.cross_entropy_with_softmax(z, target_vector)\n",
    "\n",
    "    zMax = C.reduce_max(z)\n",
    "    zT = C.times_transpose(z, target_vector)\n",
    "    error_on_samples = C.less(zT, zMax)\n",
    "\n",
    "    return (z, cross_entropy, error_on_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the main differences to the api function `cross_entropy_with_softmax` are:\n",
    "* We include an embedding.\n",
    "* We return a tripple (z, cross_entropy, error_on_samples) instead of just returnting the cross entropy.\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "To explain how to integrate sampled softmax let us look at a toy example. In this toy example we first transform one-hot input vectors via some random projection into some lower dimensional vector $h$. The modeling task is to reverse this mapping using (sampled) softmax. Well, as already said this is a toy example.\n",
    "\n",
    "So let's first look how we could solve this with the normal softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log, exp, sqrt\n",
    "import timeit\n",
    "\n",
    "# Creates 'num_vectors' random one-hot vectors of dimension 'num_classes'.\n",
    "# This creates tuple with a list of one-hot vectors, and liost with the indices they encode.\n",
    "def get_random_one_hot_data(num_classes, num_vectors):\n",
    "    indices = np.random.choice(range(num_classes), size=num_vectors, p=data_sampling_weights).reshape((1,num_vectors))\n",
    "    list_of_vectors = C.one_hot(indices, num_classes)\n",
    "    return (list_of_vectors, indices.flatten())\n",
    "\n",
    "# Create a network that:\n",
    "# * Transforms the input one hot-vectors with a constant random embedding\n",
    "# * Applies a linear decoding with parameters we want to learn\n",
    "# * \n",
    "def create_model(labels, vocab_dim, hidden_dim, softmax_sample_size):\n",
    "    embedding_data = np.random.normal(scale = sqrt(1.0/hidden_dim), size=(vocab_dim, hidden_dim)).astype(np.float32)\n",
    "    input_embedding = C.constant(shape = (vocab_dim, hidden_dim), value = embedding_data)\n",
    "    latent_vector = C.times(labels, input_embedding)\n",
    "    # Connect the latent output to (sampled/full) softmax.\n",
    "    if use_sampled_softmax:\n",
    "        sampling_weights = np.asarray(softmax_sampling_weights, dtype=np.float32)\n",
    "        softmax_input, ce, errs = cross_entropy_with_sampled_softmax(latent_vector, labels, vocab_dim, hidden_dim, softmax_sample_size, sampling_weights, use_sparse = use_sparse)\n",
    "    else:\n",
    "        softmax_input, ce, errs = cross_entropy_with_full_softmax(latent_vector, labels, vocab_dim, hidden_dim)\n",
    "\n",
    "    return softmax_input, ce, errs\n",
    "\n",
    "def train(vocab_dim, hidden_dim):\n",
    "    labels = C.input_variable(shape=vocab_dim, is_sparse = use_sparse)\n",
    "    softmax_input, cross_entropy, errs = create_model(labels, vocab_dim, hidden_dim, softmax_sample_size)\n",
    "\n",
    "    # Setup the trainer\n",
    "    lr_per_sample = C.learning_rate_schedule(learning_rate, C.UnitType.sample)\n",
    "    momentum_time_constant = C.momentum_as_time_constant_schedule(2000)\n",
    "    learner = C.momentum_sgd(softmax_input.parameters, lr_per_sample, momentum_time_constant, True)\n",
    "    trainer = C.Trainer(softmax_input, cross_entropy, errs, learner)\n",
    "\n",
    "    # Run training\n",
    "    minbatch = 0\n",
    "    average_cross_entropy = compute_average_cross_entropy(softmax_input, test_set_size, vocab_dim)\n",
    "    print(\"minbatch = %d average_cross_entropy = %.3f\\tperplexity = %.3f\"\n",
    "            % (minbatch, average_cross_entropy, exp(average_cross_entropy)))\n",
    "\n",
    "    while True:\n",
    "        minbatch += 1\n",
    "\n",
    "        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "        label_data, indices = get_random_one_hot_data(vocab_dim, minibatch_size)\n",
    "        arguments = ({labels : label_data})\n",
    "\n",
    "        t_start = timeit.default_timer()\n",
    "        trainer.train_minibatch(arguments)\n",
    "        t_end = timeit.default_timer()\n",
    "        samples_per_second = minibatch_size / (t_end - t_start)\n",
    "        if minbatch % 10 == 0:\n",
    "            average_cross_entropy = compute_average_cross_entropy(softmax_input, test_set_size, vocab_dim)\n",
    "            print(\"minbatch = %d average_cross_entropy = %.3f perplexity = %.3f samples/s = %.1f\"\n",
    "                    % (minbatch, average_cross_entropy, exp(average_cross_entropy), samples_per_second))\n",
    "            average_cross_entropy = compute_average_cross_entropy(softmax_input, test_set_size, vocab_dim)\n",
    "            print(\"minbatch = %d average_cross_entropy = %.3f perplexity = %.3f samples/s = %.1f\"\n",
    "                    % (minbatch, average_cross_entropy, exp(average_cross_entropy), samples_per_second))\n",
    "\n",
    "def compute_average_cross_entropy(softmax_input, test_set_size, vocab_dim):\n",
    "    vectors, indices = get_random_one_hot_data(vocab_dim, test_set_size)\n",
    "    total_cross_entropy = 0.0\n",
    "    arguments = (vectors)\n",
    "    z = softmax_input.eval(arguments).reshape(test_set_size, vocab_dim)\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        log_p = log_softmax(z[i], indices[i])\n",
    "        total_cross_entropy -= log_p\n",
    "\n",
    "    return total_cross_entropy / len(indices)\n",
    "\n",
    "# Computes exp(z[index])/np.sum(exp[z]) for a one-dimensional numpy array in an numerically stable way.\n",
    "def log_softmax(z,    # numpy array\n",
    "                index # index into the array\n",
    "            ):\n",
    "    max_z = np.max(z)\n",
    "    return z[index] - max_z - log(np.sum(np.exp(z - max_z)))\n",
    "\n",
    "def zipf(index):\n",
    "    return 1.0 / (index + 5.0)\n",
    "\n",
    "def entropy(p):\n",
    "    return -np.sum(np.log(p)*p)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # train the LM\n",
    "    np.random.seed(1)\n",
    "\n",
    "    softmax_sample_size = 10\n",
    "    learning_rate = 0.01\n",
    "    minibatch_size = 100\n",
    "    vocab_dim = 20\n",
    "    hidden_dim = 10\n",
    "    use_sampled_softmax = False\n",
    "    use_sparse = use_sampled_softmax\n",
    "    test_set_size = 1000\n",
    "\n",
    "\n",
    "    zipf_sampling_weights = np.asarray([ zipf(i) for i in range(vocab_dim)], dtype=np.float32)\n",
    "    data_sampling_weights = zipf_sampling_weights/np.sum(zipf_sampling_weights)\n",
    "    softmax_sampling_weights =  np.power(data_sampling_weights, 0.5)\n",
    "\n",
    "    from cntk.device import set_default_device, cpu, gpu\n",
    "\n",
    "    print(\"entropy\"+str(entropy(data_sampling_weights)))\n",
    "    train(vocab_dim, hidden_dim)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
