{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled Softmax\n",
    "\n",
    "For classification and prediction problems a typical criterion function is cross-entropy with softmax. If the number of output classes is high the computation of this criterion and the corresponding gradients could be quite costly. Sampled Softmax is a heuristic to speed up training in these cases.\n",
    "\n",
    "## Basics\n",
    "\n",
    "The softmax function is used in neural networks if we want to interpret the network output as a probability distribution over a set of classes $C$ with $|C|=N_C$.\n",
    "\n",
    "Softmax maps an $N_C$-dimensional vector $z$, which has unrestricted values, to an $N_C$ dimensional vector $p$ whith non-negative values that sum up to 1 so that they can be interpreted as probabilities. More precisely:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_i &= softmax(z, i)\\\\\n",
    "    &= \\frac{exp(z_i)}{\\sum_{k\\in C} exp(z_k)}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In what follows we assume that the input $z$ to the softmax is computed from some hidden vector $h$ of dimension $N_h$  in a specific way, namely:\n",
    "\n",
    "$$ z = W h + b $$\n",
    "\n",
    "where $W$ is a learnable weight matrix of dimension $(N_c, N_h)$ and is $b$ a learnable bias vector.\n",
    "We restrict ourself to this specific choice of $z$ because it helps in implementing an efficient sampled softmax.\n",
    "\n",
    "In a typical use-case like for example a recurrent language model, the hidden vector $h$ would be the output of the recurrent layers and $C$ would be the set of words to predict.   \n",
    "\n",
    "As a training criterion, we use cross-entropy which is a function of the expected (true) class $t\\in C$ and the probability predicted for it:\n",
    "\n",
    "$$cross\\_entropy := -log(p_t)$$\n",
    "\n",
    "## Sampled Softmax from the outside\n",
    "\n",
    "For the normal softmax the CNTK Python-api provides the function [cross_entropy_with_softmax](https://cntk.ai/pythondocs/cntk.ops.html?highlight=softmax#cntk.ops.cross_entropy_with_softmax). This takes as input the vector $N_C$-dimensional vector $z$. As mentioned for our sampled softmax implementation we assume that this z is computed by $ z = W h + b $. In sampled softmax this has to be part of the whole implementation of the criterion.\n",
    "\n",
    "Below we show the code for `cross_entropy_with_sampled_softmax_and_embedding`. Letâ€™s look at signature first.\n",
    "\n",
    "One fundamental difference to the corresponding function in the Python-api (`cross_entropy_with_softmax`) is that in the Python api function the input corresponds to $z$ and must have the same dimension as the target vector, while in cross_entropy_with_full_softmax the input corresponds to our hidden vector $h$ can have any dimension (hidden_dim).\n",
    "Actually, hidden_dim will be typically much lower than the dimension of the target vector.\n",
    "\n",
    "We also have some additional parameter `num_samples, sampling_weights, allow_duplicates` that control the random sampling. \n",
    "Another difference to the api function is that we return a tripple (z, cross_entropy_on_samples, error_on_samples).\n",
    "\n",
    "We will come back to the details of the implementation below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cntk as C\n",
    "\n",
    "# Creates a subgraph computing cross-entropy with sampled softmax.\n",
    "def cross_entropy_with_sampled_softmax_and_embedding(\n",
    "    hidden_vector,            # Node providing hidden input\n",
    "    target_vector,            # Node providing the expected labels (as sparse vectors)\n",
    "    vocab_dim,                # Vocabulary size\n",
    "    hidden_dim,               # Dimension of the hidden vector\n",
    "    num_samples,              # Number of samples to use for sampled softmax\n",
    "    sampling_weights,         # Node providing weights to be used for the weighted sampling\n",
    "    allow_duplicates = False, # Boolean flag to control whether to use sampling with replacemement \n",
    "                              # (allow_duplicates == True) or without replacement.\n",
    "    ):\n",
    "    # define the parameters leanabe parameters\n",
    "    b = C.Parameter(shape = (vocab_dim, 1), init = C.init_bias_default_or_0)\n",
    "    W = C.Parameter(shape = (vocab_dim, hidden_dim), init = C.init_default_or_glorot_uniform)\n",
    "\n",
    "    # Define the node that generates a set of random samples per minibatch\n",
    "    # Sparse matrix (num_samples * vocab_dim)\n",
    "    sample_selector = C.random_sample(sampling_weights, num_samples, allow_duplicates)\n",
    "\n",
    "    # For each of the samples we also need the probablity that it in the sampled set.\n",
    "    inclusion_probs = C.random_sample_inclusion_frequency(sampling_weights, num_samples, allow_duplicates) # dense row [1 * vocab_size]\n",
    "    log_prior = C.log(inclusion_probs) # dense row [1 * vocab_dim]\n",
    "\n",
    "    # Create a submatrix wS of 'weights\n",
    "    W_sampled = C.times(sample_selector, W) # [num_samples * hidden_dim]\n",
    "    z_sampled = C.times_transpose(W_sampled, hidden_vector) + C.times(sample_selector, b) - C.times_transpose (sample_selector, log_prior)# [num_samples]\n",
    "\n",
    "    # Getting the weight vector for the true label. Dimension hidden_dim\n",
    "    W_target = C.times(target_vector, W) # [1 * hidden_dim]\n",
    "    z_target = C.times_transpose(W_target, hidden_vector) + C.times(target_vector, b) - C.times_transpose(target_vector, log_prior) # [1]\n",
    "\n",
    "\n",
    "    z_reduced = C.reduce_log_sum(z_sampled)\n",
    "    \n",
    "    # Compute the cross entropy that is used for training.\n",
    "    # We don't check whether any of the classes in the random samples conincides with the true label, so it might\n",
    "    # happen that the true class is counted\n",
    "    # twice in the normalising demnominator of sampled softmax.\n",
    "    cross_entropy_on_samples = C.log_add_exp(z_target, z_reduced) - z_target\n",
    "\n",
    "    # For applying the model we also output a node providing the input for the full softmax\n",
    "    z = C.times_transpose(W, hidden_vector) + b\n",
    "    z = C.reshape(z, shape = (vocab_dim))\n",
    "\n",
    "    zSMax = C.reduce_max(z_sampled)\n",
    "    error_on_samples = C.less(zT, zSMax)\n",
    "    return (z, cross_entropy_on_samples, error_on_samples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a better idea of what the inputs and outputs are and how this all differs from the normal softmax we give below a corresponding function using normal softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates subgraph computing cross-entropy with (full) softmax.\n",
    "def cross_entropy_with_softmax_and_embedding(\n",
    "    hidden_vector,  # Node providing hidden input\n",
    "    target_vector,  # Node providing the expected labels (as sparse vectors)\n",
    "    vocab_dim,      # Vocabulary size\n",
    "    hidden_dim      # Dimension of the hidden vector\n",
    "    ):\n",
    "    # Setup bias and weights\n",
    "    b = C.Parameter(shape = (vocab_dim, 1), init = C.init_bias_default_or_0)\n",
    "    W = C.Parameter(shape = (vocab_dim, hidden_dim), init = C.init_default_or_glorot_uniform)\n",
    "\n",
    "    \n",
    "    z = C.reshape( C.times_transpose(W, hidden_vector) + b, (1,vocab_dim))\n",
    "    \n",
    "    # Use cross_entropy_with_softmax\n",
    "    cross_entropy = C.cross_entropy_with_softmax(z, target_vector)\n",
    "\n",
    "    zMax = C.reduce_max(z)\n",
    "    zT = C.times_transpose(z, target_vector)\n",
    "    error_on_samples = C.less(zT, zMax)\n",
    "\n",
    "    return (z, cross_entropy, error_on_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the main differences to the api function `cross_entropy_with_softmax` are:\n",
    "* We include an embedding.\n",
    "* We return a tripple (z, cross_entropy, error_on_samples) instead of just returnting the cross entropy.\n",
    "\n",
    "\n",
    "## A toy example\n",
    "\n",
    "To explain how to integrate sampled softmax let us look at a toy example. In this toy example we first transform one-hot input vectors via some random projection into a lower dimensional vector $h$. The modeling task is to reverse this mapping using (sampled) softmax. Well, as already said this is a toy example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: 2.87987\n",
      "minbatch = 0 average_cross_entropy = 3.098\tperplexity = 22.145\n",
      "minbatch = 10 average_cross_entropy = 3.095 perplexity = 22.077 samples/s = 8324.9\n",
      "minbatch = 20 average_cross_entropy = 3.024 perplexity = 20.575 samples/s = 8323.2\n",
      "minbatch = 30 average_cross_entropy = 2.941 perplexity = 18.944 samples/s = 8335.6\n",
      "minbatch = 40 average_cross_entropy = 3.059 perplexity = 21.303 samples/s = 8096.3\n",
      "minbatch = 50 average_cross_entropy = 2.997 perplexity = 20.030 samples/s = 8312.5\n",
      "minbatch = 60 average_cross_entropy = 2.967 perplexity = 19.429 samples/s = 8297.5\n",
      "minbatch = 70 average_cross_entropy = 2.989 perplexity = 19.862 samples/s = 8327.9\n",
      "minbatch = 80 average_cross_entropy = 2.957 perplexity = 19.242 samples/s = 8329.4\n",
      "minbatch = 90 average_cross_entropy = 2.918 perplexity = 18.499 samples/s = 8200.8\n",
      "minbatch = 100 average_cross_entropy = 2.948 perplexity = 19.072 samples/s = 8300.1\n",
      "minbatch = 110 average_cross_entropy = 2.933 perplexity = 18.777 samples/s = 8227.4\n",
      "minbatch = 120 average_cross_entropy = 3.075 perplexity = 21.643 samples/s = 8079.8\n",
      "minbatch = 130 average_cross_entropy = 2.993 perplexity = 19.948 samples/s = 7696.5\n",
      "minbatch = 140 average_cross_entropy = 2.932 perplexity = 18.758 samples/s = 8264.0\n",
      "minbatch = 150 average_cross_entropy = 2.954 perplexity = 19.190 samples/s = 7861.6\n",
      "minbatch = 160 average_cross_entropy = 2.992 perplexity = 19.920 samples/s = 8114.0\n",
      "minbatch = 170 average_cross_entropy = 3.013 perplexity = 20.339 samples/s = 8034.3\n",
      "minbatch = 180 average_cross_entropy = 2.865 perplexity = 17.548 samples/s = 7489.4\n",
      "minbatch = 190 average_cross_entropy = 3.012 perplexity = 20.324 samples/s = 8076.7\n",
      "minbatch = 200 average_cross_entropy = 2.862 perplexity = 17.504 samples/s = 8057.8\n",
      "minbatch = 210 average_cross_entropy = 2.971 perplexity = 19.512 samples/s = 8167.6\n",
      "minbatch = 220 average_cross_entropy = 2.810 perplexity = 16.610 samples/s = 8298.6\n",
      "minbatch = 230 average_cross_entropy = 2.914 perplexity = 18.432 samples/s = 7992.0\n",
      "minbatch = 240 average_cross_entropy = 2.834 perplexity = 17.010 samples/s = 8233.5\n",
      "minbatch = 250 average_cross_entropy = 2.905 perplexity = 18.261 samples/s = 8227.4\n",
      "minbatch = 260 average_cross_entropy = 2.901 perplexity = 18.193 samples/s = 7523.1\n",
      "minbatch = 270 average_cross_entropy = 2.893 perplexity = 18.038 samples/s = 8013.9\n",
      "minbatch = 280 average_cross_entropy = 2.794 perplexity = 16.344 samples/s = 8217.8\n",
      "minbatch = 290 average_cross_entropy = 2.883 perplexity = 17.860 samples/s = 8227.1\n",
      "minbatch = 300 average_cross_entropy = 2.930 perplexity = 18.728 samples/s = 8265.5\n",
      "minbatch = 310 average_cross_entropy = 2.831 perplexity = 16.970 samples/s = 7895.9\n",
      "minbatch = 320 average_cross_entropy = 2.865 perplexity = 17.550 samples/s = 8033.7\n",
      "minbatch = 330 average_cross_entropy = 2.977 perplexity = 19.629 samples/s = 7933.1\n",
      "minbatch = 340 average_cross_entropy = 2.707 perplexity = 14.985 samples/s = 8161.6\n",
      "minbatch = 350 average_cross_entropy = 2.838 perplexity = 17.090 samples/s = 8277.5\n",
      "minbatch = 360 average_cross_entropy = 2.884 perplexity = 17.887 samples/s = 8278.7\n",
      "minbatch = 370 average_cross_entropy = 2.864 perplexity = 17.528 samples/s = 8294.5\n",
      "minbatch = 380 average_cross_entropy = 2.762 perplexity = 15.827 samples/s = 8053.6\n",
      "minbatch = 390 average_cross_entropy = 2.923 perplexity = 18.598 samples/s = 7639.9\n",
      "minbatch = 400 average_cross_entropy = 2.791 perplexity = 16.291 samples/s = 7963.9\n",
      "minbatch = 410 average_cross_entropy = 2.766 perplexity = 15.898 samples/s = 7658.9\n",
      "minbatch = 420 average_cross_entropy = 2.785 perplexity = 16.192 samples/s = 8265.5\n",
      "minbatch = 430 average_cross_entropy = 2.939 perplexity = 18.896 samples/s = 8127.0\n",
      "minbatch = 440 average_cross_entropy = 2.705 perplexity = 14.953 samples/s = 8175.9\n",
      "minbatch = 450 average_cross_entropy = 2.711 perplexity = 15.049 samples/s = 7865.9\n",
      "minbatch = 460 average_cross_entropy = 2.743 perplexity = 15.528 samples/s = 8014.7\n",
      "minbatch = 470 average_cross_entropy = 2.802 perplexity = 16.475 samples/s = 8056.1\n",
      "minbatch = 480 average_cross_entropy = 2.699 perplexity = 14.869 samples/s = 8115.4\n",
      "minbatch = 490 average_cross_entropy = 2.843 perplexity = 17.163 samples/s = 8124.2\n",
      "minbatch = 500 average_cross_entropy = 2.712 perplexity = 15.064 samples/s = 8312.8\n",
      "minbatch = 510 average_cross_entropy = 2.670 perplexity = 14.443 samples/s = 8211.2\n",
      "minbatch = 520 average_cross_entropy = 2.726 perplexity = 15.266 samples/s = 8324.1\n",
      "minbatch = 530 average_cross_entropy = 2.805 perplexity = 16.523 samples/s = 8319.3\n",
      "minbatch = 540 average_cross_entropy = 2.763 perplexity = 15.854 samples/s = 8333.2\n",
      "minbatch = 550 average_cross_entropy = 2.729 perplexity = 15.318 samples/s = 8123.9\n",
      "minbatch = 560 average_cross_entropy = 2.763 perplexity = 15.844 samples/s = 7430.1\n",
      "minbatch = 570 average_cross_entropy = 2.745 perplexity = 15.557 samples/s = 8286.9\n",
      "minbatch = 580 average_cross_entropy = 2.821 perplexity = 16.789 samples/s = 8323.8\n",
      "minbatch = 590 average_cross_entropy = 2.587 perplexity = 13.287 samples/s = 8264.6\n",
      "minbatch = 600 average_cross_entropy = 2.704 perplexity = 14.937 samples/s = 8286.6\n",
      "minbatch = 610 average_cross_entropy = 2.553 perplexity = 12.841 samples/s = 8256.7\n",
      "minbatch = 620 average_cross_entropy = 2.780 perplexity = 16.123 samples/s = 8057.3\n",
      "minbatch = 630 average_cross_entropy = 2.680 perplexity = 14.583 samples/s = 8273.7\n",
      "minbatch = 640 average_cross_entropy = 2.703 perplexity = 14.929 samples/s = 8260.2\n",
      "minbatch = 650 average_cross_entropy = 2.682 perplexity = 14.617 samples/s = 8150.8\n",
      "minbatch = 660 average_cross_entropy = 2.596 perplexity = 13.413 samples/s = 8270.8\n",
      "minbatch = 670 average_cross_entropy = 2.765 perplexity = 15.886 samples/s = 8428.8\n",
      "minbatch = 680 average_cross_entropy = 2.498 perplexity = 12.159 samples/s = 8347.5\n",
      "minbatch = 690 average_cross_entropy = 2.587 perplexity = 13.285 samples/s = 8304.8\n",
      "minbatch = 700 average_cross_entropy = 2.637 perplexity = 13.971 samples/s = 8287.5\n",
      "minbatch = 710 average_cross_entropy = 2.651 perplexity = 14.163 samples/s = 8077.9\n",
      "minbatch = 720 average_cross_entropy = 2.468 perplexity = 11.801 samples/s = 8334.4\n",
      "minbatch = 730 average_cross_entropy = 2.685 perplexity = 14.652 samples/s = 7993.6\n",
      "minbatch = 740 average_cross_entropy = 2.667 perplexity = 14.391 samples/s = 8000.2\n",
      "minbatch = 750 average_cross_entropy = 2.478 perplexity = 11.923 samples/s = 8318.1\n",
      "minbatch = 760 average_cross_entropy = 2.504 perplexity = 12.237 samples/s = 7911.1\n",
      "minbatch = 770 average_cross_entropy = 2.549 perplexity = 12.795 samples/s = 8249.4\n",
      "minbatch = 780 average_cross_entropy = 2.634 perplexity = 13.936 samples/s = 8279.0\n",
      "minbatch = 790 average_cross_entropy = 2.515 perplexity = 12.361 samples/s = 8313.4\n",
      "minbatch = 800 average_cross_entropy = 2.660 perplexity = 14.296 samples/s = 7904.7\n",
      "minbatch = 810 average_cross_entropy = 2.628 perplexity = 13.842 samples/s = 8321.1\n",
      "minbatch = 820 average_cross_entropy = 2.689 perplexity = 14.710 samples/s = 8320.5\n",
      "minbatch = 830 average_cross_entropy = 2.595 perplexity = 13.394 samples/s = 8246.2\n",
      "minbatch = 840 average_cross_entropy = 2.607 perplexity = 13.561 samples/s = 8271.9\n",
      "minbatch = 850 average_cross_entropy = 2.569 perplexity = 13.057 samples/s = 8286.9\n",
      "minbatch = 860 average_cross_entropy = 2.622 perplexity = 13.762 samples/s = 8276.3\n",
      "minbatch = 870 average_cross_entropy = 2.627 perplexity = 13.834 samples/s = 8324.1\n",
      "minbatch = 880 average_cross_entropy = 2.594 perplexity = 13.382 samples/s = 8357.7\n",
      "minbatch = 890 average_cross_entropy = 2.416 perplexity = 11.197 samples/s = 8316.1\n",
      "minbatch = 900 average_cross_entropy = 2.494 perplexity = 12.107 samples/s = 8136.3\n",
      "minbatch = 910 average_cross_entropy = 2.520 perplexity = 12.434 samples/s = 8270.8\n",
      "minbatch = 920 average_cross_entropy = 2.523 perplexity = 12.467 samples/s = 8205.7\n",
      "minbatch = 930 average_cross_entropy = 2.469 perplexity = 11.805 samples/s = 8296.3\n",
      "minbatch = 940 average_cross_entropy = 2.642 perplexity = 14.035 samples/s = 8189.6\n",
      "minbatch = 950 average_cross_entropy = 2.450 perplexity = 11.591 samples/s = 8327.9\n",
      "minbatch = 960 average_cross_entropy = 2.610 perplexity = 13.598 samples/s = 8327.0\n",
      "minbatch = 970 average_cross_entropy = 2.485 perplexity = 12.005 samples/s = 8293.9\n",
      "minbatch = 980 average_cross_entropy = 2.614 perplexity = 13.652 samples/s = 8077.9\n",
      "minbatch = 990 average_cross_entropy = 2.460 perplexity = 11.703 samples/s = 8324.1\n",
      "minbatch = 1000 average_cross_entropy = 2.540 perplexity = 12.677 samples/s = 7856.1\n",
      "minbatch = 1010 average_cross_entropy = 2.601 perplexity = 13.476 samples/s = 8080.6\n",
      "minbatch = 1020 average_cross_entropy = 2.526 perplexity = 12.503 samples/s = 8067.0\n",
      "minbatch = 1030 average_cross_entropy = 2.430 perplexity = 11.355 samples/s = 8299.2\n",
      "minbatch = 1040 average_cross_entropy = 2.560 perplexity = 12.936 samples/s = 8210.0\n",
      "minbatch = 1050 average_cross_entropy = 2.568 perplexity = 13.044 samples/s = 7714.8\n",
      "minbatch = 1060 average_cross_entropy = 2.422 perplexity = 11.264 samples/s = 7828.2\n",
      "minbatch = 1070 average_cross_entropy = 2.586 perplexity = 13.280 samples/s = 8313.4\n",
      "minbatch = 1080 average_cross_entropy = 2.515 perplexity = 12.368 samples/s = 8266.1\n",
      "minbatch = 1090 average_cross_entropy = 2.482 perplexity = 11.970 samples/s = 8016.9\n",
      "minbatch = 1100 average_cross_entropy = 2.439 perplexity = 11.459 samples/s = 7998.0\n",
      "minbatch = 1110 average_cross_entropy = 2.459 perplexity = 11.694 samples/s = 8205.1\n",
      "minbatch = 1120 average_cross_entropy = 2.559 perplexity = 12.923 samples/s = 8263.4\n",
      "minbatch = 1130 average_cross_entropy = 2.506 perplexity = 12.256 samples/s = 8250.0\n",
      "minbatch = 1140 average_cross_entropy = 2.523 perplexity = 12.464 samples/s = 8038.1\n",
      "minbatch = 1150 average_cross_entropy = 2.455 perplexity = 11.647 samples/s = 8023.8\n",
      "minbatch = 1160 average_cross_entropy = 2.505 perplexity = 12.245 samples/s = 7751.6\n",
      "minbatch = 1170 average_cross_entropy = 2.476 perplexity = 11.891 samples/s = 7651.4\n",
      "minbatch = 1180 average_cross_entropy = 2.320 perplexity = 10.176 samples/s = 7339.9\n",
      "minbatch = 1190 average_cross_entropy = 2.438 perplexity = 11.451 samples/s = 8229.1\n",
      "minbatch = 1200 average_cross_entropy = 2.479 perplexity = 11.935 samples/s = 8322.0\n",
      "minbatch = 1210 average_cross_entropy = 2.349 perplexity = 10.471 samples/s = 8147.7\n",
      "minbatch = 1220 average_cross_entropy = 2.547 perplexity = 12.769 samples/s = 8301.3\n",
      "minbatch = 1230 average_cross_entropy = 2.436 perplexity = 11.427 samples/s = 8298.9\n",
      "minbatch = 1240 average_cross_entropy = 2.425 perplexity = 11.298 samples/s = 8086.5\n",
      "minbatch = 1250 average_cross_entropy = 2.574 perplexity = 13.116 samples/s = 8161.0\n",
      "minbatch = 1260 average_cross_entropy = 2.357 perplexity = 10.564 samples/s = 8252.1\n",
      "minbatch = 1270 average_cross_entropy = 2.508 perplexity = 12.276 samples/s = 8151.6\n",
      "minbatch = 1280 average_cross_entropy = 2.534 perplexity = 12.598 samples/s = 8276.3\n",
      "minbatch = 1290 average_cross_entropy = 2.449 perplexity = 11.579 samples/s = 7280.4\n",
      "minbatch = 1300 average_cross_entropy = 2.516 perplexity = 12.383 samples/s = 8148.0\n",
      "minbatch = 1310 average_cross_entropy = 2.417 perplexity = 11.217 samples/s = 8198.5\n",
      "minbatch = 1320 average_cross_entropy = 2.506 perplexity = 12.260 samples/s = 8296.9\n",
      "minbatch = 1330 average_cross_entropy = 2.400 perplexity = 11.020 samples/s = 7207.7\n",
      "minbatch = 1340 average_cross_entropy = 2.279 perplexity = 9.763 samples/s = 8256.1\n",
      "minbatch = 1350 average_cross_entropy = 2.465 perplexity = 11.758 samples/s = 8122.2\n",
      "minbatch = 1360 average_cross_entropy = 2.395 perplexity = 10.971 samples/s = 8263.7\n",
      "minbatch = 1370 average_cross_entropy = 2.409 perplexity = 11.126 samples/s = 8126.4\n",
      "minbatch = 1380 average_cross_entropy = 2.377 perplexity = 10.777 samples/s = 8234.0\n",
      "minbatch = 1390 average_cross_entropy = 2.390 perplexity = 10.909 samples/s = 8187.3\n",
      "minbatch = 1400 average_cross_entropy = 2.392 perplexity = 10.935 samples/s = 8320.2\n",
      "minbatch = 1410 average_cross_entropy = 2.440 perplexity = 11.472 samples/s = 8277.2\n",
      "minbatch = 1420 average_cross_entropy = 2.396 perplexity = 10.981 samples/s = 8351.1\n",
      "minbatch = 1430 average_cross_entropy = 2.321 perplexity = 10.186 samples/s = 8255.6\n",
      "minbatch = 1440 average_cross_entropy = 2.414 perplexity = 11.177 samples/s = 8309.8\n",
      "minbatch = 1450 average_cross_entropy = 2.401 perplexity = 11.038 samples/s = 8241.3\n",
      "minbatch = 1460 average_cross_entropy = 2.374 perplexity = 10.740 samples/s = 8331.2\n",
      "minbatch = 1470 average_cross_entropy = 2.396 perplexity = 10.977 samples/s = 7994.2\n",
      "minbatch = 1480 average_cross_entropy = 2.420 perplexity = 11.248 samples/s = 8073.4\n",
      "minbatch = 1490 average_cross_entropy = 2.309 perplexity = 10.066 samples/s = 8224.5\n",
      "minbatch = 1500 average_cross_entropy = 2.280 perplexity = 9.780 samples/s = 8125.3\n",
      "minbatch = 1510 average_cross_entropy = 2.342 perplexity = 10.402 samples/s = 8209.5\n",
      "minbatch = 1520 average_cross_entropy = 2.341 perplexity = 10.397 samples/s = 8325.5\n",
      "minbatch = 1530 average_cross_entropy = 2.384 perplexity = 10.846 samples/s = 8262.3\n",
      "minbatch = 1540 average_cross_entropy = 2.450 perplexity = 11.586 samples/s = 8305.4\n",
      "minbatch = 1550 average_cross_entropy = 2.337 perplexity = 10.353 samples/s = 8287.2\n",
      "minbatch = 1560 average_cross_entropy = 2.429 perplexity = 11.348 samples/s = 8401.8\n",
      "minbatch = 1570 average_cross_entropy = 2.387 perplexity = 10.879 samples/s = 8254.4\n",
      "minbatch = 1580 average_cross_entropy = 2.441 perplexity = 11.479 samples/s = 8308.7\n",
      "minbatch = 1590 average_cross_entropy = 2.356 perplexity = 10.550 samples/s = 8309.0\n",
      "minbatch = 1600 average_cross_entropy = 2.316 perplexity = 10.130 samples/s = 8219.0\n",
      "minbatch = 1610 average_cross_entropy = 2.263 perplexity = 9.611 samples/s = 8245.1\n",
      "minbatch = 1620 average_cross_entropy = 2.232 perplexity = 9.322 samples/s = 8277.8\n",
      "minbatch = 1630 average_cross_entropy = 2.380 perplexity = 10.807 samples/s = 8286.9\n",
      "minbatch = 1640 average_cross_entropy = 2.228 perplexity = 9.281 samples/s = 8273.1\n",
      "minbatch = 1650 average_cross_entropy = 2.312 perplexity = 10.099 samples/s = 7870.4\n",
      "minbatch = 1660 average_cross_entropy = 2.255 perplexity = 9.534 samples/s = 8242.2\n",
      "minbatch = 1670 average_cross_entropy = 2.236 perplexity = 9.353 samples/s = 7829.0\n",
      "minbatch = 1680 average_cross_entropy = 2.366 perplexity = 10.657 samples/s = 8318.1\n",
      "minbatch = 1690 average_cross_entropy = 2.300 perplexity = 9.972 samples/s = 8320.8\n",
      "minbatch = 1700 average_cross_entropy = 2.252 perplexity = 9.508 samples/s = 8403.9\n",
      "minbatch = 1710 average_cross_entropy = 2.184 perplexity = 8.881 samples/s = 7867.2\n",
      "minbatch = 1720 average_cross_entropy = 2.287 perplexity = 9.845 samples/s = 8265.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d01e55b95899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"entropy: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_sampling_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-d01e55b95899>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(vocab_dim, hidden_dim)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mt_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mt_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msamples_per_second\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminibatch_size\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt_end\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\src\\cntk_a\\bindings\\python\\cntk\\trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, arguments, outputs, device)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments,\n\u001b[0;32m--> 111\u001b[0;31m                     device)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from math import log, exp, sqrt\n",
    "import timeit\n",
    "\n",
    "# Creates random one-hot vectors of dimension 'num_classes'.\n",
    "# Returns a tuple with a list of one-hot vectors, and list with the indices they encode.\n",
    "def get_random_one_hot_data(num_classes, num_vectors):\n",
    "    indices = np.random.choice(range(num_classes), size=num_vectors, p=data_sampling_weights).reshape((1,num_vectors))\n",
    "    list_of_vectors = C.one_hot(indices, num_classes)\n",
    "    return (list_of_vectors, indices.flatten())\n",
    "\n",
    "# Create a network that:\n",
    "# * Transforms the input one hot-vectors with a constant random embedding\n",
    "# * Applies a linear decoding with parameters we want to learn\n",
    "def create_model(labels, vocab_dim, hidden_dim, softmax_sample_size):\n",
    "    # random projection matrix\n",
    "    random_data = np.random.normal(scale = sqrt(1.0/hidden_dim), size=(vocab_dim, hidden_dim)).astype(np.float32)\n",
    "    random_matrix = C.constant(shape = (vocab_dim, hidden_dim), value = random_data)\n",
    "    \n",
    "    h = C.times(labels, random_matrix)\n",
    "    # Connect the latent output to (sampled/full) softmax.\n",
    "    if use_sampled_softmax:\n",
    "        sampling_weights = np.asarray(softmax_sampling_weights, dtype=np.float32)\n",
    "        softmax_input, ce, errs = cross_entropy_with_sampled_softmax_and_embedding(h, labels, vocab_dim, hidden_dim, softmax_sample_size, sampling_weights, use_sparse = use_sparse)\n",
    "    else:\n",
    "        softmax_input, ce, errs = cross_entropy_with_softmax_and_embedding(h, labels, vocab_dim, hidden_dim)\n",
    "\n",
    "    return softmax_input, ce, errs\n",
    "\n",
    "def train(vocab_dim, hidden_dim):\n",
    "    labels = C.input_variable(shape=vocab_dim, is_sparse = use_sparse)\n",
    "    softmax_input, cross_entropy, errs = create_model(labels, vocab_dim, hidden_dim, softmax_sample_size)\n",
    "\n",
    "    # Setup the trainer\n",
    "    lr_per_sample = C.learning_rate_schedule(learning_rate, C.UnitType.sample)\n",
    "    momentum_time_constant = C.momentum_as_time_constant_schedule(2000)\n",
    "    learner = C.momentum_sgd(softmax_input.parameters, lr_per_sample, momentum_time_constant, True)\n",
    "    trainer = C.Trainer(softmax_input, cross_entropy, errs, learner)\n",
    "\n",
    "    # Run training\n",
    "    minbatch = 0\n",
    "    average_cross_entropy = compute_average_cross_entropy(softmax_input, test_set_size, vocab_dim)\n",
    "    print(\"minbatch = %d average_cross_entropy = %.3f\\tperplexity = %.3f\"\n",
    "            % (minbatch, average_cross_entropy, exp(average_cross_entropy)))\n",
    "\n",
    "    while True:\n",
    "        minbatch += 1\n",
    "\n",
    "        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "        label_data, indices = get_random_one_hot_data(vocab_dim, minibatch_size)\n",
    "        arguments = ({labels : label_data})\n",
    "\n",
    "        t_start = timeit.default_timer()\n",
    "        trainer.train_minibatch(arguments)\n",
    "        t_end = timeit.default_timer()\n",
    "        samples_per_second = minibatch_size / (t_end - t_start)\n",
    "        if minbatch % 10 == 0:\n",
    "            average_cross_entropy = compute_average_cross_entropy(softmax_input, test_set_size, vocab_dim)\n",
    "            print(\"minbatch = %d average_cross_entropy = %.3f perplexity = %.3f samples/s = %.1f\"\n",
    "                    % (minbatch, average_cross_entropy, exp(average_cross_entropy), samples_per_second))\n",
    "\n",
    "def compute_average_cross_entropy(softmax_input, test_set_size, vocab_dim):\n",
    "    vectors, indices = get_random_one_hot_data(vocab_dim, test_set_size)\n",
    "    total_cross_entropy = 0.0\n",
    "    arguments = (vectors)\n",
    "    z = softmax_input.eval(arguments).reshape(test_set_size, vocab_dim)\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        log_p = log_softmax(z[i], indices[i])\n",
    "        total_cross_entropy -= log_p\n",
    "\n",
    "    return total_cross_entropy / len(indices)\n",
    "\n",
    "# Computes exp(z[index])/np.sum(exp[z]) for a one-dimensional numpy array in an numerically stable way.\n",
    "def log_softmax(z,    # numpy array\n",
    "                index # index into the array\n",
    "            ):\n",
    "    max_z = np.max(z)\n",
    "    return z[index] - max_z - log(np.sum(np.exp(z - max_z)))\n",
    "\n",
    "def zipf(index):\n",
    "    return 1.0 / (index + 5.0)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # train the LM\n",
    "    np.random.seed(1)\n",
    "\n",
    "    softmax_sample_size = 10\n",
    "    learning_rate = 0.01\n",
    "    minibatch_size = 100\n",
    "    vocab_dim = 20\n",
    "    hidden_dim = 10\n",
    "    use_sampled_softmax = False\n",
    "    use_sparse = use_sampled_softmax\n",
    "    test_set_size = 100\n",
    "\n",
    "\n",
    "    zipf_sampling_weights = np.asarray([ zipf(i) for i in range(vocab_dim)], dtype=np.float32)\n",
    "    data_sampling_distribution = zipf_sampling_weights/np.sum(zipf_sampling_weights)\n",
    "    softmax_sampling_weights =  np.power(data_sampling_weights, 0.5)\n",
    "\n",
    "    from cntk.device import set_default_device, cpu, gpu\n",
    "\n",
    "    train(vocab_dim, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return -np.sum(np.log(p)*p)\n",
    "\n",
    "print(\"entropy: \"+str(entropy(data_sampling_distribution)))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
