{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CNTK 105: Basic autoencoder with MNIST data\n",
    "\n",
    "**Prerequisites**: We assume that you have successfully downloaded the MNIST data by completing the tutorial titled CNTK_103A_MNIST_DataLoader.ipynb.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial we introduce you to the basics of [Autoencoders](https://en.wikipedia.org/wiki/Autoencoder). An autoencoder is an artificial neural network used for unsupervised learning of efficient encodings. In other words, they are used for lossy data-specific compression that is learnt automatically instead of relying on human engineered features. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. \n",
    "\n",
    "The autoencoders are very specific to the data-set on hand and are different from standard codecs such as JPEG, MPEG standard based encodings. Once the information is encoded and decoded back to original dimensions some amount of information is lost in the process. Given these encodings are specific to data, autoencoders are not used for compression. However, there are two areas where autoencoders have been found very effective in denoising and  dimensionality reduction.\n",
    "\n",
    "Autoencoders have attracted attention since they have long been thought to be a potential approach for unsupervised learning. Truly unsupervised approaches involve learning useful representations without the need for labels. Autoencoders fall under self-supervised learning, a specific instance of supervised learning where the targets are generated from the input data. \n",
    "\n",
    "**Goal** \n",
    "\n",
    "Our goal is to train an autoencoder that compresses MNIST digits image to a vector of smaller dimension and then restores the image. The MNIST data comprises of hand-written digits with little background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cntk.ai/jup/MNIST-image.jpg\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 1\n",
    "Image(url=\"http://cntk.ai/jup/MNIST-image.jpg\", width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this tutorial, we will use the [MNIST hand-written digits data](https://en.wikipedia.org/wiki/MNIST_database) to illustrate encoding the images and decoding (restoring) them using feed-forward networks. We will visualize the original and the restored images. We illustrate feed forward network based both simple autoencoder and deep autoencoder. More advanced autoencoders will be covered in future 200 series tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the relevant modules\n",
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import CNTK \n",
    "import cntk as C\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the block below, we check if we are running this notebook in the CNTK internal test machines by looking for environment variables defined there. We then select the right target device (GPU vs CPU) to test this notebook. In other cases, we use CNTK's default policy to use the best available device (GPU, if available, else CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "C.device.try_set_default_device(C.device.gpu(0))\n",
    "# Select the right target device when this notebook is being tested:\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        C.device.try_set_default_device(C.device.cpu())\n",
    "    else:\n",
    "        C.device.try_set_default_device(C.device.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are two run modes:\n",
    "- *Fast mode*: `isFast` is set to `True`. This is the default mode for the notebooks, which means we train for fewer iterations or train / test on limited data. This ensures functional correctness of the notebook though the models produced are far from what a completed training would produce.\n",
    "\n",
    "- *Slow mode*: We recommend the user to set this flag to `False` once the user has gained familiarity with the notebook content and wants to gain insight from running the notebooks for a longer period with different parameters for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "isFast = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data reading\n",
    "\n",
    "In this section, we will read the data generated in CNTK 103 Part A.\n",
    "\n",
    "The data is in the following format:\n",
    "\n",
    "    |labels 0 0 0 0 0 0 0 1 0 0 |features 0 0 0 0 ... \n",
    "                                                  (784 integers each representing a pixel)\n",
    "    \n",
    " In this tutorial we are going to use the image pixels corresponding the integer stream named \"features\". We define a `create_reader` function to read the training and test data using the [CTF deserializer](https://cntk.ai/pythondocs/cntk.io.html?highlight=ctfdeserializer#cntk.io.CTFDeserializer). The labels are [1-hot encoded](https://en.wikipedia.org/wiki/One-hot). We ignore them in this tutorial. \n",
    "\n",
    "We also check if the training and test data file has been downloaded and available for reading by the `create_reader` function. In this tutorial we are using the MNIST data you have downloaded using CNTK_103A_MNIST_DataLoader notebook. The dataset has 60,000 training images and 10,000 test images with each image being 28 x 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels_viz = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "        features   = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory is data\\MNIST\n"
     ]
    }
   ],
   "source": [
    "# Ensure the training and test data is generated and available for this tutorial.\n",
    "# We search in two locations in the toolkit for the cached MNIST data set.\n",
    "data_found = False\n",
    "for data_dir in [os.path.join(\"..\", \"Examples\", \"Image\", \"DataSets\", \"MNIST\"),\n",
    "                 os.path.join(\"data\", \"MNIST\")]:\n",
    "    train_file = os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\n",
    "    test_file = os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\n",
    "    if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "        data_found = True\n",
    "        break\n",
    "        \n",
    "if not data_found:\n",
    "    raise ValueError(\"Please generate the data by completing CNTK 103 Part A\")\n",
    "print(\"Data directory is {0}\".format(data_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='#Model Creation'></a>\n",
    "## Model Creation\n",
    "\n",
    "We start with a simple single fully-connected feedforward network as encoder and as decoder (as shown in the figure below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cntk.ai/jup/SimpleAEfig.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 2\n",
    "Image(url=\"http://cntk.ai/jup/SimpleAEfig.jpg\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The input data are a set of hand written digits images each 28 x28 pixels. In this tutorial, we will consider each image as a linear array of 784 pixel values. These pixels are considered as an input having 784 dimensions, one per pixel. Since the goal of the autoencoder is to compress the data and reconstruct the original image, the output dimension is same as the input dimension. We will compress the input to mere 32 dimensions (referred to as the `encoding_dim`). Additionally, since the maximum input value is 255, we normalize the input between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "encoding_dim = 32\n",
    "output_dim = input_dim\n",
    "\n",
    "def create_model(features):\n",
    "    with C.layers.default_options(init = C.glorot_uniform()):\n",
    "        # We scale the input pixels to 0-1 range\n",
    "        encode = C.layers.Dense(encoding_dim, activation = C.relu)(features/255.0)\n",
    "        decode = C.layers.Dense(input_dim, activation = C.sigmoid)(encode)\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup the network for training and testing\n",
    "\n",
    "In previous tutorials, we have defined each of the training and testing phases separately. In this tutorial, we combine the two componets in one place such that this template could be used as a recipe for your usage.  \n",
    "\n",
    "The `train_and_test` function performs two major tasks:\n",
    "- Train the model\n",
    "- Evaluate the accuracy of the model on test data\n",
    "\n",
    "For training:\n",
    "\n",
    "> The function takes a reader (`reader_train`), a model function (`model_func`) and the target (a.k.a `label`) as input. In this tutorial, we show how to create and pass your **own** loss function. We normalize the `label` function to emit value between 0 and 1 for us to compute the label error using `C.classification_error` function.\n",
    "\n",
    "> We use Adam optimizer in this tutorial from a range of [learners](https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner) (optimizers) available in the toolkit.  \n",
    "\n",
    "For testing:\n",
    "\n",
    "> The function additionally takes a reader  (`reader_test`) and evaluates the predicted pixel values made by the model against reference data, in this case the original pixel values for each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_and_test(reader_train, reader_test, model_func):\n",
    "    \n",
    "    ###############################################\n",
    "    # Training the model\n",
    "    ###############################################\n",
    "    \n",
    "    # Instantiate the input and the label variables\n",
    "    input = C.input(input_dim)\n",
    "    label = C.input(input_dim)\n",
    "    \n",
    "    # Create the model function\n",
    "    model = model_func(input)\n",
    "    \n",
    "    # The labels for this network is same as the input MNIST image.\n",
    "    # Note: Inside the model we are scaling the input to 0-1 range\n",
    "    # Hence we rescale the label to the same range\n",
    "    # We show how one can use their custom loss function\n",
    "    # loss = -(y* log(p)+ (1-y) * log(1-p)) where p = model output and y = target\n",
    "    # We have normalized the input between 0-1. Hence we scale the target to same range\n",
    "    \n",
    "    target = label/255.0 \n",
    "    loss = -(target * C.log(model) + (1 - target) * C.log(1 - model))\n",
    "    label_error  = C.classification_error(model, target)\n",
    "    \n",
    "    # training config\n",
    "    epoch_size = 30000        # 30000 samples is half the dataset size \n",
    "    minibatch_size = 64\n",
    "    num_sweeps_to_train_with = 5 if isFast else 100\n",
    "    num_samples_per_sweep = 60000\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) // minibatch_size\n",
    " \n",
    "    \n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    lr_per_sample = [0.00003]\n",
    "    lr_schedule = C.learning_rate_schedule(lr_per_sample, C.UnitType.sample, epoch_size)\n",
    "    \n",
    "    # Momentum\n",
    "    momentum_as_time_constant = C.momentum_as_time_constant_schedule(700)\n",
    "    \n",
    "    # We use a variant of the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from \n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.fsadagrad(model.parameters,\n",
    "                         lr=lr_schedule, momentum=momentum_as_time_constant) \n",
    "    \n",
    "    # Instantiate the trainer\n",
    "    progress_printer = C.logging.ProgressPrinter(0)\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "    \n",
    "    # Map the data streams to the input and labels.\n",
    "    # Note: for autoencoders input == label\n",
    "    input_map = {\n",
    "        input  : reader_train.streams.features,\n",
    "        label  : reader_train.streams.features\n",
    "    } \n",
    "    \n",
    "    aggregate_metric = 0\n",
    "    for i in range(num_minibatches_to_train):\n",
    "        # Read a mini batch from the training data file\n",
    "        data = reader_train.next_minibatch(minibatch_size, input_map = input_map)\n",
    "        \n",
    "        # Run the trainer on and perform model training\n",
    "        trainer.train_minibatch(data)\n",
    "        samples = trainer.previous_minibatch_sample_count\n",
    "        aggregate_metric += trainer.previous_minibatch_evaluation_average * samples\n",
    "        \n",
    "    train_error = (aggregate_metric*100.0) / (trainer.total_number_of_samples_seen)\n",
    "    print(\"Average training error: {0:0.2f}%\".format(train_error))\n",
    "        \n",
    "    #############################################################################\n",
    "    # Testing the model\n",
    "    # Note: we use a test file reader to read data different from a training data\n",
    "    #############################################################################\n",
    "        \n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 32\n",
    "    num_samples = 10000\n",
    "    num_minibatches_to_test = num_samples / test_minibatch_size\n",
    "    test_result = 0.0\n",
    "    \n",
    "    # Test error metric calculation\n",
    "    metric_numer    = 0\n",
    "    metric_denom    = 0\n",
    "\n",
    "    test_input_map = {\n",
    "        input  : reader_test.streams.features,\n",
    "        label  : reader_test.streams.features\n",
    "    }\n",
    "\n",
    "    for i in range(0, int(num_minibatches_to_test)):\n",
    "        \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "        # with one pixel per dimension that we will encode / decode with the \n",
    "        # trained model.\n",
    "        data = reader_test.next_minibatch(test_minibatch_size,\n",
    "                                       input_map = test_input_map)\n",
    "\n",
    "        # Specify the mapping of input variables in the model to actual\n",
    "        # minibatch data to be tested with\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        \n",
    "        # minibatch data to be trained with\n",
    "        metric_numer += np.abs(eval_error * test_minibatch_size)\n",
    "        metric_denom += test_minibatch_size\n",
    "\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    test_error = (metric_numer*100.0) / (metric_denom) \n",
    "    print(\"Average test error: {0:0.2f}%\".format(test_error))\n",
    "    \n",
    "    return model, train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us train the simple autoencoder. We create a training and a test reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per sample: 3e-05\n",
      "      544        544      0.947      0.947            64\n",
      "      544        544      0.931      0.923           192\n",
      "      543        543      0.921      0.913           448\n",
      "      542        541      0.924      0.927           960\n",
      "      537        532      0.924      0.924          1984\n",
      "      493        451      0.821      0.721          4032\n",
      "      383        275      0.639       0.46          8128\n",
      "      303        223      0.524      0.409         16320\n",
      "      251        199      0.396      0.268         32704\n",
      "      209        168      0.281      0.167         65472\n",
      "      174        139      0.194      0.107        131008\n",
      "      144        113      0.125     0.0554        262080\n",
      "Average training error: 11.33%\n",
      "Average test error: 3.12%\n"
     ]
    }
   ],
   "source": [
    "num_label_classes = 10\n",
    "reader_train = create_reader(train_file, True, input_dim, num_label_classes)\n",
    "reader_test = create_reader(test_file, False, input_dim, num_label_classes)\n",
    "model, simple_ae_train_error, simple_ae_test_error = train_and_test(reader_train, \n",
    "                                                                    reader_test, \n",
    "                                                                    model_func = create_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualize the simple autoencoder results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image statistics:\n",
      "Max: 255.00, Median: 0.00, Mean: 24.07, Min: 0.00\n",
      "Decoded image statistics:\n",
      "Max: 249.56, Median: 0.58, Mean: 27.02, Min: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Read some data to run the eval\n",
    "num_label_classes = 10\n",
    "reader_eval = create_reader(test_file, False, input_dim, num_label_classes)\n",
    "\n",
    "eval_minibatch_size = 50\n",
    "eval_input_map = { input  : reader_eval.streams.features }    \n",
    "    \n",
    "eval_data = reader_eval.next_minibatch(eval_minibatch_size,\n",
    "                                  input_map = eval_input_map)\n",
    "\n",
    "img_data = eval_data[input].asarray()\n",
    "\n",
    "# Select a random image\n",
    "np.random.seed(0) \n",
    "idx = np.random.choice(eval_minibatch_size)\n",
    "\n",
    "orig_image = img_data[idx,:,:]\n",
    "decoded_image = model.eval(orig_image)[0]*255\n",
    "\n",
    "# Print image statistics\n",
    "def print_image_stats(img, text):\n",
    "    print(text)\n",
    "    print(\"Max: {0:.2f}, Median: {1:.2f}, Mean: {2:.2f}, Min: {3:.2f}\".format(np.max(img),\n",
    "                                                                              np.median(img),\n",
    "                                                                              np.mean(img),\n",
    "                                                                              np.min(img))) \n",
    "    \n",
    "# Print original image\n",
    "print_image_stats(orig_image, \"Original image statistics:\")\n",
    "\n",
    "# Print decoded image\n",
    "print_image_stats(decoded_image, \"Decoded image statistics:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us plot the original and the decoded image. They should look visually similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a helper function to plot a pair of images\n",
    "def plot_image_pair(img1, text1, img2, text2):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(6, 6))\n",
    "\n",
    "    axes[0].imshow(img1, cmap=\"gray\")\n",
    "    axes[0].set_title(text1)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(img2, cmap=\"gray\")\n",
    "    axes[1].set_title(text2)\n",
    "    axes[1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE1hJREFUeJzt3XnQVfV9x/HPFx72zQVQQIQpGDdABa1b2zhCCxgdo6O0\nWkdh1GoysdWmjjVBgwYTzcSY2jTaEqdogVqlrjO0E2mbtirojCbGdaZsssgiy8O+8+sf5zx65vl9\nD9z78Ky/+37NMHP5nu/Z7j33e3/P+f3OORZCEACg4+vU1hsAAGgeFHQASAQFHQASQUEHgERQ0AEg\nERR0AEgEBb1CZvYdM/tFc+dWsKxgZiNLpv2bmd3UHOsBmsrMLjGz1S0xr5ntMLPfafrW1Za6tt6A\ntmBmUyV9W9IISdskvSjp3hBCfdk8IYQfVLr8anKPRghhcmusB+2Tma2QdIKkA5IOSvpI0jOS/iGE\ncKgNN63ZhBB6t/U2dCQ110I3s29LekTS3ZL6SbpA0jBJr5lZ15J5avKHDx3CFSGEPsqO4Ycl3SPp\nqbbdJLSVmiroZtZX0gOS7ggh/HsIYX8IYYWkKZKGS7ohz5thZvPNbI6ZbZM0NY/NKSzrRjP71Mw2\nmdl9ZrbCzCYU5p+Tvx6enza5ycxWmtlGM/tuYTm/a2aLzKzezNaa2c/Kflic/fmVmd2Sv55qZm+Y\n2WP5spaZ2UV5fJWZbSienjGzr5nZr81sWz59RqNlH27/OpnZX5vZ0nz6c2Z2XNUfCJpNCGFrCOEV\nSX8s6SYzGyVJZtbNzH6cH3vrzexJM+vRMJ+ZXWlmv8mPg6VmNimPDzazV8xss5ktMbNbC/P0MLPZ\nZrbFzD6SdF5xW/J5/9XMPjez5Wb255XO21jxlGM+38/zU4078uP9RDP7ab68T8zsnMK8DcfodjP7\nyMyuKkzrbGaP5t/H5Wb2rXxddfn0fmb2VP6dXGNmM82sc1M+m9ZUUwVd0kWSukt6oRgMIeyQtEDS\nHxbCV0qaL+kYSXOL+WZ2hqSfS/pTSYOUtfSHHGHdvyfpVEnjJd1vZqfn8YOS7pLUX9KF+fRvVrlf\nDc6X9FtJx0uaJ+lZZV+Ykcp+rH5mZg1/wu6UdGO+f1+T9A0z+3qF+3eHpK9L+qqkwZK2SPq7Jm4z\nmlEI4W1JqyX9fh56WNJXJJ2t7DgYIul+KWtMKDtFc7ey4+APJK3I53s2X85gSddI+oGZXZpP+56y\n05UjJE2UVGwodJL0qqT38nWNl3SnmU080rwVmiJpurLvy15JiyS9m/9/vqSfFHKX5u9DP2UNuTlm\nNiifdqukyfn7MlbZ8Vw0W9mprJGSzpH0R5JuqXJbW18IoWb+KStq60qmPSzptfz1DEn/02j6DElz\n8tf3S/rnwrSekvZJmuDkDpcUJJ1UyH9b0p+UbMedkl4s/D9IGlmS+ytJt+Svp0r6v8K00fm8JxRi\nmySdXbKsn0p6rML9+1jS+ML0QZL2S6pr68+4lv4pK74TnPhiSd+VZMp+uEcUpl0oaXn++u8bPvNG\n8w9V1tDoU4j9UNLs/PUySZMK0/5M0ur89fmSVjZa3r2S/vFI85bs4xfHv7IiO6sw7Q5JHxf+P1pS\n/WGW9RtJV+av/1PSbYVpE/J11Snrl9grqUdh+nWS/qutP/Mj/au1c8MbJfU3s7oQwoFG0wbl0xus\nOsxyBhenhxB2mdmmI6x7XeH1Lkm9JcnMvqKsVXGussJZJ+mdIyyrzPrC6935tjWONaz3fGU/YqMk\ndZXUTdLzed6R9m+YpBfNrNjxdlDZF2FNE7cdzWeIpM2SBig7pt4xs4ZpJqnh1MFQZX+ZNjZY0uYQ\nwvZC7FNlx2jD9FWNpjUYJmmwmRUHGHSW9L8VzFuJxseze3xL2WlDSX+prFGlfFr/ku0ovh4mqYuk\ntYX3rZMOXxPahVo75bJI2S/v1cVgfhpisqT/KIQPdxvKtZJOKszfQ9lpjqZ4QtInkk4JIfSV9B1l\nX7qWNk/SK5KGhhD6SXqysN4j7d8qSZNDCMcU/nUPIVDM25iZnaesoL+urIGyW9KZhc+pX/hy5Mgq\nZac+GvtM0nFm1qcQO1lf/livVfZjUJzWYJWyvwCKx0afEMJlFczbbMxsmKRZkr4l6fgQwjGSPlDJ\nMd5om1YpqxP9C/vQN4RwZktsa3OqqYIeQtiq7Fza35rZJDPrYmbDJT2n7HzhP1W4qPmSrsg7Hbsq\nO8XS1CLcR9nQyR1mdpqkbzRxOU1Z7+YQwp78XOr1hWlH2r8nJT2Uf2lkZgPM7MpW2m44zKyvmV2u\n7Nz3nBDC+yEbujhL0mNmNjDPG1I4n/2UpGlmNj7v6B5iZqeFEFZJelPSD82su5mNkXSzpIZBAc9J\nutfMjjWzk5Sd+mjwtqTtZnZP3gHa2cxG5T80R5q3OfVS1ij7PN/vacr+Gm3wnKS/yPf5GGWjgyRJ\nIYS1kn4p6dH8fe1kZiPM7KsttK3NpqYKuiSFEH6krBX8Y2WF9C1lv8jjQwh7K1zGh8oOxGeV/dLv\nkLRB2a96tf5KWTHdruzL9y9NWEZTfFPSg2a2Xdk58+caJlSwf3+jrHX/y3z+xcrOnaL1vZp/BquU\nnTf/iaRphen3SFoiabFlI7YWKuucV8g6UKdJekzSVkn/rex0g5SdMx6urLX+oqTvhRAW5tMeUHaq\nZLmywvdFQyiEcFDS5co6G5cr+yvhF8o6Jg87b3MKIXwk6VFlf5WvV3Z+/Y1Cyqx8/b+V9Gtlp54a\nxvNL2YCBrsrG9m9R1sgZpHbO8hP+OAr5KZt6ZadNlrf19jS31PcPMLPJkp4MIQw7YnI7VnMt9OZi\nZleYWU8z66Wstf++vhzy1eGlvn+obfnpoMvMrM7MhigbTvliW2/X0aKgN92Vyv4c/UzSKcqGIab0\n507q+4faZspO/2xRdsrlY+Xj8zsyTrkAQCJooQNAIijoAJCIVr1S1Mw4v4MWFUJojYuyIhzbaGmV\nHNu00AEgERR0AEgEBR0AEkFBB4BEUNABIBEUdABIBAUdABJBQQeARFDQASARtfZMUQDtWOEZnl/g\nBoKVo4UOAImgoANAIijoAJAICjoAJIJOUQCS/A7JsninTn5bsK4uLildu3aNYmUdnXv27IliBw8e\ndHO9ZdR6ByotdABIBAUdABJBQQeARFDQASARFHQASASjXKp0wgknuPEZM2ZEsdtvv93N9Xri586d\nG8Xuu+8+d/4VK1aUbyBqljcaxRt1Ikm9evWqeLmjRo2KYldffbWbO2DAgCjWu3fvKPbpp5+68y9Y\nsCCKvfPOO25ufX19FDt06JCbWyujX2ihA0AiKOgAkAgKOgAkgoIOAImgU/QwvA7QhQsXurlnnHFG\nFCvroPFcf/31Uezpp592c+kUrW1ll9137tw5inmX3UvSwIEDo9jEiRPd3GuuuSaKDRo0yM3t06eP\nG2+sbHDBmjVrotiyZcvc3B07dkSxffv2VbT+alVzW4SyDtjW6JilhQ4AiaCgA0AiKOgAkAgKOgAk\ngoIOAIlglMthzJw5M4qdfPLJbu6sWbOi2JYtW9zcu+66K4p16dIlit19993u/GUjbZCeah4u0bNn\nzyh2/PHHu7njxo2LYmPHjq14G1auXOnmduvWLYp5o1HKRq5s3LgxipXdvsAbwbN//343t5oRJt7+\nlo1y8T6LsgdytAZa6ACQCAo6ACSCgg4AiaCgA0Ai6BQ9jK1bt0axm2++2c2dP39+xcsdMmRIFLv2\n2mujmNfBJPmdQS11yTPaltcZV3ZLiQMHDkSxsg7FDRs2RLGXX365yq2LVXr7gbJOSq9DsX///m6u\n14G6e/duN9dbXzW3UCjrFPU+i7LltkZnKS10AEgEBR0AEkFBB4BEUNABIBEUdABIhLXm07DNrDYe\nvX0E3uiV1157LYpdfPHF7vwXXXRRFHvrrbeOfsMSEELwhyO0sJY6tstGTHi8ES3eLSUkqUePHlGs\ne/fubu727dsr3gZvud7DNAYMGODO791ao76+3s19++23o5g3ekeq7mEz3iiXakaueKONpKN/wEUl\nxzYtdABIBAUdABJBQQeARFDQASARXPrfBs4555woVtYBitpWTWee10FXNv/evXsrXq7Xmed1HJbl\nep2EZc8V8O7fvm7dOjfX24dq3q9qlF22X02ndWtoX1sDAGgyCjoAJIKCDgCJoKADQCIo6ACQCEa5\ntAHvUmjP+++/78aXLFnSnJuDRLTUCA/v4Q5lD3wYPnx4FLv00kuj2OjRo935P/jggyi2dOlSN3fn\nzp1u3FPNAy6OdmRRW6KFDgCJoKADQCIo6ACQCAo6ACSCTtE2cOutt1aUt379eje+adOm5twcJMLr\n+CvrvPSU5Xr3VD/zzDPd3ClTpkQx77YW+/fvd+d/4YUXoljZ8e7dUqBsH6q5RN97H6u5l3lrPmOi\nMVroAJAICjoAJIKCDgCJoKADQCIo6ACQCEa5tKCRI0e68fPOO6+i+fv16+fGL7jggihWdpuAai6P\nRnrKRlx4o0Hq6vxy4B3H06dPd3O9h7d4y12+fLk7f9euXaOYN8pGknr16hXFyi7F996HspE23jLK\n3se2HNHioYUOAImgoANAIijoAJAICjoAJIJO0RbUt29fNz5gwICK5i/rPH3jjTei2DPPPOPmPvLI\nI1Hsk08+qWj9SJfXKdq9e3c396qrropiY8aMcXO9Y97rmN+zZ487/2mnnRbFevfu7ebu3bu3onVJ\n0ocffhjF1q5d6+Z690Nvb52fZWihA0AiKOgAkAgKOgAkgoIOAImgU7QFbdu2zY0vXLgwio0dOzaK\nHXfccRWv68Ybb3Tjw4YNi2KXX365m7tr166K14eOrZp7p3sdjZ9//rmb68XXrVsXxXbs2OHO7w0E\nuOSSS9xcr7N0w4YNbu7s2bOjWNnzBjx0igIAWhUFHQASQUEHgERQ0AEgERR0AEiEtWbvrZl1jK7i\nNuBd8uzdW1qS7rzzzih27rnnVryu9957z41fdtllUcwbodCehRAqf8x9M+pox7Y3oqVbt25u7pAh\nQ6LYiBEj3Fzvkv59+/ZFsbJbB3jH4KhRo9zcgQMHRrFNmza5ud4tMObNm+fmbt++PYq1h1EulRzb\ntNABIBEUdABIBAUdABJBQQeARHDpfzvh3aO87L7lCxYsiGKLFi1yc0899dQodtZZZ7m53gN6kaZq\nHpq8evXqKLZmzRo313vAstfZWnY/dO8y/7KHpXvHa8+ePd1c7xYY1TxAu0x76CwtooUOAImgoANA\nIijoAJAICjoAJIKCDgCJYJRLB7R169Yotnv37jbYEqTEe9q95I9+qWaEiDe/93AKSercuXMUO3Dg\ngJvrbYN3mwGp/IEalWpvo1nK0EIHgERQ0AEgERR0AEgEBR0AEkGnaDs2ePBgN37bbbdFsdNPP73i\n5S5btsyNe093R/tTdmn60V6yXs1yvc5LSerUKW4jevctHz9+vDv/2WefHcXKbkmxd+/eKLZ06VI3\n9913341iZR2o1bw37a2zlBY6ACSCgg4AiaCgA0AiKOgAkAgKOgAkglEu7cTkyZOj2AMPPODmjhs3\nruLleiNavHVJ5U9MR9vxRld4I0kkqa6u8q+zNzqjbMSGt75evXq5uSeddFIUu+6666LYDTfc4M7f\nv3//KFZ26f+SJUui2NNPP+3mLl68uOLletrbaJYytNABIBEUdABIBAUdABJBQQeARNAp2oKmTZvm\nxh966KEoduyxx0axskuePc8//7wbnz59ehTzOpPQcZRdht6jR48oVtZR6i3j4MGDbq537/LRo0e7\nubfccksUmzRpUhTr3r27O7+3DRs3bnRz58yZE8VeffVVN3fbtm0Vraujo4UOAImgoANAIijoAJAI\nCjoAJIKCDgCJYJRLM5k6dWoUe+KJJ9zcLl26HNW6Zs6cGcW+//3vu7nVXN6MtlPNwyXKcr2HTnTr\n1s3N9S7RLxsRc+KJJ0axm266yc298MILK94Gjzca5aWXXnJz582bF8U2b97s5h46dKjibejIaKED\nQCIo6ACQCAo6ACSCgg4AiaBTtJlMnDgxih1t56d3iwBJevDBB6NYipcx15Jqnipf1sG3a9euKFZ2\n7/StW7dGsVNOOcXNHTNmTBQbOnSom+vtx86dO6PY+vXr3fkff/zxKDZ37lw3t76+PorVSudnGVro\nAJAICjoAJIKCDgCJoKADQCIo6ACQCEa5NJPXX389ik2ZMsXNXbNmTRSbMGFCFCt7EEWt9+SnqOwz\n9UaNlD2Bft++fVFsy5Ytbq438qTssvlVq1ZFsc8++8zN7dOnTxTbsGFDFHvzzTfd+VeuXBnF9u/f\n7+aWvQ+1jBY6ACSCgg4AiaCgA0AiKOgAkAhrzY4FM6MXAy0qhOBfQ9/CWvPYLrtNQGuur+yWAh6v\nw7es7tDRWa6SY5sWOgAkgoIOAImgoANAIijoAJAICjoAJIJL/4EOprVHglTzkA20LVroAJAICjoA\nJIKCDgCJoKADQCIo6ACQCAo6ACSCgg4AiaCgA0AiKOgAkAgKOgAkgoIOAImgoANAIijoAJAICjoA\nJIKCDgCJMJ6yDQBpoIUOAImgoANAIijoAJAICjoAJIKCDgCJoKADQCIo6ACQCAo6ACSCgg4AiaCg\nA0AiKOgAkAgKOgAkgoIOAImgoANAIijoAJAICjoAJIKCDgCJoKADQCIo6ACQCAo6ACSCgg4AiaCg\nA0AiKOgAkIj/B0JuciRsayw0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cbb599ed30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the original and the decoded image\n",
    "img1 = orig_image.reshape(28,28)\n",
    "text1 = 'Original image'\n",
    "\n",
    "img2 = decoded_image.reshape(28,28)\n",
    "text2 = 'Decoded image'\n",
    "\n",
    "plot_image_pair(img1, text1, img2, text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deep Auto encoder\n",
    "\n",
    "We do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of dense layers. Let us create a deep autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cntk.ai/jup/DeepAEfig.jpg\" width=\"500\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 3\n",
    "Image(url=\"http://cntk.ai/jup/DeepAEfig.jpg\", width=500, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The encoding dimensions are 128, 64 and 32 while the decoding dimensions are symmetrically opposite 64, 128 and 784. This increases the number of parameters used to model the transformation and achieves lower error rates at the cost of longer training duration and memory footprint. If we train this deep encoder for larger number iterations by turning the `isFast` flag to be `False`, we get a lower error and the reconstructed images are also marginally better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "encoding_dims = [128,64,32]\n",
    "decoding_dims = [64,128]\n",
    "\n",
    "encoded_model = None\n",
    "\n",
    "def create_deep_model(features):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform()):\n",
    "        encode = C.element_times(C.constant(1.0/255.0), features)\n",
    "\n",
    "        for encoding_dim in encoding_dims:\n",
    "            encode = C.layers.Dense(encoding_dim, activation = C.relu)(encode)\n",
    "\n",
    "        global encoded_model\n",
    "        encoded_model= encode\n",
    "        \n",
    "        decode = encode\n",
    "        for decoding_dim in decoding_dims:\n",
    "            decode = C.layers.Dense(decoding_dim, activation = C.relu)(decode)\n",
    "\n",
    "        decode = C.layers.Dense(input_dim, activation = C.sigmoid)(decode)\n",
    "        return decode  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per sample: 3e-05\n",
      "      543        543       0.71       0.71            64\n",
      "      543        543      0.749      0.768           192\n",
      "      543        543      0.757      0.763           448\n",
      "      542        541      0.742      0.729           960\n",
      "      528        514      0.642      0.547          1984\n",
      "      411        299      0.566      0.494          4032\n",
      "      313        217       0.52      0.474          8128\n",
      "      259        204      0.458      0.397         16320\n",
      "      217        176      0.355      0.252         32704\n",
      "      179        141      0.254      0.153         65472\n",
      "      148        118      0.172     0.0896        131008\n",
      "      124       98.9      0.112     0.0527        262080\n",
      "Average training error: 10.33%\n",
      "Average test error: 3.44%\n"
     ]
    }
   ],
   "source": [
    "num_label_classes = 10\n",
    "reader_train = create_reader(train_file, True, input_dim, num_label_classes)\n",
    "reader_test = create_reader(test_file, False, input_dim, num_label_classes)\n",
    "\n",
    "model, deep_ae_train_error, deep_ae_test_error = train_and_test(reader_train, \n",
    "                                                                reader_test, \n",
    "                                                                model_func = create_deep_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualize the deep autoencoder results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image statistics:\n",
      "Max: 255.00, Median: 0.00, Mean: 24.07, Min: 0.00\n",
      "Decoded image statistics:\n",
      "Max: 251.74, Median: 0.02, Mean: 23.84, Min: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Run the same image as the simple autoencoder through the deep encoder\n",
    "orig_image = img_data[idx,:,:]\n",
    "decoded_image = model.eval(orig_image)[0]*255\n",
    "\n",
    "# Print image statistics\n",
    "def print_image_stats(img, text):\n",
    "    print(text)\n",
    "    print(\"Max: {0:.2f}, Median: {1:.2f}, Mean: {2:.2f}, Min: {3:.2f}\".format(np.max(img),\n",
    "                                                                              np.median(img),\n",
    "                                                                              np.mean(img),\n",
    "                                                                              np.min(img))) \n",
    "    \n",
    "# Print original image\n",
    "print_image_stats(orig_image, \"Original image statistics:\")\n",
    "\n",
    "# Print decoded image\n",
    "print_image_stats(decoded_image, \"Decoded image statistics:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us plot the original and the decoded image with the deep autoencoder. They should look visually similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhEYSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cbbfd4f9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the original and the decoded image\n",
    "img1 = orig_image.reshape(28,28)\n",
    "text1 = 'Original image'\n",
    "\n",
    "img2 = decoded_image.reshape(28,28)\n",
    "text2 = 'Decoded image'\n",
    "\n",
    "plot_image_pair(img1, text1, img2, text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We have shown how to encode and decode an input. In this section we will explore how we can compare one to another and also show how to extract an encoded input for a given input. For visualizing high dimension data in 2D, [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) is probably one of the best methods. However, it typically requires relatively low-dimensional data. So a good strategy for visualizing similarity relationships in high-dimensional data is to encode data into a low-dimensional space (e.g. 32 dimensional) using an autoencoder first, extract the encoding of the input data followed by using t-SNE for mapping the compressed data to a 2D plane. \n",
    "\n",
    "We will use the deep autoencoder outputs to:\n",
    "- Compare two images and\n",
    "- Show how we can retrieve an encoded (compressed) data. \n",
    "\n",
    "First we need to read some image data along with their labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read some data to run get the image data and the corresponding labels\n",
    "num_label_classes = 10\n",
    "reader_viz = create_reader(test_file, False, input_dim, num_label_classes)\n",
    "\n",
    "image = C.input(input_dim)\n",
    "image_label = C.input(num_label_classes)\n",
    "\n",
    "viz_minibatch_size = 50\n",
    "\n",
    "viz_input_map = { \n",
    "    image  : reader_viz.streams.features, \n",
    "    image_label  : reader_viz.streams.labels_viz \n",
    "}    \n",
    "    \n",
    "viz_data = reader_eval.next_minibatch(viz_minibatch_size,\n",
    "                                  input_map = viz_input_map)\n",
    "\n",
    "img_data   = viz_data[image].asarray()\n",
    "imglabel_raw = viz_data[image_label].asarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [7, 24, 39, 44, 46]\n",
      "3: [1, 13, 18, 26, 37, 40, 43]\n",
      "9: [8, 12, 23, 28, 42, 49]\n"
     ]
    }
   ],
   "source": [
    "# Map the image labels into indices in minibatch array\n",
    "img_labels = [np.argmax(imglabel_raw[i,:,:]) for i in range(0, imglabel_raw.shape[0])]       \n",
    "        \n",
    "from collections import defaultdict\n",
    "label_dict=defaultdict(list)\n",
    "for img_idx, img_label, in enumerate(img_labels):\n",
    "    label_dict[img_label].append(img_idx)        \n",
    "    \n",
    "# Print indices corresponding to 3 digits\n",
    "randIdx = [1, 3, 9]\n",
    "for i in randIdx:\n",
    "    print(\"{0}: {1}\".format(i, label_dict[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will [compute cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity) between two images using `scipy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def image_pair_cosine_distance(img1, img2):\n",
    "    if img1.size != img2.size:\n",
    "        raise ValueError(\"Two images need to be of same dimension\")\n",
    "    return 1 - spatial.distance.cosine(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between two original image: 0.294\n",
      "Distance between two decoded image: 0.364\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cbbb13c470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cbc0748240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let s compute the distance between two images of the same number\n",
    "digit_of_interest = 6\n",
    "\n",
    "digit_index_list = label_dict[digit_of_interest]\n",
    "\n",
    "if len(digit_index_list) < 2:\n",
    "    print(\"Need at least two images to compare\")\n",
    "else:\n",
    "    imgA = img_data[digit_index_list[0],:,:][0] \n",
    "    imgB = img_data[digit_index_list[1],:,:][0]\n",
    "    \n",
    "    # Print distance between original image\n",
    "    imgA_B_dist = image_pair_cosine_distance(imgA, imgB)\n",
    "    print(\"Distance between two original image: {0:.3f}\".format(imgA_B_dist))\n",
    "    \n",
    "    # Plot the two images\n",
    "    img1 = imgA.reshape(28,28)\n",
    "    text1 = 'Original image 1'\n",
    "\n",
    "    img2 = imgB.reshape(28,28)\n",
    "    text2 = 'Original image 2'\n",
    "\n",
    "    plot_image_pair(img1, text1, img2, text2)\n",
    "    \n",
    "    # Decode the encoded stream \n",
    "    imgA_decoded =  model.eval([imgA])[0]\n",
    "    imgB_decoded =  model.eval([imgB])   [0]    \n",
    "    imgA_B_decoded_dist = image_pair_cosine_distance(imgA_decoded, imgB_decoded)\n",
    "\n",
    "    # Print distance between original image\n",
    "    print(\"Distance between two decoded image: {0:.3f}\".format(imgA_B_decoded_dist))\n",
    "    \n",
    "    # Plot the two images\n",
    "    # Plot the original and the decoded image\n",
    "    img1 = imgA_decoded.reshape(28,28)\n",
    "    text1 = 'Decoded image 1'\n",
    "\n",
    "    img2 = imgB_decoded.reshape(28,28)\n",
    "    text2 = 'Decoded image 2'\n",
    "\n",
    "    plot_image_pair(img1, text1, img2, text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note: The cosine distance between the original images comparable to the distance between the corresponding decoded images. A value of 1 indicates high similarity between the images and 0 indicates no similarity.\n",
    "\n",
    "Let us now see how to get the encoded vector corresponding to an input image. This should have the dimension of the choke point in the network shown in the figure with the box labeled `E`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the original image is 784 and the encoded image is  32\n",
      "\n",
      "The encoded image: \n",
      "[ 11.03867531   0.           2.82885122   6.88763714  18.50736427\n",
      "   4.74289703  16.39323616   0.           5.42420244  13.35921574\n",
      "   4.85624743   0.           9.28452587   0.          14.61203957   0.\n",
      "  17.59536743  19.19153214   0.          18.20401192   6.55403948\n",
      "   7.19688129   0.          15.9497509    6.13383865  10.26971817\n",
      "   9.00766468  19.94887733   8.21201897   9.97682953  22.68340302\n",
      "  11.01931667]\n"
     ]
    }
   ],
   "source": [
    "imgA = img_data[digit_index_list[0],:,:][0] \n",
    "imgA_encoded =  encoded_model.eval([imgA])\n",
    "\n",
    "print(\"Length of the original image is {0:3d} and the encoded image is {1:3d}\".format(len(imgA), \n",
    "                                                                                      len(imgA_encoded[0])))\n",
    "print(\"\\nThe encoded image: \")\n",
    "print(imgA_encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us compare the distance between different digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between two original image: 0.376\n",
      "Distance between two decoded image: 0.435\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cbba40a518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wsbLse0lbH4+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cbba40ae48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digitA = 3\n",
    "digitB = 8\n",
    "\n",
    "digitA_index = label_dict[digitA]\n",
    "digitB_index = label_dict[digitB]\n",
    "\n",
    "imgA = img_data[digitA_index[0],:,:][0] \n",
    "imgB = img_data[digitB_index[0],:,:][0]\n",
    "\n",
    "# Print distance between original image\n",
    "imgA_B_dist = image_pair_cosine_distance(imgA, imgB)\n",
    "print(\"Distance between two original image: {0:.3f}\".format(imgA_B_dist))\n",
    "    \n",
    "# Plot the two images\n",
    "img1 = imgA.reshape(28,28)\n",
    "text1 = 'Original image 1'\n",
    "\n",
    "img2 = imgB.reshape(28,28)\n",
    "text2 = 'Original image 2'\n",
    "\n",
    "plot_image_pair(img1, text1, img2, text2)\n",
    "    \n",
    "# Decode the encoded stream \n",
    "imgA_decoded =  model.eval([imgA])[0]\n",
    "imgB_decoded =  model.eval([imgB])[0]    \n",
    "imgA_B_decoded_dist = image_pair_cosine_distance(imgA_decoded, imgB_decoded)\n",
    "\n",
    "#Print distance between original image\n",
    "print(\"Distance between two decoded image: {0:.3f}\".format(imgA_B_decoded_dist))\n",
    "\n",
    "# Plot the original and the decoded image\n",
    "img1 = imgA_decoded.reshape(28,28)\n",
    "text1 = 'Decoded image 1'\n",
    "\n",
    "img2 = imgB_decoded.reshape(28,28)\n",
    "text2 = 'Decoded image 2'\n",
    "\n",
    "plot_image_pair(img1, text1, img2, text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Print the results of the deep encoder test error for regression testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11675742686\n"
     ]
    }
   ],
   "source": [
    "# Simple autoencoder test error\n",
    "print(simple_ae_test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.43888063784\n"
     ]
    }
   ],
   "source": [
    "# Deep autoencoder test error\n",
    "print(deep_ae_test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Suggested tasks\n",
    "\n",
    "- Try different activation functions.\n",
    "- Find which images are more similar to one another (a) using original image and (b) decoded image.\n",
    "- Try using mean square error as the loss function. Does it improve the performance of the encoder in terms of reduced errors.\n",
    "- Can you try different network structure to reduce the error further. Explain your observations.\n",
    "- Can you use a different distance metric to compute similarity between the MNIST images.\n",
    "- Try a deep encoder with [1000, 500, 250, 128, 64, 32]. What is the training error for same number of iterations? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
