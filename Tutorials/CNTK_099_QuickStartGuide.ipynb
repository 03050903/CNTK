{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cntk\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "cntk.cntk_py.set_fixed_random_seed(1) # fix the random seed so that examples produce the same results each time\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "matplotlib.pyplot.rcParams['figure.figsize'] = (40,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNTK Quick Start Guide\n",
    "\n",
    "Welcome to CNTK and the wonders of deep learning! This tutorial will give a brief overview of CNTK. It is meant for users that are new to CNTK but have some experience with deep neural networks.\n",
    "The focus will be on how the basic steps of deep learning are done in CNTK.\n",
    "\n",
    "To train a deep model, you will need to:\n",
    "\n",
    " * Define your **model structure**.\n",
    " * Prepare your **data**.\n",
    " * **Train** it.\n",
    " * **Evaluate** its accuracy.\n",
    " * **Deploy** it.\n",
    "\n",
    "This tutorial is structured along these five steps.\n",
    "\n",
    "To run this tutorial, you will need CNTK 2.0 and ideally a CUDA-capable GPU (deep learning is no fun without GPUs).\n",
    "\n",
    "## Defining Your Model Structure\n",
    "\n",
    "So let us dive right in. Below we will introduce CNTK's programming model--*networks are function objects* and CNTK's data model. We will put that into action for logistic regression and MNIST digit recognition,\n",
    "using CNTK's Functional API.\n",
    "Lastly, we will replicate one example using CNTK's lower-level, TensorFlow/Theano-like graph API.\n",
    "\n",
    "#### Programming Model: Networks are Function Objects\n",
    "\n",
    "In CNTK, a neural network is a function object.\n",
    "On one hand, a neural network in CNTK is just a function that you can call\n",
    "to apply it to data.\n",
    "On the other hand, a neural network contains learnable parameters\n",
    "that can be accessed like object members.\n",
    "Complicated function objects can be composed as hierarchies of simpler ones, which,\n",
    "for example, represent layers.\n",
    "The function-object approach is similar to Keras, Chainer, Dynet, Pytorch,\n",
    "and the recent Sonnet.\n",
    "\n",
    "The following illustrates the function-object approach with pseudo-code, using the example\n",
    "of a fully-connected layer (called `Dense` in CNTK)::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[ 0.06484214  0.0668272  -0.06177861  0.05454117  0.00123526]\n",
      " [-0.08024099  0.04517105  0.06488221 -0.09801994 -0.02767334]]\n",
      "y = [-0.0953493   0.15588782  0.06788126 -0.14056186 -0.05405867]\n"
     ]
    }
   ],
   "source": [
    "# numpy *pseudo-code* for CNTK Dense layer (vastly simplified, e,g. no back-prop)\n",
    "def Dense(out_dim, activation):\n",
    "    # create the learnable parameters\n",
    "    b = np.zeros(out_dim)\n",
    "    W = np.ndarray((0,out_dim)) # input dimension is unknown\n",
    "    # define the function itself\n",
    "    def dense(x):\n",
    "        if len(W) == 0:         # first call: reshape and initialize W\n",
    "            W.resize((x.shape[-1], W.shape[-1]), refcheck=False)\n",
    "            W[:] = np.random.randn(*W.shape) * 0.05\n",
    "        return activation(x @ W + b)\n",
    "    # return it as a function object: can be called & holds the parameters as members\n",
    "    dense.W = W\n",
    "    dense.b = b\n",
    "    return dense\n",
    "\n",
    "d = Dense(5, np.tanh)    # create the function object\n",
    "y = d(np.array([1, 2]))  # apply it like a function\n",
    "W = d.W                  # access member like an object\n",
    "print('W =', d.W)\n",
    "print('y =', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the real CNTK function objects are not actual Python lambdas.\n",
    "Rather, they are static graphs in C++,\n",
    "wrapped in the Python class `Function` that exposes `__call__()` and `__getattr__()` methods.\n",
    "However, CNTK allows to define networks as Python expressions and functions,\n",
    "as long as they can be converted into static graphs of type `Function`.\n",
    "\n",
    "The function object is CNTK's single abstraction used to represent different levels of neural networks, which\n",
    "are only distinguished by convention:\n",
    "\n",
    " * **basic operations** without learnable parameters (e.g. `times()`, `__add__()`, `sigmoid()`...)\n",
    " * **layers** (`Dense()`, `Embedding()`, `Convolution()`...). Layers map one input to one output.\n",
    " * **recurrent step functions** (`LSTM()`, `GRU()`, `RNNStep()`). Step functions map a previous state and a new input to a new state.\n",
    " * **loss and metric** functions (`cross_entropy_with_softmax()`, `binary_cross_entropy()`, `squared_error()`, `classification_error()`...).\n",
    "   In CNTK, losses and metric are not special, just functions.\n",
    " * **models**. Models are defined by the user and map features to predictions or scores, and is what gets deployed in the end.\n",
    " * **criterion function**. The criterion function maps (features, labels) to (loss, metric).\n",
    "   The Trainer optimizes the loss by SGD, and logs the metric, which may be non-differentiable.\n",
    "\n",
    "Higher-order layers compose objects into more complex ones, including:\n",
    "\n",
    " * **stacking** (`Sequential()`, `For()`)\n",
    " * **recurrence** (`Recurrence()`, `Fold()`, `Unfold()`...)\n",
    "\n",
    "Users can use arbitrary Python expressions as CNTK function objects with `Function()`.\n",
    "This is similar to Keras' `Lambda()`.\n",
    "Expressions can be written as multi-line functions through decorator syntax (`@Function`).\n",
    "\n",
    "Lastly, function objects enable parameter sharing. If you call the same\n",
    "function object at multiple places, all invocations will naturally share the same learnable parameters.\n",
    "\n",
    "In summary, the function object is CNTK's single abstraction for conveniently defining\n",
    "simple and complex models, parameter sharing, and training objectives.\n",
    "\n",
    "(Note that under the hood, CNTK uses a static graph,\n",
    "and it is possible to define CNTK networks as graphs like TensorFlow and Theano, as discussed\n",
    "further below.)\n",
    "\n",
    "### CNTK's Data model: Sequences of Tensors\n",
    "\n",
    "CNTK can operate on two types of data:\n",
    "\n",
    " * **tensors** (that is, N-dimensional arrays), dense or sparse\n",
    " * **sequences** of tensors\n",
    "\n",
    "The distinction is that the shape of a tensor is static during operation,\n",
    "while the length of a sequence depends on data.\n",
    "Tensors have *static axes*, while a sequence has a *dynamic axis*.\n",
    "\n",
    "In CNTK, categorical data is represented as sparse one-hot tensors, not as integer vectors.\n",
    "This allows to write embeddings and loss functions in a unified fashion as matrix products.\n",
    "\n",
    "CNTK adopts Python's type-annotation syntax to declare CNTK types.\n",
    "For example,\n",
    "\n",
    " * `Tensor[(13,42)]` denotes a tensor with 13 rows and 42 columns, and\n",
    " * `Sequence[SparseTensor[300000]]` a sequence of sparse vectors, which for example could represent a word out of a 300k dictionary\n",
    "\n",
    "Note that unlike Python type hints, this works with Python 2.7. There are two more data types: 'batch of tensors' and 'batch of sequences of tensors'.\n",
    "These are used internally for minibatching, but we generally try to hide batching from the user:\n",
    "We want users to think in tensors and sequences, and leave mini-batching to CNTK.\n",
    "To this end, unlike other toolkits, CNTK transparently batches *sequences with different lengths*\n",
    "into the same minibatch, and handles all the necessary padding and packing.\n",
    "Workarounds like 'bucketing' are not needed.\n",
    "\n",
    "### Your First CNTK Network: Simple Logistic Regression\n",
    "\n",
    "Let us put all of this in action for a very simple example of logistic regression.\n",
    "For this example, we create a synthetic data set of 2-dimensional normal-distributed \n",
    "data points, which should be classified into belonging to one of two classes.\n",
    "Note that CNTK expects the labels as one-hot encoded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.2741797   3.56347561]\n",
      " [ 5.12873602  5.79089499]\n",
      " [ 1.3574543   5.5718112 ]\n",
      " [ 3.54340553  2.46254587]]\n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "input_dim_lr = 2    # classify 2-dimensional data\n",
    "num_classes_lr = 2  # into one of two classes\n",
    "\n",
    "# This example uses synthetic data from normal distributions, which we generate in the following.\n",
    "#  X_lr[corpus_size,input_dim] - input data\n",
    "#  Y_lr[corpus_size]           - labels (0 or 1), in one-hot representation\n",
    "np.random.seed(0)\n",
    "def generate_synthetic_data(N):\n",
    "    Y = np.random.randint(size=N, low=0, high=num_classes_lr)  # labels\n",
    "    X = (np.random.randn(N, input_dim_lr)+3) * (Y[:,None]+1)   # data\n",
    "    # Our model expects float32 features, and cross-entropy expects one-hot encoded labels.\n",
    "    Y = scipy.sparse.csr_matrix((np.ones(N,np.float32), (range(N), Y)), shape=(N, num_classes_lr))\n",
    "    X = X.astype(np.float32)\n",
    "    return X, Y\n",
    "X_train_lr, Y_train_lr = generate_synthetic_data(20000)\n",
    "X_test_lr,  Y_test_lr  = generate_synthetic_data(1024)\n",
    "X_train_lr, Y_train_lr.todense() # let's have a peek\n",
    "print(X_train_lr[:4])\n",
    "print(Y_train_lr[:4].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the model function. The model function maps input data to predictions.\n",
    "It is the final product of the training process.\n",
    "In this example, we use the simplest of all models: logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lr = cntk.layers.Dense(num_classes_lr, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the criterion function. The criterion function is\n",
    "the harness via which the trainer uses to optimize the model:\n",
    "It maps (input vectors, labels) to (loss, metric).\n",
    "The loss is used for the SGD updates. We choose cross entropy.\n",
    "Specifically, `cross_entropy_with_softmax()` first applies\n",
    "the `softmax()` function to the network's output, as\n",
    "cross entropy expects probabilities.\n",
    "We do not include `softmax()` in the model function itself, because\n",
    "it is not necessary for using the model.\n",
    "As the metric, we count classification errors (this metric is not differentiable).\n",
    "\n",
    "We define criterion function as Python code and convert it to a `Function` object.\n",
    "A single expression can be written as `Function(lambda x, y: `*expression of x and y*`)`,\n",
    "similar to Keras' `Lambda()`.\n",
    "To avoid evaluating the model twice, we use a Python function definition\n",
    "with decorator syntax. Lastly, this is a good time to tell CNTK about the\n",
    "data types of our inputs, which is done via the decorator `@Function.with_signature(`*argument types*`)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite(data: Tensor[2], label_one_hot: SparseTensor[2]) -> Tuple[Tensor[1], Tensor[1]]\n",
      "[[-0.89681542 -0.89061725]\n",
      " [-0.11949861 -1.17324626]]\n"
     ]
    }
   ],
   "source": [
    "@cntk.Function.with_signature(cntk.layers.Tensor[input_dim_lr], cntk.layers.SparseTensor[num_classes_lr])\n",
    "def criterion_lr(data, label_one_hot):\n",
    "    z = model_lr(data)  # apply model. Computes a non-normalized log probability for every output class.\n",
    "    loss   = cntk.cross_entropy_with_softmax(z, label_one_hot) # this applies softmax to z under the hood\n",
    "    metric = cntk.classification_error(z, label_one_hot)\n",
    "    return loss, metric\n",
    "print(criterion_lr)\n",
    "print(model_lr.W.value) # W now has known shape and thus gets initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decorator will 'compile' the function into a static graph\n",
    "by calling `criterion()` passing it input variables of the given data types as arguments.\n",
    "Thus, `criterion` is not a Python function but a static graph in a CNTK `Function` object.\n",
    "\n",
    "We are now ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 0.1\n",
      " Minibatch[   1-  50]: loss = 0.663274 * 1600, metric = 37.31% * 1600;\n",
      " Minibatch[  51- 100]: loss = 0.481867 * 1600, metric = 20.56% * 1600;\n",
      " Minibatch[ 101- 150]: loss = 0.402196 * 1600, metric = 12.94% * 1600;\n",
      " Minibatch[ 151- 200]: loss = 0.386619 * 1600, metric = 13.75% * 1600;\n",
      " Minibatch[ 201- 250]: loss = 0.328646 * 1600, metric = 9.19% * 1600;\n",
      " Minibatch[ 251- 300]: loss = 0.301831 * 1600, metric = 9.50% * 1600;\n",
      " Minibatch[ 301- 350]: loss = 0.299345 * 1600, metric = 9.44% * 1600;\n",
      " Minibatch[ 351- 400]: loss = 0.279577 * 1600, metric = 8.94% * 1600;\n",
      " Minibatch[ 401- 450]: loss = 0.281061 * 1600, metric = 8.25% * 1600;\n",
      " Minibatch[ 451- 500]: loss = 0.261366 * 1600, metric = 7.81% * 1600;\n",
      " Minibatch[ 501- 550]: loss = 0.244967 * 1600, metric = 7.12% * 1600;\n",
      " Minibatch[ 551- 600]: loss = 0.243953 * 1600, metric = 8.31% * 1600;\n",
      "Finished Epoch[1]: loss = 0.344399 * 20000, metric = 12.58% * 20000 7.356s (2718.9 samples/s);\n",
      "[[-1.25055134 -0.53687745]\n",
      " [-0.99188197 -0.30085728]]\n"
     ]
    }
   ],
   "source": [
    "learner = cntk.sgd(model_lr.parameters, cntk.learning_rate_schedule(0.1, cntk.UnitType.minibatch))\n",
    "progress_writer = cntk.logging.ProgressPrinter(50)\n",
    "\n",
    "criterion_lr.train((X_train_lr, Y_train_lr), parameter_learners=[learner], callbacks=[progress_writer])\n",
    "\n",
    "print(model_lr.W.value) # peek at updated W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `learner` is the object that actually performs the model update. Alternative learners include `momentum_sgd()` and `adam()`. The `progress_writer` is a stock logging callback that prints the output you see above, and can be replaced by your own\n",
    "or the stock `TensorBoardProgressWriter`to visualize training progress using TensorBoard.\n",
    "\n",
    "The `train()` function is feeding our data `(X_train_lr, Y_train_lr)` minibatch by minibatch to the model and updates it, where the data is a tuple in the same order as the arguments of `criterion_mn()`.\n",
    "\n",
    "Let us test how we are doing on our test set (this will also run minibatch by minibatch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Evaluation [1]: Minibatch[1-32]: metric = 8.11% * 1024;\n"
     ]
    }
   ],
   "source": [
    "test_metric = criterion_lr.test((X_test_lr, Y_test_lr), callbacks=[progress_writer]).metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And lastly, let us run a few samples through our model and see how it is doing.\n",
    "Oops, `criterion` knew the input types, but `model_lr` does not,\n",
    "so we tell it using `update_signature()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense(x: Tensor[2]) -> Tensor[2]\n"
     ]
    }
   ],
   "source": [
    "model_lr.update_signature(cntk.layers.Tensor[input_dim_lr])\n",
    "print(model_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call it like any Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label    : [0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0]\n",
      "Predicted: [0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "z = model_lr(X_test_lr[:25])\n",
    "print(\"Label    :\", [label.todense().argmax() for label in Y_test_lr[:25]])\n",
    "print(\"Predicted:\", [z[i,:].argmax() for i in range(len(z))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Second CNTK Network: MNIST Digit Recognition\n",
    "\n",
    "Let us do the same thing as above on an actual task--the MNIST benchmark, which is sort of the \"hello world\" of deep learning.\n",
    "The MNIST task is to recognize scans of hand-written digits. We first download and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFcAAABUCAYAAAD3XKvCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAABMxJREFUeJzt2t9LU30cwPH3mVtzpdtYlmwVlA5KNhFMl4TURV00iP6A\nru2/6V/oH+giCBRq3WlQw2yWrEDHMtwkM23zOO244/k+Fz2GPZfW98yd5/OCIQyGn7358t35ZSil\nEHr4Wj2Al0lcjSSuRhJXI4mrkcTVSOJqJHE1krgaSVyNJK5GElcjiauRxNVI4mrkb/UA/zruF5WN\no3xIVq5Gx2Xl/nUHd1gO32kxDOO3v7p5Om6lUmFqaopisUgsFuPOnTukUikikYgrM3h2W1hbW2N2\ndpbHjx/z4sULlpaW6OjocHUGz8VVSrG/v8+HDx/I5XK8evUKy7JIJBIkk0lOnjzp2iye2xYcx2F7\ne5uZmRmePXuGz+djaGiI4eFhotEoPp9768lTcR3HoVarkcvlyOfzNBoNBgYGyGazXL9+XbaFP2FZ\nFtVqlcnJSRYXF+np6SGbzTI+Ps6lS5dcn8dTcev1OktLS8zMzFCr1Uin00xMTJBMJlsyj6fiVqtV\n5ubm2N3d5fz586RSKWKxGCdOnGjJPJ7Ycw+OED59+kQ+n8cwDK5evcqNGzcIhUKu/ogd5omVq5Ri\na2uLUqnEwsIC0WiUTCbD2NgYfn/r1o8n4jqOQ7lcZnl5mb29Pa5cucKFCxcIhUItnavt4yqlsG2b\n+fl5SqUSgUCAy5cv093dze7ubktn88Se22w2KRQKlEolms0m1WqVly9fsr6+zuDgIIlEgq6uLvcH\nU0odh9eR2batvn79qsbHxxU/rwsrQAUCAZVOp9XDhw9VsVhUzWbzT/7Nkb5X228LjUaDlZUVLMv6\n7X3btlleXubRo0c8efKE9+/fuz5b228Lm5ubzM/PY5omnZ2dnD59mlQqRTAYZGNjg7dv3zI7O0t/\nfz/Dw8Ouztb2cb99+8abN28wTZPe3l4ymQz37t2js7OTQqHA3NwclUqF1dVV12dr+7j1ep2PHz/S\naDS4efMmDx48YGRkhO/fv7O2ttayEwjwwKHY1tYWi4uLWJZFJBIhHo8TCoXw+/2/wu7s7LCzs4Pj\nOL/d9tGt7eNalsXm5iYA4XCYM2fO4Pf72d7eplaruRrzv9p+WzgQDAaJxWKcPXsWpRSrq6uUy2Uc\nxyEcDhMOh13fIjwR1+fz0dPTQzgcxnEcGo0G7969I5/PY9s2fX19Lbme64m4hmHQ29tLJBLBNE2m\np6d5/fo1X758IR6Pk8lkSKfTrs/V9nsu/IwbjUYxDINyuczTp08pFAr4fD5GR0cZGRnh4sWLrs/l\niZV7oFKpUK/Xef78ORsbGwwODnL//n36+vpcexDkME/E3d/fZ2FhgZWVFTo6OjBNk6GhIbLZLNeu\nXSMWi0nco4jH49y+fZvPnz9j2zZdXV3cvXuX0dFRxsbGSCQSLbtgbrTyOPCQIw+xvr5OsVgkl8th\nWRbJZJJbt25x7tw5Tp069bfmO9Kyb/u4zWaTHz9+YJomjuMQDAbp7u4mEAj8zecU/p9xXSLP5x43\nElcjiauRxNVI4mp0XE4i3D99coGsXI0krkYSVyOJq5HE1UjiaiRxNZK4GklcjSSuRhJXI4mrkcTV\nSOJqJHE1krgaSVyNJK5GElcjiauRxNVI4mokcTWSuBpJXI0krkYSVyOJq5HE1egfQbq1Uutu84wA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x57f57c3128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape_mn = (28, 28)  # MNIST digits are 28 x 28\n",
    "num_classes_mn = 10        # classify as one of 10 digits\n",
    "\n",
    "# Fetch the MNIST data. Best done with scikit-learn.\n",
    "try:\n",
    "    from sklearn import datasets, utils\n",
    "    mnist = datasets.fetch_mldata(\"MNIST original\")\n",
    "    X, Y = mnist.data / 255.0, mnist.target\n",
    "    X_train_mn, X_test_mn = X[:60000].reshape((-1,28,28)), X[60000:].reshape((-1,28,28))\n",
    "    Y_train_mn, Y_test_mn = Y[:60000].astype(int), Y[60000:].astype(int)\n",
    "except: # workaround if scikit-learn is not present\n",
    "    import requests, io, gzip\n",
    "    X_train_mn, X_test_mn = (np.fromstring(gzip.GzipFile(fileobj=io.BytesIO(requests.get('http://yann.lecun.com/exdb/mnist/' + name + '-images-idx3-ubyte.gz').content)).read()[16:], dtype=np.uint8).reshape((-1,28,28)).astype(np.float32) / 255.0 for name in ('train', 't10k'))\n",
    "    Y_train_mn, Y_test_mn = (np.fromstring(gzip.GzipFile(fileobj=io.BytesIO(requests.get('http://yann.lecun.com/exdb/mnist/' + name + '-labels-idx1-ubyte.gz').content)).read()[8:], dtype=np.uint8).astype(int) for name in ('train', 't10k'))\n",
    "\n",
    "# Shuffle the training data.\n",
    "np.random.seed(0) # always use the same reordering, for reproducability\n",
    "idx = np.random.permutation(len(X_train_mn))\n",
    "X_train_mn, Y_train_mn = X_train_mn[idx], Y_train_mn[idx]\n",
    "\n",
    "# Further split off a cross-validation set\n",
    "X_train_mn, X_cv_mn = X_train_mn[:54000], X_train_mn[54000:]\n",
    "Y_train_mn, Y_cv_mn = Y_train_mn[:54000], Y_train_mn[54000:]\n",
    "\n",
    "# Our model expects float32 features, and cross-entropy expects one-hot encoded labels.\n",
    "Y_train_mn, Y_cv_mn, Y_test_mn = (scipy.sparse.csr_matrix((np.ones(len(Y),np.float32), (range(len(Y)), Y)), shape=(len(Y), 10)) for Y in (Y_train_mn, Y_cv_mn, Y_test_mn))\n",
    "X_train_mn, X_cv_mn, X_test_mn = (X.astype(np.float32) for X in (X_train_mn, X_cv_mn, X_test_mn))\n",
    "\n",
    "# have a peek\n",
    "matplotlib.pyplot.rcParams['figure.figsize'] = (0.5,0.5)\n",
    "matplotlib.pyplot.axis('off')\n",
    "_ = matplotlib.pyplot.imshow(X_train_mn[13], cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the CNTK model function to map (28x28)-dimensional images to a 10-dimensional score vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with cntk.layers.default_options(activation=cntk.ops.relu, pad=False):\n",
    "    model_mn = cntk.layers.Sequential([\n",
    "        cntk.layers.Convolution2D((5,5), num_filters=32, reduction_rank=0, pad=True), # reduction_rank=0 for B&W images\n",
    "        cntk.layers.MaxPooling((2,2), strides=(2,2)),\n",
    "        cntk.layers.Convolution2D((3,3), num_filters=48),\n",
    "        cntk.layers.MaxPooling((2,2), strides=(2,2)),\n",
    "        cntk.layers.Convolution2D((3,3), num_filters=64),\n",
    "        cntk.layers.Dense(96),\n",
    "        cntk.layers.Dropout(dropout_rate=0.5),\n",
    "        cntk.layers.Dense(num_classes_mn, activation=None) # no activation in final layer (softmax is done in criterion)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a tad bit more complicated! It consists of several convolution-pooling layeres and two\n",
    "fully-connected layers for classification which is typical for MNIST. This demonstrates several aspects of CNTK's Functional API.\n",
    "\n",
    "First, we create each layer using a function from CNTK's layers library (`cntk.layers`).\n",
    "\n",
    "Second, the higher-order layer `Sequential()` creates a new function that applies all those layers\n",
    "one after another. This is known [forward function composition](https://en.wikipedia.org/wiki/Function_composition).\n",
    "Note that unlike some other toolkits, you cannot `Add()` more layers afterwards to a sequential layer.\n",
    "CNTK's `Function` objects are immutable, besides their learnable parameters (to edit a `Function` object, you can `clone()` it).\n",
    "If you prefer that style, create your layers as a Python list and pass that to `Sequential()`.\n",
    "\n",
    "Third, the context manager `default_options()` allows to specify defaults for various optional arguments to layers,\n",
    "such as that the activation function is always `relu`, unless overriden.\n",
    "\n",
    "Lastly, note that `relu` is passed as the actual function, not a string.\n",
    "Any function can be an activation function.\n",
    "It is also allowed to pass a Python lambda directly, for example relu could also be\n",
    "realized manually by saying `activation=lambda x: cntk.ops.element_max(x, 0)`.\n",
    "\n",
    "The criterion function is defined like in the previous example, to map maps (28x28)-dimensional features and according\n",
    "labels to loss and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@cntk.Function.with_signature(cntk.layers.Tensor[input_shape_mn], cntk.layers.SparseTensor[num_classes_mn])\n",
    "def criterion_mn(data, label_one_hot):\n",
    "    z = model_mn(data)\n",
    "    loss   = cntk.cross_entropy_with_softmax(z, label_one_hot)\n",
    "    metric = cntk.classification_error(z, label_one_hot)\n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training, let us throw momentum into the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = len(X_train_mn)\n",
    "lr = cntk.learning_rate_schedule([0.001]*12 + [0.0005]*6 + [0.00025]*6 + [0.000125]*3 + [0.0000625]*3 + [0.00003125], cntk.learners.UnitType.sample, epoch_size=N)\n",
    "momentums = cntk.learners.momentum_as_time_constant_schedule([0]*5 + [1024], epoch_size=N)\n",
    "minibatch_sizes = cntk.minibatch_size_schedule([256]*6 + [512]*9 + [1024]*7 + [2048]*8 + [4096], epoch_size=N)\n",
    "\n",
    "learner = cntk.learners.momentum_sgd(model_mn.parameters, lr, momentums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks unusual in several aspects.\n",
    "First, the learning rate is specified as a list (`[0.001]*12 + [0.0005]*6 +`...). Together with the `epoch_size` parameter, this tells CNTK to use 0.001 for 12 epochs, and then continue with 0.005 for another 6, etc.\n",
    "\n",
    "Second, the learning rate has `cntk.learners.UnitType.sample` (vs. `cntk.UnitType.minibatch`). This tells CNTK to apply the learning rate not to the minibatch-*average*, but to each sample (or, the minibatch-*sum*). To get the same convergence, the learning rate must be divided by the minibatch size, hence the small value.\n",
    "\n",
    "Third, momentum is specified as a time constant of 1024. This says that for a minibatch size of 64, after 1024/64=16 minibatches, each sample gradient still contributes with a weight of 1/e.\n",
    "\n",
    "But why?? In our experience, the effect of learning rates and momentum on training convergence, when specified this way, tends to be independent of the minibatch size within a wide range. The minibatch size now just means how 'outdated' the model is before we update it.\n",
    "The minibatch size is crucial for efficiency of GPUs and parallel training, and this way it can be tuned while keeping learning rate and momentum the same. We find that it is possible to crank up the minibatch size to much larger values during the training. The minibatch-size schedule yields 6-times speed-up (!) towards the end (on a Titan-X).\n",
    "\n",
    "Having this out of the way, let us now train the model. On a Titan-X, this will run for about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per sample: 0.001\n",
      "Momentum per sample: 0.0\n",
      "Finished Epoch[1]: loss = 0.545271 * 54000, metric = 17.65% * 54000 8.561s (6307.7 samples/s);\n",
      "Finished Epoch[2]: loss = 0.134691 * 54000, metric = 3.95% * 54000 8.210s (6577.3 samples/s);\n",
      "Finished Epoch[3]: loss = 0.095339 * 54000, metric = 2.72% * 54000 8.934s (6044.3 samples/s);\n",
      "Finished Epoch[4]: loss = 0.074736 * 54000, metric = 2.13% * 54000 9.816s (5501.2 samples/s);\n",
      "Finished Epoch[5]: loss = 0.062433 * 54000, metric = 1.83% * 54000 8.356s (6462.4 samples/s);\n",
      "Momentum per sample: 0.9990239141819757\n",
      "Finished Epoch[6]: loss = 0.054977 * 54000, metric = 1.58% * 54000 8.846s (6104.5 samples/s);\n",
      "Finished Epoch[7]: loss = 0.047248 * 54000, metric = 1.33% * 54000 4.783s (11290.0 samples/s);\n",
      "Finished Epoch[8]: loss = 0.039817 * 54000, metric = 1.13% * 54000 5.593s (9654.9 samples/s);\n",
      "Finished Epoch[9]: loss = 0.036338 * 54000, metric = 1.11% * 54000 4.781s (11294.7 samples/s);\n",
      "Finished Epoch[10]: loss = 0.033888 * 54000, metric = 1.05% * 54000 4.382s (12323.1 samples/s);\n",
      "Finished Epoch[11]: loss = 0.029883 * 54000, metric = 0.91% * 54000 4.838s (11161.6 samples/s);\n",
      "Finished Epoch[12]: loss = 0.028776 * 54000, metric = 0.89% * 54000 4.768s (11325.5 samples/s);\n",
      "Learning rate per sample: 0.0005\n",
      "Finished Epoch[13]: loss = 0.022807 * 54000, metric = 0.67% * 54000 5.890s (9168.1 samples/s);\n",
      "Finished Epoch[14]: loss = 0.019911 * 54000, metric = 0.64% * 54000 5.214s (10356.7 samples/s);\n",
      "Finished Epoch[15]: loss = 0.018327 * 54000, metric = 0.56% * 54000 5.137s (10512.0 samples/s);\n",
      "Finished Epoch[16]: loss = 0.018387 * 54000, metric = 0.57% * 54000 4.092s (13196.5 samples/s);\n",
      "Finished Epoch[17]: loss = 0.017701 * 54000, metric = 0.54% * 54000 3.700s (14594.6 samples/s);\n",
      "Finished Epoch[18]: loss = 0.016981 * 54000, metric = 0.51% * 54000 2.876s (18776.1 samples/s);\n",
      "Learning rate per sample: 0.00025\n",
      "Finished Epoch[19]: loss = 0.015300 * 54000, metric = 0.46% * 54000 3.175s (17007.9 samples/s);\n",
      "Finished Epoch[20]: loss = 0.014769 * 54000, metric = 0.46% * 54000 3.314s (16294.5 samples/s);\n",
      "Finished Epoch[21]: loss = 0.013246 * 54000, metric = 0.41% * 54000 2.564s (21060.8 samples/s);\n",
      "Finished Epoch[22]: loss = 0.013669 * 54000, metric = 0.40% * 54000 3.243s (16651.2 samples/s);\n",
      "Finished Epoch[23]: loss = 0.014086 * 54000, metric = 0.39% * 54000 3.150s (17142.9 samples/s);\n",
      "Finished Epoch[24]: loss = 0.013156 * 54000, metric = 0.39% * 54000 1.823s (29621.5 samples/s);\n",
      "Learning rate per sample: 0.000125\n",
      "Finished Epoch[25]: loss = 0.012511 * 54000, metric = 0.40% * 54000 1.815s (29752.1 samples/s);\n",
      "Finished Epoch[26]: loss = 0.010865 * 54000, metric = 0.33% * 54000 2.063s (26175.5 samples/s);\n",
      "Finished Epoch[27]: loss = 0.011505 * 54000, metric = 0.36% * 54000 1.797s (30050.1 samples/s);\n",
      "Learning rate per sample: 6.25e-05\n",
      "Finished Epoch[28]: loss = 0.011247 * 54000, metric = 0.34% * 54000 2.038s (26496.6 samples/s);\n",
      "Finished Epoch[29]: loss = 0.011194 * 54000, metric = 0.34% * 54000 2.102s (25689.8 samples/s);\n",
      "Finished Epoch[30]: loss = 0.010723 * 54000, metric = 0.29% * 54000 1.798s (30033.4 samples/s);\n",
      "Learning rate per sample: 3.125e-05\n",
      "Finished Epoch[31]: loss = 0.010906 * 54000, metric = 0.36% * 54000 3.305s (16338.9 samples/s);\n",
      "Finished Epoch[32]: loss = 0.010538 * 54000, metric = 0.33% * 54000 1.458s (37037.0 samples/s);\n",
      "Finished Epoch[33]: loss = 0.010781 * 54000, metric = 0.33% * 54000 1.634s (33047.7 samples/s);\n",
      "Finished Epoch[34]: loss = 0.010076 * 54000, metric = 0.30% * 54000 1.503s (35928.1 samples/s);\n",
      "Finished Epoch[35]: loss = 0.010218 * 54000, metric = 0.34% * 54000 1.459s (37011.7 samples/s);\n",
      "Finished Epoch[36]: loss = 0.010806 * 54000, metric = 0.32% * 54000 1.687s (32009.5 samples/s);\n",
      "Finished Epoch[37]: loss = 0.010291 * 54000, metric = 0.32% * 54000 1.548s (34883.7 samples/s);\n",
      "Finished Epoch[38]: loss = 0.009849 * 54000, metric = 0.28% * 54000 1.462s (36935.7 samples/s);\n",
      "Finished Epoch[39]: loss = 0.009730 * 54000, metric = 0.28% * 54000 1.884s (28662.4 samples/s);\n",
      "Finished Epoch[40]: loss = 0.009622 * 54000, metric = 0.29% * 54000 1.967s (27453.0 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-313]: metric = 0.70% * 10000;\n"
     ]
    }
   ],
   "source": [
    "progress_writer = cntk.logging.ProgressPrinter()\n",
    "criterion_mn.train((X_train_mn, Y_train_mn), minibatch_size=minibatch_sizes,\n",
    "                   max_epochs=40, parameter_learners=[learner], callbacks=[progress_writer])\n",
    "test_metric = criterion_mn.test((X_test_mn, Y_test_mn), callbacks=[progress_writer]).metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph API Example: MNIST Digit Recognition Again\n",
    "\n",
    "CNTK also allows networks to be written in graph style like TensorFlow and Theano. The following defines the same model and criterion function as above, and you would get the same result if you had run the training above with the definition below (if you try that now, results will differ due to random initialization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite(images: Tensor[28,28], labels: SparseTensor[10]) -> Tuple[Tensor[1], Tensor[1]]\n"
     ]
    }
   ],
   "source": [
    "images = cntk.input_variable(input_shape_mn, name='images')\n",
    "with cntk.layers.default_options(activation=cntk.ops.relu, pad=False):\n",
    "    r = cntk.layers.Convolution2D((5,5), num_filters=32, reduction_rank=0, pad=True)(images)\n",
    "    r = cntk.layers.MaxPooling((2,2), strides=(2,2))(r)\n",
    "    r = cntk.layers.Convolution2D((3,3), num_filters=48)(r)\n",
    "    r = cntk.layers.MaxPooling((2,2), strides=(2,2))(r)\n",
    "    r = cntk.layers.Convolution2D((3,3), num_filters=64)(r)\n",
    "    r = cntk.layers.Dense(96)(r)\n",
    "    r = cntk.layers.Dropout(dropout_rate=0.5)(r)\n",
    "    model_mn = cntk.layers.Dense(num_classes_mn, activation=None)(r)\n",
    "\n",
    "label_one_hot = cntk.input_variable(num_classes_mn, is_sparse=True, name='labels')\n",
    "loss   = cntk.cross_entropy_with_softmax(model_mn, label_one_hot)\n",
    "metric = cntk.classification_error(model_mn, label_one_hot)\n",
    "criterion_mn = cntk.combine([loss, metric])\n",
    "print(criterion_mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Your Data\n",
    "\n",
    "Once you have decided your model structure and defined it, you are facing the question on feeding\n",
    "your training data to the CNTK training process.\n",
    "\n",
    "The above examples simply feed the data as numpy/scipy arrays.\n",
    "That is only one of three ways CNTK provides for feeding data to the trainer:\n",
    "\n",
    " 1. As **numpy/scipy arrays**, for small data sets that can just be loaded into RAM.\n",
    " 2. Through instances of **CNTK's MinibatchSource class**, for large data sets that do not fit into RAM.\n",
    " 3. Through an **explicit minibatch-loop** when the above do not apply.\n",
    "\n",
    "### 1. Feeding Data Via Numpy/Scipy Arrays\n",
    "\n",
    "The `train()` and `test()` functions accept a tuple of numpy or scipy arrays for their `minibatch_source` arguments.\n",
    "The tuple members must be in the same order as the arguments of the `criterion` function that `train()` or `test()` are called on.\n",
    "For dense tensors, use numpy arrays, while sparse data should have the type `scipy.sparse.csr_matrix`.\n",
    "\n",
    "Each of the arguments should be a Python list of numpy/scipy arrays, where each list entry represents a data item. For arguments declared as `Sequence[...]`, the first axis of the numpy/scipy array is the sequence length, while the remaining axes are the shape of each token of the sequence. Arguments that are not sequences consist of a single tensor. The shapes, data types (`np.float32/float64`) and sparseness must match the argument types as declared in the criterion function.\n",
    "\n",
    "As an optimization, arguments that are not sequences can also be passed as a single large numpy/scipy array (instead of a list). This is what is done in the examples above.\n",
    "\n",
    "Note that it is the responsibility of the user to randomize the data.\n",
    "\n",
    "### 2. Feeding Data Using the `MinibatchSource` class for Reading Data\n",
    "\n",
    "Production-scale training data sometimes does not fit into RAM. For example, a typical speech corpus may be several hundred GB large. For this case, CNTK provides the `MinibatchSource` class, which provides:\n",
    "\n",
    " * A **chunked randomization algorithm** that holds only part of the data in RAM at any given time.\n",
    " * **Distributed reading** where each worker reads a different subset.\n",
    " * A **transformation pipeline** for images and image augmentation.\n",
    " * **Composability** across multiple data types (e.g. image captioning).\n",
    "\n",
    "At present, the `MinibatchSource` class implements a limited set of data types in the form of \"deserializers\":\n",
    "\n",
    " * **Images** (`ImageDeserializer`).\n",
    " * **Speech files** (`HTKFeatureDeserializer`, `HTKMLFDeserializer`).\n",
    " * Images in CNTK's **canonical text format (CTF)**, which encodes any of CNTK's data types in a human-readable text format.\n",
    "\n",
    "The following example shows how to instantiate an image reader that crops images to 80% at random offsets, and scales the result to 32x32 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_width, image_height, num_channels = (32, 32, 3)\n",
    "num_classes = 1000\n",
    "def create_image_reader(map_file, mean_file, train):\n",
    "    transforms = []\n",
    "    # train uses data augmentation (translation only)\n",
    "    if train:\n",
    "        transforms += [\n",
    "            cntk.io.transforms.crop(crop_type='randomside', side_ratio=0.8) \n",
    "        ]\n",
    "    transforms += [\n",
    "        cntk.io.transforms.scale(width=image_width, height=image_height, channels=num_channels, interpolations='linear'),\n",
    "        cntk.io.transforms.mean(mean_file)\n",
    "    ]\n",
    "    # deserializer\n",
    "    return cntk.io.MinibatchSource(cntk.io.ImageDeserializer(map_file, cntk.io.StreamDefs(\n",
    "        features = cntk.io.StreamDef(field='image', transforms=transforms),\n",
    "        labels   = cntk.io.StreamDef(field='label', shape=num_classes)\n",
    "    )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Feeding Data Via an Explicit Minibatch Loop\n",
    "\n",
    "Instead of feeding your data as a whole to CNTK's `train()` and `test()` functions which implement a minibatch loop internally,\n",
    "you can realize your own minibatch loop and call the lower-level APIs `train_minibatch()` and `test_minibatch()`.\n",
    "This is useful when your data is not in a form suitable for the above, such as being generated on the fly as in variants of reinforcement learning. The `train_minibatch()` and `test_minibatch()` methods require you to instantiate an object of class `Trainer` that takes a subset of the arguments of `train()`. The following implements the logistic-regression example from above through explicit minibatch loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 0.1\n",
      " Minibatch[   1-  50]: loss = 0.663274 * 1600, metric = 37.31% * 1600;\n",
      " Minibatch[  51- 100]: loss = 0.481867 * 1600, metric = 20.56% * 1600;\n",
      " Minibatch[ 101- 150]: loss = 0.402196 * 1600, metric = 12.94% * 1600;\n",
      " Minibatch[ 151- 200]: loss = 0.386619 * 1600, metric = 13.75% * 1600;\n",
      " Minibatch[ 201- 250]: loss = 0.328646 * 1600, metric = 9.19% * 1600;\n",
      " Minibatch[ 251- 300]: loss = 0.301831 * 1600, metric = 9.50% * 1600;\n",
      " Minibatch[ 301- 350]: loss = 0.299345 * 1600, metric = 9.44% * 1600;\n",
      " Minibatch[ 351- 400]: loss = 0.279577 * 1600, metric = 8.94% * 1600;\n",
      " Minibatch[ 401- 450]: loss = 0.281061 * 1600, metric = 8.25% * 1600;\n",
      " Minibatch[ 451- 500]: loss = 0.261366 * 1600, metric = 7.81% * 1600;\n",
      " Minibatch[ 501- 550]: loss = 0.244967 * 1600, metric = 7.12% * 1600;\n",
      " Minibatch[ 551- 600]: loss = 0.243953 * 1600, metric = 8.31% * 1600;\n",
      "Finished Epoch[1]: loss = 0.344399 * 20000, metric = 12.58% * 20000 9.583s (2087.0 samples/s);\n",
      "Finished Evaluation [4]: Minibatch[1-32]: metric = 8.11% * 1024;\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model, so that we can start afresh. This is a direct copy from above.\n",
    "model_lr = cntk.layers.Dense(num_classes_lr, activation=None)\n",
    "@cntk.Function.with_signature(cntk.layers.Tensor[input_dim_lr], cntk.layers.SparseTensor[num_classes_lr])\n",
    "def criterion_lr(data, label_one_hot):\n",
    "    z = model_lr(data)  # apply model. Computes a non-normalized log probability for every output class.\n",
    "    loss   = cntk.cross_entropy_with_softmax(z, label_one_hot) # this applies softmax to z under the hood\n",
    "    metric = cntk.classification_error(z, label_one_hot)\n",
    "    return loss, metric\n",
    "\n",
    "# Create the learner; same as above.\n",
    "learner = cntk.sgd(model_lr.parameters, cntk.learning_rate_schedule(0.1, cntk.UnitType.minibatch))\n",
    "\n",
    "# This time we must create a Trainer instance ourselves.\n",
    "trainer = cntk.Trainer(None, criterion_lr, [learner], [cntk.logging.ProgressPrinter(50)])\n",
    "\n",
    "# Train the model by spoon-feeding minibatch by minibatch.\n",
    "minibatch_size = 32\n",
    "for i in range(0, len(X_train_lr), minibatch_size): # loop over minibatches\n",
    "    x = X_train_lr[i:i+minibatch_size] # get one minibatch worth of data\n",
    "    y = Y_train_lr[i:i+minibatch_size]\n",
    "    trainer.train_minibatch({criterion_lr.arguments[0]: x, criterion_lr.arguments[1]: y})  # update model from one minibatch\n",
    "trainer.summarize_training_progress()\n",
    "\n",
    "# Test error rate minibatch by minibatch\n",
    "evaluator = cntk.Evaluator(criterion_lr.outputs[1], [progress_writer]) # metric is the second output of criterion_lr()\n",
    "for i in range(0, len(X_test_lr), minibatch_size): # loop over minibatches\n",
    "    x = X_test_lr[i:i+minibatch_size] # get one minibatch worth of data\n",
    "    y = Y_test_lr[i:i+minibatch_size]\n",
    "    evaluator.test_minibatch({criterion_lr.arguments[0]: x, criterion_lr.arguments[1]: y})  # test one minibatch\n",
    "evaluator.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the trainer and perform model training\n",
    "training_progress_output_freq = 50\n",
    "\n",
    "plotdata = {\"batchsize\":[], \"loss\":[], \"error\":[]}\n",
    "\n",
    "for i in range(0, num_minibatches_to_train):\n",
    "    features, labels = generate_random_data_sample(minibatch_size, input_dim, num_output_classes)\n",
    "    \n",
    "    # Specify input variables mapping in the model to actual minibatch data to be trained with\n",
    "    trainer.train_minibatch({feature : features, label : labels})\n",
    "    batchsize, loss, error = print_training_progress(trainer, i, \n",
    "                                                     training_progress_output_freq, verbose=1)\n",
    "    \n",
    "    if not (loss == \"NA\" or error ==\"NA\"):\n",
    "        plotdata[\"batchsize\"].append(batchsize)\n",
    "        plotdata[\"loss\"].append(loss)\n",
    "        plotdata[\"error\"].append(error)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the moving average loss to smooth out the noise in SGD\n",
    "plotdata[\"avgloss\"] = moving_average(plotdata[\"loss\"])\n",
    "plotdata[\"avgerror\"] = moving_average(plotdata[\"error\"])\n",
    "\n",
    "# Plot the training loss and the training error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(plotdata[\"batchsize\"], plotdata[\"avgloss\"], 'b--')\n",
    "plt.xlabel('Minibatch number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Minibatch run vs. Training loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(plotdata[\"batchsize\"], plotdata[\"avgerror\"], 'r--')\n",
    "plt.xlabel('Minibatch number')\n",
    "plt.ylabel('Label Prediction Error')\n",
    "plt.title('Minibatch run vs. Label Prediction Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation / Testing \n",
    "\n",
    "Now that we have trained the network. Let us evaluate the trained network on data that hasn't been used for training. This is called **testing**. Let us create some new data and evaluate the average error and loss on this set. This is done using `trainer.test_minibatch`. Note the error on this previously unseen data is comparable to training error. This is a **key** check. Should the error be larger than the training error by a large margin, it indicates that the trained model will not perform well on data that it has not seen during training. This is known as [overfitting][]. There are several ways to address overfitting that is beyond the scope of this tutorial but the Cognitive Toolkit provides the necessary components to address overfitting.\n",
    "\n",
    "Note: We are testing on a single minibatch for illustrative purposes. In practice one runs several minibatches of test data and reports the average. \n",
    "\n",
    "**Question** Why is this suggested? Try plotting the test error over several set of generated data sample and plot using plotting functions used for training. Do you see a pattern?\n",
    "\n",
    "[overfitting]: https://en.wikipedia.org/wiki/Overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the trained model on newly generated dataset\n",
    "test_minibatch_size = 25\n",
    "features, labels = generate_random_data_sample(test_minibatch_size, input_dim, num_output_classes)\n",
    "\n",
    "trainer.test_minibatch({feature : features, label : labels}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking prediction / evaluation \n",
    "For evaluation, we map the output of the network between 0-1 and convert them into probabilities for the two classes. This suggests the chances of each observation being malignant and benign. We use a softmax function to get the probabilities of each of the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = C.softmax(z)\n",
    "result = out.eval({feature : features})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the ground-truth label with the predictions. They should be in agreement.\n",
    "\n",
    "**Question:** \n",
    "- How many predictions were mislabeled? Can you change the code below to identify which observations were misclassified? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Label    :\", [np.argmax(label) for label in labels])\n",
    "print(\"Predicted:\", [np.argmax(result[i,:]) for i in range(len(result))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "It is desirable to visualize the results. In this example, the data is conveniently in two dimensions and can be plotted. For data with higher dimensions, visualization can be challenging. There are advanced dimensionality reduction techniques that allow for such visualizations [t-sne][].\n",
    "\n",
    "[t-sne]: https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "print(mydict['b'].value)\n",
    "\n",
    "bias_vector   = mydict['b'].value\n",
    "weight_matrix = mydict['w'].value\n",
    "\n",
    "# Plot the data \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# given this is a 2 class \n",
    "colors = ['r' if l == 0 else 'b' for l in labels[:,0]]\n",
    "plt.scatter(features[:,0], features[:,1], c=colors)\n",
    "plt.plot([0, bias_vector[0]/weight_matrix[0][1]], \n",
    "         [ bias_vector[1]/weight_matrix[0][0], 0], c = 'g', lw = 3)\n",
    "plt.xlabel(\"Scaled age (in yrs)\")\n",
    "plt.ylabel(\"Tumor size (in cm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exploration Suggestions** \n",
    "- Try exploring how the classifier behaves with different data distributions - suggest changing the `minibatch_size` parameter from 25 to say 64. Why is the error increasing?\n",
    "- Try exploring different activation functions\n",
    "- Try exploring different learners \n",
    "- You can explore training a [multiclass logistic regression][] classifier.\n",
    "\n",
    "[multiclass logistic regression]: https://en.wikipedia.org/wiki/Multinomial_logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
