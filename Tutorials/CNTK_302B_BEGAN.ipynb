{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK 302 Part B: Boundary Equilibrium GAN with CelebA data\n",
    "\n",
    "## Introduction\n",
    "Generative models have gained a lot of attention in deep learning community which has tranditionally leveraged discriminatorive models for semi-supervised and unsupervised learning. [Generative Adversarial Network (GAN)](https://arxiv.org/pdf/1406.2661v1.pdf) (Goodfellow *et al.*, 2014) is one of the most popular generative model because of its promising results in [various tasks](https://github.com/HKCaesar/really-awesome-gan) in computer vision and natural language processing. However, the original version of GANs are notorious for being difficult to train. Without carefully balancing the convengence of the generator and discriminator, GANs could easily suffer from vanishing gradient or mode collapse (where the model is only able to produce a single or a few samples). In this tutorial, we will introduce an implementation of [Boundary Equilibrium GAN](https://arxiv.org/pdf/1703.10717.pdf) (BEGAN) which improves stability of GAN training and quality of generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two run modes:\n",
    "* *Fast mode: * `isFast` is set to `True`. This is the default mode for the notebooks, which means we train for fewer iterations or train / test on limited data. This ensures functional correctness of the notebook though the models produced are far from what a completed training would produce.\n",
    "* *Slow mode: * We recommend the user to set this flag to `False` once the user has gained familiarity with the notebook content and wants to gain insight from running the notebooks for a longer period with different parameters for training.\n",
    "\n",
    "**Note: **If the `isFlag` is set to `False` the notebook will take a few hours on a GPU enabled machine. You can try fewer iterations by setting the `num_minibatches` to a smaller number which comes at the expense of quality of the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isFast = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In BEGAN network, the generator is a convolutional neural network consists of convolution and upscale layers. The input of the generator is a 100-dimensional random vector, and the output of the generator is a flattened $3\\times64\\times64$ image. The discriminator is has an autoencoder architecuture. Both the input and the output of the discriminator are a flattened image. The encoder is consisted of convolution and strided convolution layers, and the decoder has the same struction as the generator. During training, we want the discriminator to have low reconstruction error for real images and high reconstruction error for fake images (those generated from the generator). The generator tries to confuse the discriminator by generating images that the discriminator can reconstruction with low error.\n",
    "\n",
    "To balance the training of generator and discriminator, BEGAN introduce a hyper-parameter $\\gamma$ as an equilibrium we are going to maintain in order to balance the training.\n",
    "\n",
    "$$\\gamma = \\frac{\\mathbb{E}[\\mathcal{L}(G(z))]}{\\mathbb{E}[\\mathcal{L}(x)]}$$\n",
    "where $\\mathcal{L}$ is the reconstruction error measured in L-1 norm.\n",
    "\n",
    "In order to maintain this equilibrium, we need to control the effort allocated to the generator and the discriminator. BEGAN uses a prorpotional control law to maintain this equilibrium,\n",
    "\n",
    "$$\\begin{cases}\n",
    "    \\begin{array}{ll}\n",
    "        \\mathcal{L}_D = \\mathcal{L}(x) - k_t \\mathcal{L}(G(z_D))&\\textrm{for }\\theta_D\\\\\n",
    "        \\mathcal{L}_G = \\mathcal{L}(G(z_G))&\\textrm{for }\\theta_G\\\\\n",
    "        k_{t+1} = k_t + \\lambda_k(\\gamma \\mathcal{L}(x)-\\mathcal{L}(G(z_G)))&\\textrm{for each training step }t\n",
    "    \\end{array}\n",
    "   \\end{cases}$$\n",
    "\n",
    "where $\\mathcal{L}_D$ and $\\mathcal{L}_G$ are the loss of the discriminator and the generator, respectively. In this proportional control rule, $k_t\\in[0,1]$ controls how much emphasis is put on $\\mathcal{L}(G(z_D))$ during gradient descent. $\\lambda_k$ is the proportional gain, or learning rate in machine learning terms, for $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "In this tutorial, we will use [CelebA dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) $^{[1]}$. The dataset contains 202599 celebrity face images. Among them we use the first 162770 images as training images for BEGAN. All images in the dataset are $178\\times218\\times3$. We downloads the dataset from Google drive and prepare it by creating map files for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modification of\n",
    "- https://github.com/carpedm20/BEGAN-tensorflow/blob/master/download.py\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params={ 'id': id }, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination, chunk_size=32*1024):\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in tqdm(response.iter_content(chunk_size), total=total_size, \n",
    "                          unit='B', unit_scale=True, desc=destination):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def loadData():\n",
    "    print ('Downloading CelebA')\n",
    "    images_path = 'images'\n",
    "    filename, drive_id  = \"img_align_celeba.zip\", \"0B7EVK8r0v71pZjFTYXZWM3FlRnM\"\n",
    "    save_path = filename\n",
    "    download_file_from_google_drive(drive_id, save_path)\n",
    "    print ('Done.')\n",
    "    try:\n",
    "        print ('Extracting files...')\n",
    "        with zipfile.ZipFile(save_path) as zf:\n",
    "            zf.extractall('.')\n",
    "        print ('Done.')\n",
    "    finally:\n",
    "        os.rename(\"img_align_celeba\", images_path)\n",
    "        os.remove(save_path)\n",
    "\n",
    "\n",
    "# Paths for saving the map files\n",
    "data_dir = './data/CelebA/'\n",
    "train_map = './train_map.txt'\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "try:\n",
    "    os.chdir(data_dir)   \n",
    "    loadData()\n",
    "    print ('Writing train map file...')\n",
    "    with open(train_map, 'w') as f:\n",
    "        for i in range(1, 162771):\n",
    "            f.write(os.path.join(os.path.abspath('images'), '{:06d}'.format(i) + '.jpg\\t0\\n'))\n",
    "        print ('Done.')\n",
    "finally:\n",
    "    os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "The input to the GANs will be a vector of random numbers. At the end of the traning, the GAN \"learns\" to generate images drawn from the CelebA dataset. Because the images in CelebA is $3\\times218\\times178$, we crop the $3\\times128\\times128$ center part of the images and resize them to $3\\times64\\times64$ while reading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image dimensionalities\n",
    "img_h, img_w = 64, 64\n",
    "img_c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join('data', 'CelebA')\n",
    "train_file = os.path.join(data_path, 'train_map.txt')\n",
    "\n",
    "def create_reader(map_file, train):\n",
    "    print(\"Reading map file:\", map_file)\n",
    "    \n",
    "    import cntk.io.transforms as xforms\n",
    "    transforms = [xforms.crop(crop_type='center', crop_size=128),\n",
    "                  xforms.scale(width=img_w, height=img_h, channels=img_c, interpolations='linear')]\n",
    "    # deserializer\n",
    "    return C.io.MinibatchSource(C.io.ImageDeserializer(map_file, C.io.StreamDefs(\n",
    "        features = C.io.StreamDef(field='image', transforms=transforms), # first column in map file is referred to as 'image'\n",
    "        labels   = C.io.StreamDef(field='label', shape=10)      # and second as 'label'\n",
    "    )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random noise we will use to train the GAN is provided by the `noise_sample` function to generate random noise samples from a uniform distribution within the interval $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "def noise_sample(num_samples):\n",
    "    return np.random.uniform(\n",
    "        low = -1.0,\n",
    "        high = 1.0,\n",
    "        size = [num_samples, g_input_dim]\n",
    "    ).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "We assume that you already have some basic familarity with GAN framework. If not, we refer you to our GAN tutorial CNTK 206A for a brief introcution of the basics of GAN.\n",
    "\n",
    "### Model components\n",
    "We build a computational graph for our model, one for the generator and one for the discriminator. First, we establish some of the architectural parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# architectural parameters\n",
    "kernel_h, kernel_w = 5, 5 \n",
    "stride_h, stride_w = 2, 2\n",
    "\n",
    "# Input / Output parameter of Generator and Discriminator\n",
    "g_input_dim = 64\n",
    "g_output_dim = d_input_dim = (img_c, img_h, img_w)\n",
    "repeat_num = int(np.log2(img_h)) - 2\n",
    "\n",
    "# Convolutional kernel size\n",
    "dkernel = 3\n",
    "\n",
    "# gamma controls the balance between G/D training\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Generator\n",
    "The generator takes a 100-dimensional random vector as input ($z$) and outputs a 12288($3\\times64\\times64$) dimensional flattened image. In this tutorial, we use blocks of $3\\times3$ convolution layers with eponential linear units (ELUs) repeated by $2$ times, then each block is followed by an upscale layer except for the last layer. In the last block, we use a tanh activation to make sure that the outputs are in the interval $[-1,1]$. Note that we DO NOT use fractionally strided convolutions and batch normalizations as in traditional GAN frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upscale(x):\n",
    "    x = C.reshape(x, (1, x.shape[0], x.shape[1], x.shape[2]))\n",
    "    size = (1, 2, 2)\n",
    "    kernel = C.constant(np.ones((1, 1, 2, 2), dtype=np.float32))\n",
    "    up = C.convolution_transpose(kernel, x, auto_padding=[False], strides=size)\n",
    "    return C.reshape(up, (up.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    print('Generator input shape:', z.shape)\n",
    "    \n",
    "    hidden_num = 128\n",
    "    \n",
    "    h = C.layers.Dense((hidden_num, 8, 8), activation=None)(z)\n",
    "    print('h0 shape:', h.shape)\n",
    "    \n",
    "    for i in range(repeat_num):\n",
    "        h = C.layers.Convolution2D(dkernel, hidden_num, activation=C.elu, pad=[True])(h)\n",
    "        h = C.layers.Convolution2D(dkernel, hidden_num, activation=C.elu, pad=[True])(h)\n",
    "        if i < repeat_num - 1:\n",
    "            h = upscale(h)\n",
    "        print('h' + str(i+1), 'shape:', h.shape)\n",
    "        \n",
    "    h = C.layers.Convolution2D(dkernel, img_c, activation=None, pad=[True])(h)\n",
    "    print('Generator output shape: ', h.shape)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "The discriminator has an auto-encoder structure. The input and output of the discriminator are both 12288-dimensional flattened image. In the encoder, we use blocks of $3\\times3$ convolution layers with ELUs repeated by 2 times, then each block is followed by a strided convolution layer except for the last block. The decoder has the same structure as the generator. The layer between the encoder and the decoder is a fully connected (dense) layer with no non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    print('Discriminator input shape: ', x.shape)\n",
    "    \n",
    "    hidden_num = 128\n",
    "    h = C.layers.Convolution2D(dkernel, hidden_num, activation=C.elu, pad=[True])(x)\n",
    "    print('Encoder h0 shape: ', h.shape)\n",
    "    \n",
    "    for i in range(repeat_num):\n",
    "        h = C.layers.Convolution2D(dkernel, hidden_num * (i + 1), activation=C.elu, pad=[True])(h)\n",
    "        h = C.layers.Convolution2D(dkernel, hidden_num * (i + 1), activation=C.elu, pad=[True])(h)\n",
    "        if i < repeat_num - 1:\n",
    "            h = C.layers.Convolution2D(dkernel, hidden_num * (i + 1), activation=C.elu, pad=[True], strides=2)(h)\n",
    "        print('Encoder h' + str(i+1), 'shape:', h.shape)\n",
    "        \n",
    "    h = C.layers.Dense(g_input_dim, activation=None)(h)\n",
    "    print('Latent code shape: ', h.shape)\n",
    "    h = C.layers.Dense((hidden_num, 8, 8), activation=None)(h)\n",
    "    print('Decoder h0 shape: ', h.shape)\n",
    "    \n",
    "    print(h.shape)\n",
    "    \n",
    "    for i in range(repeat_num):\n",
    "        h = C.layers.Convolution2D(dkernel, hidden_num, activation=C.elu, pad=[True])(h)\n",
    "        h = C.layers.Convolution2D(dkernel, hidden_num, activation=C.elu, pad=[True])(h)\n",
    "        if i < repeat_num - 1:\n",
    "            h = upscale(h)\n",
    "        print('Decoder h' + str(i+1), 'shape:', h.shape)\n",
    "        \n",
    "    h = C.layers.Convolution2D(dkernel, img_c, activation=None, pad=[True])(h)\n",
    "    print('Discriminator output shape:', h.shape)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a minibatch size of 16 and an initial learning rate of 0.00004 for training. We decay the learning rate by half every 75000 training steps until it is no greater than 0.00001. In the fast mode (`isFast=True`) we verify only functional correctness with 2000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training config\n",
    "minibatch_size = 16\n",
    "num_minibatches = 2000 if isFast else 200000\n",
    "lr = 0.00004\n",
    "momentum = 0.5\n",
    "lr_update_step = 500 if isFast else 75000\n",
    "lr_lower_bound = 0.00001\n",
    "lambda_k = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the graph\n",
    "The rest of the computational graph is mostly responsible for coordinating the traning algorithms and parameter updates. There are several useful tricks for training a GAN network.\n",
    "* The discriminator must be used on both the real images and the fake images generated by the generator. One way to represent this in the computational graph is to create a clone of the discriminator function with shared parameters and substituted inputs.\n",
    "* We need to update the parameters for the generator and discriminator separately using different learner and trainer. `Function.parameters` method is immportant since we should specify the paramters to be learned by a particular learner.\n",
    "* We need to update $k_t$ according to $\\gamma \\mathcal{L}(x)-\\mathcal{L}(G(z_G)$ (we will call it balance later). One way to easily get the averaged balance for each minibatch is to set it as a metric in the trainer. Then we can access the averaged balance of each minibatch using `Trainer.previous_minibatch_evaluation_average` method. Then, we can use `Variable.set_value` method to update $k_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(generator, discriminator):\n",
    "    input_dynamic_axes = [C.Axis.default_batch_axis()]\n",
    "    Z = C.input_variable(g_input_dim, dynamic_axes=input_dynamic_axes)\n",
    "    X_real = C.input_variable(d_input_dim, dynamic_axes=input_dynamic_axes)\n",
    "    X_real_scaled = (X_real - 127.5) / 127.5\n",
    "    \n",
    "    # initialize k_t as 0\n",
    "    k_t = C.constant(0.)\n",
    "    \n",
    "    # Create the model function for the generator and discriminator models\n",
    "    X_fake = generator(Z)\n",
    "    D_real = discriminator(X_real_scaled)\n",
    "    \n",
    "    D_fake = D_real.clone(\n",
    "        method = 'share',\n",
    "        substitutions = {X_real_scaled.output: X_fake.output}\n",
    "    )\n",
    "    \n",
    "    # Create loss functions and configure optimazation algorithms\n",
    "    D_loss_real = C.reduce_mean(C.abs(X_real_scaled - D_real))\n",
    "    D_loss_fake = C.reduce_mean(C.abs(X_fake - D_fake))\n",
    "    \n",
    "    D_loss = D_loss_real - k_t * D_loss_fake\n",
    "    G_loss = D_loss_fake\n",
    "    \n",
    "    # Compute balance for proportional control law\n",
    "    balance = gamma * D_loss_real - D_loss_fake\n",
    "    \n",
    "    G_learner = C.adam(\n",
    "            parameters = X_fake.parameters,\n",
    "            lr = C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "            momentum = C.momentum_schedule(momentum),\n",
    "            variance_momentum = C.momentum_schedule(0.999), \n",
    "            unit_gain=False,\n",
    "            use_mean_gradient=True\n",
    "    )\n",
    "    \n",
    "    D_learner = C.adam(\n",
    "            parameters = D_real.parameters,\n",
    "            lr = C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "            momentum = C.momentum_schedule(momentum),\n",
    "            variance_momentum = C.momentum_schedule(0.999), \n",
    "            unit_gain=False,\n",
    "            use_mean_gradient=True\n",
    "    )\n",
    "    \n",
    "    G_trainer = C.Trainer(X_fake,\n",
    "                        (G_loss, balance),\n",
    "                        G_learner)\n",
    "    D_trainer = C.Trainer(D_real,\n",
    "                        (D_loss, None),\n",
    "                        D_learner)\n",
    "    \n",
    "    return Z, X_real, X_fake, k_t, G_trainer, D_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "The code for training the GAN is very similar to the code for training GAN in the DCGAN Tutorial. There are three main changes:\n",
    "1. At each training step, we update the discriminator once and then update the generator once (in the DCGAN tutorial we update the discriminator twice and generator twice).\n",
    "1. After updating the discriminator and the generator, we update $k_t$ according to the balance.\n",
    "1. We decay the learning rate by half everytime we have trained 75000 steps until it reaches 0.00001.\n",
    "\n",
    "Note that in the orginal BEGAN, the authors mentioned that the generator and discriminator do not need to be trained alternatively. But we do train them alternatively since it is more convenient to train it this way. We didn't observe any performance decay or unstable behavior by training them alternatively.\n",
    "\n",
    "Another note is that the training of the model can take significantly long time especially if `isFast` flag is turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(reader_train, generator, discriminator):\n",
    "        \n",
    "    Z, X_real, X_fake, k_t, G_trainer, D_trainer = \\\n",
    "        build_graph(generator, discriminator)\n",
    "    \n",
    "    # print out loss for each model for upto 25 times\n",
    "    print_frequency_mbsize = num_minibatches // 25\n",
    "    \n",
    "    print(\"First row is Generator loss, second row is Discriminator loss\")\n",
    "    pp_G = C.logging.ProgressPrinter(print_frequency_mbsize)\n",
    "    pp_D = C.logging.ProgressPrinter(print_frequency_mbsize)\n",
    "\n",
    "    input_map = {X_real: reader_train.streams.features}\n",
    "    \n",
    "    # Current training rate\n",
    "    lr_t = lr\n",
    "                \n",
    "    for train_step in range(num_minibatches):\n",
    "        # Train the discriminator and the generator alternatively\n",
    "        X_data = reader_train.next_minibatch(minibatch_size, input_map)\n",
    "        Z_data = noise_sample(minibatch_size)\n",
    "        batch_inputs = {X_real: X_data[X_real].data, Z: Z_data}\n",
    "        D_trainer.train_minibatch(batch_inputs)\n",
    "        G_trainer.train_minibatch(batch_inputs)\n",
    "        \n",
    "        pp_G.update_with_trainer(G_trainer)\n",
    "        pp_D.update_with_trainer(D_trainer)\n",
    "        \n",
    "        G_trainer_loss = G_trainer.previous_minibatch_loss_average\n",
    "        \n",
    "        # Update k_t\n",
    "        balance = G_trainer.previous_minibatch_evaluation_average\n",
    "        k_update = max(min(k_t.value + lambda_k * balance, 1), 0)\n",
    "        k_t.set_value(C.NDArrayView.from_dense(np.asarray(k_update, dtype=np.float32)))\n",
    "        \n",
    "        # Decay learning rate by half after 75000 steps\n",
    "        if train_step % lr_update_step == lr_update_step - 1:\n",
    "            lr_t = max(lr_t * 0.5, lr_lower_bound)\n",
    "            print('reset learning rate to ', lr_t)\n",
    "            for learner in G_trainer.parameter_learners:\n",
    "                learner.reset_learning_rate(\n",
    "                        C.learning_rate_schedule(lr_t, C.UnitType.sample)\n",
    "                        )\n",
    "            for learner in D_trainer.parameter_learners:\n",
    "                learner.reset_learning_rate(\n",
    "                        C.learning_rate_schedule(lr_t, C.UnitType.sample)\n",
    "                        )\n",
    "            \n",
    "    return Z, X_fake, G_trainer_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_train = create_reader(train_file, True)\n",
    "\n",
    "# G_input, G_output, G_trainer_loss = train(reader_train, dense_generator, dense_discriminator)\n",
    "G_input, G_output, G_trainer_loss = train(reader_train,\n",
    "                                          generator,\n",
    "                                          discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the generator loss \n",
    "print(\"Training loss of the generator is: {0:.2f}\".format(G_trainer_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generating Fake (Synthesized) Images\n",
    "Now that we have trained the model, we can create fake images simply by feeding random noise into the generator and displaying the outputs. Below are a few images generated from random samples. To get a new set of samples, you can re-run the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, subplot_shape):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, axes = plt.subplots(*subplot_shape)\n",
    "    for image, ax in zip(images, axes.flatten()):\n",
    "        image = image[np.array([2,1,0]),:,:]\n",
    "        image = np.rollaxis(image / 2 + 0.5, 0, 3)\n",
    "        ax.imshow(image, vmin=-1.0, vmax=1.0)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "noise = noise_sample(36)\n",
    "images = G_output.eval({G_input: noise})\n",
    "plot_images(images, subplot_shape=[6, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger number of iterations should generate more realistic looking face images. A sampling of such generated images are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[1] S. Yang, P. Luo, C. C. Loy, and X. Tang, \"From Facial Parts Responses to Face Detection: A Deep Learning Approach\", in *IEEE International Conference on Computer Vision (ICCV)*, 2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
