# See agent/shared/qlearning_parameters.py for detailed explanation of each
# parameter.

[General]
Gamma = 0.99
# PreProcessing = AtariPreprocessing
# PreProcessingArgs = (4,)

[QLearningAlgo]
InitialEpsilon = 1.0
EpsilonDecayStepCount = 10000
EpsilonMinimum = 0.01
InitialQ = 0.0
TargetQUpdateFrequency = 100
QUpdateFrequency = 4
MinibatchSize = 32
# QRepresentation can be dqn, dueling-dqn, or some customized model defined in
# agent.customized_qlearn_models
QRepresentation = dqn
ErrorClipping = False

[ExperienceReplay]
Capacity = 500
StartSize = 100
Prioritized = True
PriorityAlpha = 0.7
PriorityBeta = 1
PriorityEpsilon = 0.0001

[NetworkModel]
# Use (a list of integers) when QRepresentation is dqn
HiddenLayerNodes = [20]

# Or use (a list of integers followed by two lists of integers) when
# QRepresentation is dueling-dqn
; HiddenLayerNodes = [10, [5], [5]]

[Optimization]
Momentum = 0.9
InitialEta = 0.01
EtaDecayStepCount = 10000
EtaMinimum = 0.0001
GradientClippingThreshold = 10
